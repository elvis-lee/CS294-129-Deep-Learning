{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deeplearning.classifiers.cnn import *\n",
    "from deeplearning.data_utils import get_CIFAR10_data\n",
    "from deeplearning.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from deeplearning.layers import *\n",
    "from deeplearning.fast_layers import *\n",
    "from deeplearning.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 7640) loss: 2.609699\n",
      "(Epoch 0 / 20) train acc: 0.166000; val_acc: 0.180000\n",
      "(Iteration 21 / 7640) loss: 2.043178\n",
      "(Iteration 41 / 7640) loss: 1.844493\n",
      "(Iteration 61 / 7640) loss: 1.735653\n",
      "(Iteration 81 / 7640) loss: 1.729395\n",
      "(Iteration 101 / 7640) loss: 1.681688\n",
      "(Iteration 121 / 7640) loss: 1.710809\n",
      "(Iteration 141 / 7640) loss: 1.562607\n",
      "(Iteration 161 / 7640) loss: 1.412313\n",
      "(Iteration 181 / 7640) loss: 1.342147\n",
      "(Iteration 201 / 7640) loss: 1.390245\n",
      "(Iteration 221 / 7640) loss: 1.597349\n",
      "(Iteration 241 / 7640) loss: 1.500816\n",
      "(Iteration 261 / 7640) loss: 1.415161\n",
      "(Iteration 281 / 7640) loss: 1.400503\n",
      "(Iteration 301 / 7640) loss: 1.357696\n",
      "(Iteration 321 / 7640) loss: 1.468279\n",
      "(Iteration 341 / 7640) loss: 1.380010\n",
      "(Iteration 361 / 7640) loss: 1.526878\n",
      "(Iteration 381 / 7640) loss: 1.302666\n",
      "Time elapsed (hh:mm:ss.ms) 0:18:13.685261\n",
      "(Epoch 1 / 20) train acc: 0.636000; val_acc: 0.594000\n",
      "(Iteration 401 / 7640) loss: 1.195580\n",
      "(Iteration 421 / 7640) loss: 1.292176\n",
      "(Iteration 441 / 7640) loss: 1.298495\n",
      "(Iteration 461 / 7640) loss: 1.348339\n",
      "(Iteration 481 / 7640) loss: 1.324393\n",
      "(Iteration 501 / 7640) loss: 1.389517\n",
      "(Iteration 521 / 7640) loss: 1.283150\n",
      "(Iteration 541 / 7640) loss: 1.299152\n",
      "(Iteration 561 / 7640) loss: 1.160037\n",
      "(Iteration 581 / 7640) loss: 1.221008\n",
      "(Iteration 601 / 7640) loss: 1.249169\n",
      "(Iteration 621 / 7640) loss: 1.116692\n",
      "(Iteration 641 / 7640) loss: 1.200753\n",
      "(Iteration 661 / 7640) loss: 1.239656\n",
      "(Iteration 681 / 7640) loss: 1.178981\n",
      "(Iteration 701 / 7640) loss: 1.149613\n",
      "(Iteration 721 / 7640) loss: 1.093054\n",
      "(Iteration 741 / 7640) loss: 1.145387\n",
      "(Iteration 761 / 7640) loss: 1.125160\n",
      "Time elapsed (hh:mm:ss.ms) 0:29:10.920653\n",
      "(Epoch 2 / 20) train acc: 0.689000; val_acc: 0.618000\n",
      "(Iteration 781 / 7640) loss: 1.098505\n",
      "(Iteration 801 / 7640) loss: 1.130356\n",
      "(Iteration 821 / 7640) loss: 1.173987\n",
      "(Iteration 841 / 7640) loss: 1.182344\n",
      "(Iteration 861 / 7640) loss: 1.095680\n",
      "(Iteration 881 / 7640) loss: 1.124154\n",
      "(Iteration 901 / 7640) loss: 1.001242\n",
      "(Iteration 921 / 7640) loss: 1.046903\n",
      "(Iteration 941 / 7640) loss: 1.134924\n",
      "(Iteration 961 / 7640) loss: 1.069775\n",
      "(Iteration 981 / 7640) loss: 1.055488\n",
      "(Iteration 1001 / 7640) loss: 0.967236\n",
      "(Iteration 1021 / 7640) loss: 1.107520\n",
      "(Iteration 1041 / 7640) loss: 1.073650\n",
      "(Iteration 1061 / 7640) loss: 1.085739\n",
      "(Iteration 1081 / 7640) loss: 0.997920\n",
      "(Iteration 1101 / 7640) loss: 0.918424\n",
      "(Iteration 1121 / 7640) loss: 0.939482\n",
      "(Iteration 1141 / 7640) loss: 0.912437\n",
      "Time elapsed (hh:mm:ss.ms) 0:38:13.351953\n",
      "(Epoch 3 / 20) train acc: 0.709000; val_acc: 0.623000\n",
      "(Iteration 1161 / 7640) loss: 0.887697\n",
      "(Iteration 1181 / 7640) loss: 1.049320\n",
      "(Iteration 1201 / 7640) loss: 1.076435\n",
      "(Iteration 1221 / 7640) loss: 1.039241\n",
      "(Iteration 1241 / 7640) loss: 0.849761\n",
      "(Iteration 1261 / 7640) loss: 0.889534\n",
      "(Iteration 1281 / 7640) loss: 1.015641\n",
      "(Iteration 1301 / 7640) loss: 0.941713\n",
      "(Iteration 1321 / 7640) loss: 0.818883\n",
      "(Iteration 1341 / 7640) loss: 1.095241\n",
      "(Iteration 1361 / 7640) loss: 0.847447\n",
      "(Iteration 1381 / 7640) loss: 0.956819\n",
      "(Iteration 1401 / 7640) loss: 0.866959\n",
      "(Iteration 1421 / 7640) loss: 0.922109\n",
      "(Iteration 1441 / 7640) loss: 0.810378\n",
      "(Iteration 1461 / 7640) loss: 0.905403\n",
      "(Iteration 1481 / 7640) loss: 0.894746\n",
      "(Iteration 1501 / 7640) loss: 0.853039\n",
      "(Iteration 1521 / 7640) loss: 0.869771\n",
      "Time elapsed (hh:mm:ss.ms) 0:47:36.136785\n",
      "(Epoch 4 / 20) train acc: 0.777000; val_acc: 0.643000\n",
      "(Iteration 1541 / 7640) loss: 0.879889\n",
      "(Iteration 1561 / 7640) loss: 0.871338\n",
      "(Iteration 1581 / 7640) loss: 0.798518\n",
      "(Iteration 1601 / 7640) loss: 0.912497\n",
      "(Iteration 1621 / 7640) loss: 0.949493\n",
      "(Iteration 1641 / 7640) loss: 0.985093\n",
      "(Iteration 1661 / 7640) loss: 0.805817\n",
      "(Iteration 1681 / 7640) loss: 0.914532\n",
      "(Iteration 1701 / 7640) loss: 0.841840\n",
      "(Iteration 1721 / 7640) loss: 0.741164\n",
      "(Iteration 1741 / 7640) loss: 0.942187\n",
      "(Iteration 1761 / 7640) loss: 0.891279\n",
      "(Iteration 1781 / 7640) loss: 0.784851\n",
      "(Iteration 1801 / 7640) loss: 0.963841\n",
      "(Iteration 1821 / 7640) loss: 0.794066\n",
      "(Iteration 1841 / 7640) loss: 0.810736\n",
      "(Iteration 1861 / 7640) loss: 0.798138\n",
      "(Iteration 1881 / 7640) loss: 0.864811\n",
      "(Iteration 1901 / 7640) loss: 0.798065\n",
      "Time elapsed (hh:mm:ss.ms) 0:57:17.185579\n",
      "(Epoch 5 / 20) train acc: 0.775000; val_acc: 0.662000\n",
      "(Iteration 1921 / 7640) loss: 0.741522\n",
      "(Iteration 1941 / 7640) loss: 0.814483\n",
      "(Iteration 1961 / 7640) loss: 0.954367\n",
      "(Iteration 1981 / 7640) loss: 0.760220\n",
      "(Iteration 2001 / 7640) loss: 0.770448\n",
      "(Iteration 2021 / 7640) loss: 0.759564\n",
      "(Iteration 2041 / 7640) loss: 0.869709\n",
      "(Iteration 2061 / 7640) loss: 0.786433\n",
      "(Iteration 2081 / 7640) loss: 0.802376\n",
      "(Iteration 2101 / 7640) loss: 0.842304\n",
      "(Iteration 2121 / 7640) loss: 0.747248\n",
      "(Iteration 2141 / 7640) loss: 0.791230\n",
      "(Iteration 2161 / 7640) loss: 0.801389\n",
      "(Iteration 2181 / 7640) loss: 0.746425\n",
      "(Iteration 2201 / 7640) loss: 0.702932\n",
      "(Iteration 2221 / 7640) loss: 0.739814\n",
      "(Iteration 2241 / 7640) loss: 0.770462\n",
      "(Iteration 2261 / 7640) loss: 0.704089\n",
      "(Iteration 2281 / 7640) loss: 0.699279\n",
      "Time elapsed (hh:mm:ss.ms) 1:06:58.736136\n",
      "(Epoch 6 / 20) train acc: 0.852000; val_acc: 0.646000\n",
      "(Iteration 2301 / 7640) loss: 0.604138\n",
      "(Iteration 2321 / 7640) loss: 0.691627\n",
      "(Iteration 2341 / 7640) loss: 0.686892\n",
      "(Iteration 2361 / 7640) loss: 0.580996\n",
      "(Iteration 2381 / 7640) loss: 0.744953\n",
      "(Iteration 2401 / 7640) loss: 0.777669\n",
      "(Iteration 2421 / 7640) loss: 0.730193\n",
      "(Iteration 2441 / 7640) loss: 0.691878\n",
      "(Iteration 2461 / 7640) loss: 0.767659\n",
      "(Iteration 2481 / 7640) loss: 0.701983\n",
      "(Iteration 2501 / 7640) loss: 0.658606\n",
      "(Iteration 2521 / 7640) loss: 0.645562\n",
      "(Iteration 2541 / 7640) loss: 0.787158\n",
      "(Iteration 2561 / 7640) loss: 0.570774\n",
      "(Iteration 2581 / 7640) loss: 0.681362\n",
      "(Iteration 2601 / 7640) loss: 0.716290\n",
      "(Iteration 2621 / 7640) loss: 0.660872\n",
      "(Iteration 2641 / 7640) loss: 0.757118\n",
      "(Iteration 2661 / 7640) loss: 0.677257\n",
      "Time elapsed (hh:mm:ss.ms) 1:16:01.982037\n",
      "(Epoch 7 / 20) train acc: 0.868000; val_acc: 0.642000\n",
      "(Iteration 2681 / 7640) loss: 0.702254\n",
      "(Iteration 2701 / 7640) loss: 0.702505\n",
      "(Iteration 2721 / 7640) loss: 0.654679\n",
      "(Iteration 2741 / 7640) loss: 0.667684\n",
      "(Iteration 2761 / 7640) loss: 0.713185\n",
      "(Iteration 2781 / 7640) loss: 0.572319\n",
      "(Iteration 2801 / 7640) loss: 0.670830\n",
      "(Iteration 2821 / 7640) loss: 0.595485\n",
      "(Iteration 2841 / 7640) loss: 0.661636\n",
      "(Iteration 2861 / 7640) loss: 0.798228\n",
      "(Iteration 2881 / 7640) loss: 0.670771\n",
      "(Iteration 2901 / 7640) loss: 0.600464\n",
      "(Iteration 2921 / 7640) loss: 0.670408\n",
      "(Iteration 2941 / 7640) loss: 0.509859\n",
      "(Iteration 2961 / 7640) loss: 0.518912\n",
      "(Iteration 2981 / 7640) loss: 0.532134\n",
      "(Iteration 3001 / 7640) loss: 0.637441\n",
      "(Iteration 3021 / 7640) loss: 0.550407\n",
      "(Iteration 3041 / 7640) loss: 0.601426\n",
      "Time elapsed (hh:mm:ss.ms) 1:25:02.071952\n",
      "(Epoch 8 / 20) train acc: 0.902000; val_acc: 0.647000\n",
      "(Iteration 3061 / 7640) loss: 0.566408\n",
      "(Iteration 3081 / 7640) loss: 0.581118\n",
      "(Iteration 3101 / 7640) loss: 0.566328\n",
      "(Iteration 3121 / 7640) loss: 0.540342\n",
      "(Iteration 3141 / 7640) loss: 0.534966\n",
      "(Iteration 3161 / 7640) loss: 0.505660\n",
      "(Iteration 3181 / 7640) loss: 0.636216\n",
      "(Iteration 3201 / 7640) loss: 0.462655\n",
      "(Iteration 3221 / 7640) loss: 0.605021\n",
      "(Iteration 3241 / 7640) loss: 0.561860\n",
      "(Iteration 3261 / 7640) loss: 0.594913\n",
      "(Iteration 3281 / 7640) loss: 0.562312\n",
      "(Iteration 3301 / 7640) loss: 0.552670\n",
      "(Iteration 3321 / 7640) loss: 0.547323\n",
      "(Iteration 3341 / 7640) loss: 0.553604\n",
      "(Iteration 3361 / 7640) loss: 0.537113\n",
      "(Iteration 3381 / 7640) loss: 0.530741\n",
      "(Iteration 3401 / 7640) loss: 0.524068\n",
      "(Iteration 3421 / 7640) loss: 0.509677\n",
      "Time elapsed (hh:mm:ss.ms) 1:34:10.686160\n",
      "(Epoch 9 / 20) train acc: 0.902000; val_acc: 0.653000\n",
      "(Iteration 3441 / 7640) loss: 0.505827\n",
      "(Iteration 3461 / 7640) loss: 0.498629\n",
      "(Iteration 3481 / 7640) loss: 0.464555\n",
      "(Iteration 3501 / 7640) loss: 0.475219\n",
      "(Iteration 3521 / 7640) loss: 0.490610\n",
      "(Iteration 3541 / 7640) loss: 0.468739\n",
      "(Iteration 3561 / 7640) loss: 0.531732\n",
      "(Iteration 3581 / 7640) loss: 0.552116\n",
      "(Iteration 3601 / 7640) loss: 0.551966\n",
      "(Iteration 3621 / 7640) loss: 0.541473\n",
      "(Iteration 3641 / 7640) loss: 0.502579\n",
      "(Iteration 3661 / 7640) loss: 0.452268\n",
      "(Iteration 3681 / 7640) loss: 0.422416\n",
      "(Iteration 3701 / 7640) loss: 0.467961\n",
      "(Iteration 3721 / 7640) loss: 0.448986\n",
      "(Iteration 3741 / 7640) loss: 0.547313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3761 / 7640) loss: 0.446407\n",
      "(Iteration 3781 / 7640) loss: 0.500174\n",
      "(Iteration 3801 / 7640) loss: 0.448950\n",
      "Time elapsed (hh:mm:ss.ms) 1:43:37.054093\n",
      "(Epoch 10 / 20) train acc: 0.912000; val_acc: 0.632000\n",
      "(Iteration 3821 / 7640) loss: 0.471563\n",
      "(Iteration 3841 / 7640) loss: 0.484374\n",
      "(Iteration 3861 / 7640) loss: 0.499562\n",
      "(Iteration 3881 / 7640) loss: 0.479990\n",
      "(Iteration 3901 / 7640) loss: 0.496085\n",
      "(Iteration 3921 / 7640) loss: 0.558362\n",
      "(Iteration 3941 / 7640) loss: 0.503917\n",
      "(Iteration 3961 / 7640) loss: 0.430966\n",
      "(Iteration 3981 / 7640) loss: 0.430091\n",
      "(Iteration 4001 / 7640) loss: 0.514588\n",
      "(Iteration 4021 / 7640) loss: 0.440530\n",
      "(Iteration 4041 / 7640) loss: 0.411070\n",
      "(Iteration 4061 / 7640) loss: 0.445421\n",
      "(Iteration 4081 / 7640) loss: 0.430154\n",
      "(Iteration 4101 / 7640) loss: 0.410336\n",
      "(Iteration 4121 / 7640) loss: 0.420415\n",
      "(Iteration 4141 / 7640) loss: 0.468619\n",
      "(Iteration 4161 / 7640) loss: 0.450202\n",
      "(Iteration 4181 / 7640) loss: 0.460446\n",
      "(Iteration 4201 / 7640) loss: 0.436630\n",
      "Time elapsed (hh:mm:ss.ms) 1:53:10.451395\n",
      "(Epoch 11 / 20) train acc: 0.914000; val_acc: 0.635000\n",
      "(Iteration 4221 / 7640) loss: 0.374381\n",
      "(Iteration 4241 / 7640) loss: 0.397017\n",
      "(Iteration 4261 / 7640) loss: 0.392142\n",
      "(Iteration 4281 / 7640) loss: 0.432870\n",
      "(Iteration 4301 / 7640) loss: 0.421546\n",
      "(Iteration 4321 / 7640) loss: 0.393664\n",
      "(Iteration 4341 / 7640) loss: 0.476914\n",
      "(Iteration 4361 / 7640) loss: 0.465578\n",
      "(Iteration 4381 / 7640) loss: 0.395420\n",
      "(Iteration 4401 / 7640) loss: 0.410810\n",
      "(Iteration 4421 / 7640) loss: 0.407271\n",
      "(Iteration 4441 / 7640) loss: 0.415875\n",
      "(Iteration 4461 / 7640) loss: 0.385423\n",
      "(Iteration 4481 / 7640) loss: 0.415284\n",
      "(Iteration 4501 / 7640) loss: 0.406142\n",
      "(Iteration 4521 / 7640) loss: 0.374051\n",
      "(Iteration 4541 / 7640) loss: 0.446801\n",
      "(Iteration 4561 / 7640) loss: 0.389756\n",
      "(Iteration 4581 / 7640) loss: 0.391659\n",
      "Time elapsed (hh:mm:ss.ms) 2:03:32.609799\n",
      "(Epoch 12 / 20) train acc: 0.950000; val_acc: 0.641000\n",
      "(Iteration 4601 / 7640) loss: 0.406423\n",
      "(Iteration 4621 / 7640) loss: 0.412212\n",
      "(Iteration 4641 / 7640) loss: 0.412086\n",
      "(Iteration 4661 / 7640) loss: 0.360418\n",
      "(Iteration 4681 / 7640) loss: 0.385675\n",
      "(Iteration 4701 / 7640) loss: 0.354373\n",
      "(Iteration 4721 / 7640) loss: 0.352214\n",
      "(Iteration 4741 / 7640) loss: 0.373217\n",
      "(Iteration 4761 / 7640) loss: 0.418516\n",
      "(Iteration 4781 / 7640) loss: 0.377276\n",
      "(Iteration 4801 / 7640) loss: 0.349656\n",
      "(Iteration 4821 / 7640) loss: 0.399819\n",
      "(Iteration 4841 / 7640) loss: 0.360715\n",
      "(Iteration 4861 / 7640) loss: 0.349573\n",
      "(Iteration 4881 / 7640) loss: 0.350542\n",
      "(Iteration 4901 / 7640) loss: 0.365473\n",
      "(Iteration 4921 / 7640) loss: 0.337578\n",
      "(Iteration 4941 / 7640) loss: 0.384481\n",
      "(Iteration 4961 / 7640) loss: 0.351316\n",
      "Time elapsed (hh:mm:ss.ms) 2:13:55.350787\n",
      "(Epoch 13 / 20) train acc: 0.967000; val_acc: 0.641000\n",
      "(Iteration 4981 / 7640) loss: 0.309482\n",
      "(Iteration 5001 / 7640) loss: 0.363685\n",
      "(Iteration 5021 / 7640) loss: 0.326259\n",
      "(Iteration 5041 / 7640) loss: 0.404312\n",
      "(Iteration 5061 / 7640) loss: 0.363606\n",
      "(Iteration 5081 / 7640) loss: 0.346406\n",
      "(Iteration 5101 / 7640) loss: 0.426378\n",
      "(Iteration 5121 / 7640) loss: 0.394220\n",
      "(Iteration 5141 / 7640) loss: 0.403147\n",
      "(Iteration 5161 / 7640) loss: 0.338479\n",
      "(Iteration 5181 / 7640) loss: 0.319263\n",
      "(Iteration 5201 / 7640) loss: 0.337687\n",
      "(Iteration 5221 / 7640) loss: 0.302010\n",
      "(Iteration 5241 / 7640) loss: 0.349141\n",
      "(Iteration 5261 / 7640) loss: 0.332731\n",
      "(Iteration 5281 / 7640) loss: 0.313547\n",
      "(Iteration 5301 / 7640) loss: 0.361146\n",
      "(Iteration 5321 / 7640) loss: 0.311294\n",
      "(Iteration 5341 / 7640) loss: 0.334795\n",
      "Time elapsed (hh:mm:ss.ms) 2:24:01.680252\n",
      "(Epoch 14 / 20) train acc: 0.972000; val_acc: 0.638000\n",
      "(Iteration 5361 / 7640) loss: 0.370975\n",
      "(Iteration 5381 / 7640) loss: 0.292768\n",
      "(Iteration 5401 / 7640) loss: 0.313807\n",
      "(Iteration 5421 / 7640) loss: 0.308716\n",
      "(Iteration 5441 / 7640) loss: 0.297490\n",
      "(Iteration 5461 / 7640) loss: 0.323831\n",
      "(Iteration 5481 / 7640) loss: 0.288640\n",
      "(Iteration 5501 / 7640) loss: 0.319464\n",
      "(Iteration 5521 / 7640) loss: 0.315547\n",
      "(Iteration 5541 / 7640) loss: 0.331677\n",
      "(Iteration 5561 / 7640) loss: 0.294070\n",
      "(Iteration 5581 / 7640) loss: 0.340047\n",
      "(Iteration 5601 / 7640) loss: 0.357739\n",
      "(Iteration 5621 / 7640) loss: 0.318047\n",
      "(Iteration 5641 / 7640) loss: 0.264894\n",
      "(Iteration 5661 / 7640) loss: 0.295725\n",
      "(Iteration 5681 / 7640) loss: 0.356973\n",
      "(Iteration 5701 / 7640) loss: 0.296973\n",
      "(Iteration 5721 / 7640) loss: 0.308634\n",
      "Time elapsed (hh:mm:ss.ms) 2:33:58.878275\n",
      "(Epoch 15 / 20) train acc: 0.978000; val_acc: 0.628000\n",
      "(Iteration 5741 / 7640) loss: 0.294317\n",
      "(Iteration 5761 / 7640) loss: 0.306805\n",
      "(Iteration 5781 / 7640) loss: 0.294661\n",
      "(Iteration 5801 / 7640) loss: 0.325721\n",
      "(Iteration 5821 / 7640) loss: 0.305419\n",
      "(Iteration 5841 / 7640) loss: 0.319428\n",
      "(Iteration 5861 / 7640) loss: 0.278811\n",
      "(Iteration 5881 / 7640) loss: 0.330829\n",
      "(Iteration 5901 / 7640) loss: 0.254536\n",
      "(Iteration 5921 / 7640) loss: 0.279676\n",
      "(Iteration 5941 / 7640) loss: 0.314052\n",
      "(Iteration 5961 / 7640) loss: 0.267143\n",
      "(Iteration 5981 / 7640) loss: 0.323593\n",
      "(Iteration 6001 / 7640) loss: 0.268340\n",
      "(Iteration 6021 / 7640) loss: 0.263830\n",
      "(Iteration 6041 / 7640) loss: 0.300956\n",
      "(Iteration 6061 / 7640) loss: 0.335147\n",
      "(Iteration 6081 / 7640) loss: 0.289117\n",
      "(Iteration 6101 / 7640) loss: 0.251713\n",
      "Time elapsed (hh:mm:ss.ms) 2:42:58.682679\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.630000\n",
      "(Iteration 6121 / 7640) loss: 0.255516\n",
      "(Iteration 6141 / 7640) loss: 0.255691\n",
      "(Iteration 6161 / 7640) loss: 0.279255\n",
      "(Iteration 6181 / 7640) loss: 0.311914\n",
      "(Iteration 6201 / 7640) loss: 0.296305\n",
      "(Iteration 6221 / 7640) loss: 0.279024\n",
      "(Iteration 6241 / 7640) loss: 0.291378\n",
      "(Iteration 6261 / 7640) loss: 0.297716\n",
      "(Iteration 6281 / 7640) loss: 0.266296\n",
      "(Iteration 6301 / 7640) loss: 0.252017\n",
      "(Iteration 6321 / 7640) loss: 0.255740\n",
      "(Iteration 6341 / 7640) loss: 0.275314\n",
      "(Iteration 6361 / 7640) loss: 0.240262\n",
      "(Iteration 6381 / 7640) loss: 0.259978\n",
      "(Iteration 6401 / 7640) loss: 0.267064\n",
      "(Iteration 6421 / 7640) loss: 0.302301\n",
      "(Iteration 6441 / 7640) loss: 0.240531\n",
      "(Iteration 6461 / 7640) loss: 0.252105\n",
      "(Iteration 6481 / 7640) loss: 0.230036\n",
      "Time elapsed (hh:mm:ss.ms) 2:51:55.445671\n",
      "(Epoch 17 / 20) train acc: 0.984000; val_acc: 0.634000\n",
      "(Iteration 6501 / 7640) loss: 0.279189\n",
      "(Iteration 6521 / 7640) loss: 0.242585\n",
      "(Iteration 6541 / 7640) loss: 0.248905\n",
      "(Iteration 6561 / 7640) loss: 0.293409\n",
      "(Iteration 6581 / 7640) loss: 0.237769\n",
      "(Iteration 6601 / 7640) loss: 0.257092\n",
      "(Iteration 6621 / 7640) loss: 0.224704\n",
      "(Iteration 6641 / 7640) loss: 0.237931\n",
      "(Iteration 6661 / 7640) loss: 0.248388\n",
      "(Iteration 6681 / 7640) loss: 0.241608\n",
      "(Iteration 6701 / 7640) loss: 0.222587\n",
      "(Iteration 6721 / 7640) loss: 0.215206\n",
      "(Iteration 6741 / 7640) loss: 0.273376\n",
      "(Iteration 6761 / 7640) loss: 0.235554\n",
      "(Iteration 6781 / 7640) loss: 0.250551\n",
      "(Iteration 6801 / 7640) loss: 0.239651\n",
      "(Iteration 6821 / 7640) loss: 0.274387\n",
      "(Iteration 6841 / 7640) loss: 0.244486\n",
      "(Iteration 6861 / 7640) loss: 0.239578\n",
      "Time elapsed (hh:mm:ss.ms) 3:01:28.092771\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.621000\n",
      "(Iteration 6881 / 7640) loss: 0.242686\n",
      "(Iteration 6901 / 7640) loss: 0.234286\n",
      "(Iteration 6921 / 7640) loss: 0.251921\n",
      "(Iteration 6941 / 7640) loss: 0.243481\n",
      "(Iteration 6961 / 7640) loss: 0.221472\n",
      "(Iteration 6981 / 7640) loss: 0.205748\n",
      "(Iteration 7001 / 7640) loss: 0.220088\n",
      "(Iteration 7021 / 7640) loss: 0.206007\n",
      "(Iteration 7041 / 7640) loss: 0.231252\n",
      "(Iteration 7061 / 7640) loss: 0.230524\n",
      "(Iteration 7081 / 7640) loss: 0.218507\n",
      "(Iteration 7101 / 7640) loss: 0.236727\n",
      "(Iteration 7121 / 7640) loss: 0.224574\n",
      "(Iteration 7141 / 7640) loss: 0.272380\n",
      "(Iteration 7161 / 7640) loss: 0.243672\n",
      "(Iteration 7181 / 7640) loss: 0.229381\n",
      "(Iteration 7201 / 7640) loss: 0.223918\n",
      "(Iteration 7221 / 7640) loss: 0.207292\n",
      "(Iteration 7241 / 7640) loss: 0.224517\n",
      "Time elapsed (hh:mm:ss.ms) 3:10:24.048541\n",
      "(Epoch 19 / 20) train acc: 0.987000; val_acc: 0.627000\n",
      "(Iteration 7261 / 7640) loss: 0.224500\n",
      "(Iteration 7281 / 7640) loss: 0.193154\n",
      "(Iteration 7301 / 7640) loss: 0.205801\n",
      "(Iteration 7321 / 7640) loss: 0.236464\n",
      "(Iteration 7341 / 7640) loss: 0.198066\n",
      "(Iteration 7361 / 7640) loss: 0.219976\n",
      "(Iteration 7381 / 7640) loss: 0.205828\n",
      "(Iteration 7401 / 7640) loss: 0.210813\n",
      "(Iteration 7421 / 7640) loss: 0.226667\n",
      "(Iteration 7441 / 7640) loss: 0.222903\n",
      "(Iteration 7461 / 7640) loss: 0.223190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7481 / 7640) loss: 0.214836\n",
      "(Iteration 7501 / 7640) loss: 0.208850\n",
      "(Iteration 7521 / 7640) loss: 0.204852\n",
      "(Iteration 7541 / 7640) loss: 0.224576\n",
      "(Iteration 7561 / 7640) loss: 0.206708\n",
      "(Iteration 7581 / 7640) loss: 0.206987\n",
      "(Iteration 7601 / 7640) loss: 0.219095\n",
      "(Iteration 7621 / 7640) loss: 0.241167\n",
      "Time elapsed (hh:mm:ss.ms) 3:19:56.103848\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.625000\n"
     ]
    }
   ],
   "source": [
    "# Train a really good model on CIFAR-10\n",
    "from deeplearning.classifiers.convnet1_he import *\n",
    "model = HaoConvNet1_he(hidden_dim=256, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=128,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
