{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deeplearning.classifiers.cnn import *\n",
    "from deeplearning.data_utils import get_CIFAR10_data\n",
    "from deeplearning.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from deeplearning.layers import *\n",
    "from deeplearning.fast_layers import *\n",
    "from deeplearning.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 7640) loss: 2.303151\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.132000\n",
      "(Iteration 21 / 7640) loss: 2.108633\n",
      "(Iteration 41 / 7640) loss: 1.770074\n",
      "(Iteration 61 / 7640) loss: 1.604125\n",
      "(Iteration 81 / 7640) loss: 1.694602\n",
      "(Iteration 101 / 7640) loss: 1.482351\n",
      "(Iteration 121 / 7640) loss: 1.546264\n",
      "(Iteration 141 / 7640) loss: 1.467445\n",
      "(Iteration 161 / 7640) loss: 1.430122\n",
      "(Iteration 181 / 7640) loss: 1.409327\n",
      "(Iteration 201 / 7640) loss: 1.231536\n",
      "(Iteration 221 / 7640) loss: 1.414174\n",
      "(Iteration 241 / 7640) loss: 1.344204\n",
      "(Iteration 261 / 7640) loss: 1.392385\n",
      "(Iteration 281 / 7640) loss: 1.228149\n",
      "(Iteration 301 / 7640) loss: 1.333820\n",
      "(Iteration 321 / 7640) loss: 1.349276\n",
      "(Iteration 341 / 7640) loss: 1.420570\n",
      "(Iteration 361 / 7640) loss: 1.238293\n",
      "(Iteration 381 / 7640) loss: 1.271023\n",
      "Time elapsed (hh:mm:ss.ms) 0:09:48.557490\n",
      "(Epoch 1 / 20) train acc: 0.545000; val_acc: 0.551000\n",
      "(Iteration 401 / 7640) loss: 1.262007\n",
      "(Iteration 421 / 7640) loss: 1.279590\n",
      "(Iteration 441 / 7640) loss: 1.285744\n",
      "(Iteration 461 / 7640) loss: 1.243131\n",
      "(Iteration 481 / 7640) loss: 1.260969\n",
      "(Iteration 501 / 7640) loss: 1.260715\n",
      "(Iteration 521 / 7640) loss: 1.158427\n",
      "(Iteration 541 / 7640) loss: 1.229496\n",
      "(Iteration 561 / 7640) loss: 1.214274\n",
      "(Iteration 581 / 7640) loss: 1.086893\n",
      "(Iteration 601 / 7640) loss: 1.151461\n",
      "(Iteration 621 / 7640) loss: 1.164629\n",
      "(Iteration 641 / 7640) loss: 1.063059\n",
      "(Iteration 661 / 7640) loss: 1.130589\n",
      "(Iteration 681 / 7640) loss: 1.056366\n",
      "(Iteration 701 / 7640) loss: 1.021461\n",
      "(Iteration 721 / 7640) loss: 1.262107\n",
      "(Iteration 741 / 7640) loss: 1.134393\n",
      "(Iteration 761 / 7640) loss: 1.065899\n",
      "Time elapsed (hh:mm:ss.ms) 0:18:53.445953\n",
      "(Epoch 2 / 20) train acc: 0.645000; val_acc: 0.601000\n",
      "(Iteration 781 / 7640) loss: 1.225055\n",
      "(Iteration 801 / 7640) loss: 1.079986\n",
      "(Iteration 821 / 7640) loss: 1.035835\n",
      "(Iteration 841 / 7640) loss: 1.018216\n",
      "(Iteration 861 / 7640) loss: 1.056681\n",
      "(Iteration 881 / 7640) loss: 0.894776\n",
      "(Iteration 901 / 7640) loss: 0.985264\n",
      "(Iteration 921 / 7640) loss: 1.001825\n",
      "(Iteration 941 / 7640) loss: 0.942474\n",
      "(Iteration 961 / 7640) loss: 0.903336\n",
      "(Iteration 981 / 7640) loss: 0.990377\n",
      "(Iteration 1001 / 7640) loss: 1.111803\n",
      "(Iteration 1021 / 7640) loss: 1.087417\n",
      "(Iteration 1041 / 7640) loss: 0.857158\n",
      "(Iteration 1061 / 7640) loss: 0.934056\n",
      "(Iteration 1081 / 7640) loss: 0.921397\n",
      "(Iteration 1101 / 7640) loss: 0.830935\n",
      "(Iteration 1121 / 7640) loss: 1.060295\n",
      "(Iteration 1141 / 7640) loss: 0.806637\n",
      "Time elapsed (hh:mm:ss.ms) 0:27:55.597193\n",
      "(Epoch 3 / 20) train acc: 0.697000; val_acc: 0.625000\n",
      "(Iteration 1161 / 7640) loss: 0.958078\n",
      "(Iteration 1181 / 7640) loss: 0.904621\n",
      "(Iteration 1201 / 7640) loss: 0.940390\n",
      "(Iteration 1221 / 7640) loss: 0.933715\n",
      "(Iteration 1241 / 7640) loss: 0.807276\n",
      "(Iteration 1261 / 7640) loss: 0.843279\n",
      "(Iteration 1281 / 7640) loss: 0.986804\n",
      "(Iteration 1301 / 7640) loss: 0.937766\n",
      "(Iteration 1321 / 7640) loss: 0.872060\n",
      "(Iteration 1341 / 7640) loss: 1.004188\n",
      "(Iteration 1361 / 7640) loss: 0.975750\n",
      "(Iteration 1381 / 7640) loss: 0.849723\n",
      "(Iteration 1401 / 7640) loss: 0.774487\n",
      "(Iteration 1421 / 7640) loss: 0.900693\n",
      "(Iteration 1441 / 7640) loss: 0.836998\n",
      "(Iteration 1461 / 7640) loss: 0.971803\n",
      "(Iteration 1481 / 7640) loss: 0.788229\n",
      "(Iteration 1501 / 7640) loss: 0.750536\n",
      "(Iteration 1521 / 7640) loss: 0.816756\n",
      "Time elapsed (hh:mm:ss.ms) 0:36:57.502633\n",
      "(Epoch 4 / 20) train acc: 0.740000; val_acc: 0.643000\n",
      "(Iteration 1541 / 7640) loss: 0.674101\n",
      "(Iteration 1561 / 7640) loss: 1.001122\n",
      "(Iteration 1581 / 7640) loss: 0.834778\n",
      "(Iteration 1601 / 7640) loss: 0.843142\n",
      "(Iteration 1621 / 7640) loss: 0.922333\n",
      "(Iteration 1641 / 7640) loss: 0.855826\n",
      "(Iteration 1661 / 7640) loss: 0.809147\n",
      "(Iteration 1681 / 7640) loss: 1.048819\n",
      "(Iteration 1701 / 7640) loss: 0.853094\n",
      "(Iteration 1721 / 7640) loss: 0.914426\n",
      "(Iteration 1741 / 7640) loss: 0.751242\n",
      "(Iteration 1761 / 7640) loss: 0.821250\n",
      "(Iteration 1781 / 7640) loss: 0.788743\n",
      "(Iteration 1801 / 7640) loss: 0.719426\n",
      "(Iteration 1821 / 7640) loss: 0.699155\n",
      "(Iteration 1841 / 7640) loss: 0.803622\n",
      "(Iteration 1861 / 7640) loss: 0.763599\n",
      "(Iteration 1881 / 7640) loss: 0.897048\n",
      "(Iteration 1901 / 7640) loss: 0.660763\n",
      "Time elapsed (hh:mm:ss.ms) 0:45:59.692311\n",
      "(Epoch 5 / 20) train acc: 0.737000; val_acc: 0.650000\n",
      "(Iteration 1921 / 7640) loss: 0.781240\n",
      "(Iteration 1941 / 7640) loss: 0.888095\n",
      "(Iteration 1961 / 7640) loss: 0.800112\n",
      "(Iteration 1981 / 7640) loss: 0.832255\n",
      "(Iteration 2001 / 7640) loss: 0.747361\n",
      "(Iteration 2021 / 7640) loss: 0.893170\n",
      "(Iteration 2041 / 7640) loss: 0.729312\n",
      "(Iteration 2061 / 7640) loss: 0.768933\n",
      "(Iteration 2081 / 7640) loss: 0.785543\n",
      "(Iteration 2101 / 7640) loss: 0.901257\n",
      "(Iteration 2121 / 7640) loss: 0.792483\n",
      "(Iteration 2141 / 7640) loss: 0.791548\n",
      "(Iteration 2161 / 7640) loss: 0.651185\n",
      "(Iteration 2181 / 7640) loss: 0.766969\n",
      "(Iteration 2201 / 7640) loss: 0.627220\n",
      "(Iteration 2221 / 7640) loss: 0.839329\n",
      "(Iteration 2241 / 7640) loss: 0.803494\n",
      "(Iteration 2261 / 7640) loss: 0.554369\n",
      "(Iteration 2281 / 7640) loss: 0.698623\n",
      "Time elapsed (hh:mm:ss.ms) 0:55:02.880157\n",
      "(Epoch 6 / 20) train acc: 0.754000; val_acc: 0.657000\n",
      "(Iteration 2301 / 7640) loss: 0.830512\n",
      "(Iteration 2321 / 7640) loss: 0.668750\n",
      "(Iteration 2341 / 7640) loss: 0.659689\n",
      "(Iteration 2361 / 7640) loss: 0.814232\n",
      "(Iteration 2381 / 7640) loss: 0.671135\n",
      "(Iteration 2401 / 7640) loss: 0.710970\n",
      "(Iteration 2421 / 7640) loss: 0.650495\n",
      "(Iteration 2441 / 7640) loss: 0.737180\n",
      "(Iteration 2461 / 7640) loss: 0.963011\n",
      "(Iteration 2481 / 7640) loss: 0.763579\n",
      "(Iteration 2501 / 7640) loss: 0.641255\n",
      "(Iteration 2521 / 7640) loss: 0.700376\n",
      "(Iteration 2541 / 7640) loss: 0.769760\n",
      "(Iteration 2561 / 7640) loss: 0.629886\n",
      "(Iteration 2581 / 7640) loss: 0.627128\n",
      "(Iteration 2601 / 7640) loss: 0.762756\n",
      "(Iteration 2621 / 7640) loss: 0.640688\n",
      "(Iteration 2641 / 7640) loss: 0.669323\n",
      "(Iteration 2661 / 7640) loss: 0.563714\n",
      "Time elapsed (hh:mm:ss.ms) 1:04:05.286788\n",
      "(Epoch 7 / 20) train acc: 0.779000; val_acc: 0.673000\n",
      "(Iteration 2681 / 7640) loss: 0.573083\n",
      "(Iteration 2701 / 7640) loss: 0.714339\n",
      "(Iteration 2721 / 7640) loss: 0.703083\n",
      "(Iteration 2741 / 7640) loss: 0.636753\n",
      "(Iteration 2761 / 7640) loss: 0.664047\n",
      "(Iteration 2781 / 7640) loss: 0.546789\n",
      "(Iteration 2801 / 7640) loss: 0.710989\n",
      "(Iteration 2821 / 7640) loss: 0.711468\n",
      "(Iteration 2841 / 7640) loss: 0.797032\n",
      "(Iteration 2861 / 7640) loss: 0.609699\n",
      "(Iteration 2881 / 7640) loss: 0.594156\n",
      "(Iteration 2901 / 7640) loss: 0.568465\n",
      "(Iteration 2921 / 7640) loss: 0.804378\n",
      "(Iteration 2941 / 7640) loss: 0.729571\n",
      "(Iteration 2961 / 7640) loss: 0.560027\n",
      "(Iteration 2981 / 7640) loss: 0.571114\n",
      "(Iteration 3001 / 7640) loss: 0.694055\n",
      "(Iteration 3021 / 7640) loss: 0.542272\n",
      "(Iteration 3041 / 7640) loss: 0.623030\n",
      "Time elapsed (hh:mm:ss.ms) 1:13:07.690100\n",
      "(Epoch 8 / 20) train acc: 0.826000; val_acc: 0.666000\n",
      "(Iteration 3061 / 7640) loss: 0.462407\n",
      "(Iteration 3081 / 7640) loss: 0.589475\n",
      "(Iteration 3101 / 7640) loss: 0.690828\n",
      "(Iteration 3121 / 7640) loss: 0.702325\n",
      "(Iteration 3141 / 7640) loss: 0.590605\n",
      "(Iteration 3161 / 7640) loss: 0.592051\n",
      "(Iteration 3181 / 7640) loss: 0.664572\n",
      "(Iteration 3201 / 7640) loss: 0.516210\n",
      "(Iteration 3221 / 7640) loss: 0.716248\n",
      "(Iteration 3241 / 7640) loss: 0.625302\n",
      "(Iteration 3261 / 7640) loss: 0.576507\n",
      "(Iteration 3281 / 7640) loss: 0.518477\n",
      "(Iteration 3301 / 7640) loss: 0.492858\n",
      "(Iteration 3321 / 7640) loss: 0.516801\n",
      "(Iteration 3341 / 7640) loss: 0.597269\n",
      "(Iteration 3361 / 7640) loss: 0.667604\n",
      "(Iteration 3381 / 7640) loss: 0.557706\n",
      "(Iteration 3401 / 7640) loss: 0.560521\n",
      "(Iteration 3421 / 7640) loss: 0.570362\n",
      "Time elapsed (hh:mm:ss.ms) 1:22:08.454160\n",
      "(Epoch 9 / 20) train acc: 0.841000; val_acc: 0.676000\n",
      "(Iteration 3441 / 7640) loss: 0.533651\n",
      "(Iteration 3461 / 7640) loss: 0.551616\n",
      "(Iteration 3481 / 7640) loss: 0.565806\n",
      "(Iteration 3501 / 7640) loss: 0.585374\n",
      "(Iteration 3521 / 7640) loss: 0.509125\n",
      "(Iteration 3541 / 7640) loss: 0.576308\n",
      "(Iteration 3561 / 7640) loss: 0.552272\n",
      "(Iteration 3581 / 7640) loss: 0.430704\n",
      "(Iteration 3601 / 7640) loss: 0.570795\n",
      "(Iteration 3621 / 7640) loss: 0.554500\n",
      "(Iteration 3641 / 7640) loss: 0.400579\n",
      "(Iteration 3661 / 7640) loss: 0.751763\n",
      "(Iteration 3681 / 7640) loss: 0.542249\n",
      "(Iteration 3701 / 7640) loss: 0.555297\n",
      "(Iteration 3721 / 7640) loss: 0.495940\n",
      "(Iteration 3741 / 7640) loss: 0.554303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3761 / 7640) loss: 0.569754\n",
      "(Iteration 3781 / 7640) loss: 0.460337\n",
      "(Iteration 3801 / 7640) loss: 0.499832\n",
      "Time elapsed (hh:mm:ss.ms) 1:31:11.811401\n",
      "(Epoch 10 / 20) train acc: 0.846000; val_acc: 0.662000\n",
      "(Iteration 3821 / 7640) loss: 0.436619\n",
      "(Iteration 3841 / 7640) loss: 0.499267\n",
      "(Iteration 3861 / 7640) loss: 0.513160\n",
      "(Iteration 3881 / 7640) loss: 0.487710\n",
      "(Iteration 3901 / 7640) loss: 0.507237\n",
      "(Iteration 3921 / 7640) loss: 0.551950\n",
      "(Iteration 3941 / 7640) loss: 0.459395\n",
      "(Iteration 3961 / 7640) loss: 0.523669\n",
      "(Iteration 3981 / 7640) loss: 0.588350\n",
      "(Iteration 4001 / 7640) loss: 0.420532\n",
      "(Iteration 4021 / 7640) loss: 0.539644\n",
      "(Iteration 4041 / 7640) loss: 0.469096\n",
      "(Iteration 4061 / 7640) loss: 0.430399\n",
      "(Iteration 4081 / 7640) loss: 0.469698\n",
      "(Iteration 4101 / 7640) loss: 0.508369\n",
      "(Iteration 4121 / 7640) loss: 0.534931\n",
      "(Iteration 4141 / 7640) loss: 0.428397\n",
      "(Iteration 4161 / 7640) loss: 0.466053\n",
      "(Iteration 4181 / 7640) loss: 0.461474\n",
      "(Iteration 4201 / 7640) loss: 0.518552\n",
      "Time elapsed (hh:mm:ss.ms) 1:40:14.234805\n",
      "(Epoch 11 / 20) train acc: 0.890000; val_acc: 0.660000\n",
      "(Iteration 4221 / 7640) loss: 0.488884\n",
      "(Iteration 4241 / 7640) loss: 0.440574\n",
      "(Iteration 4261 / 7640) loss: 0.495650\n",
      "(Iteration 4281 / 7640) loss: 0.409990\n",
      "(Iteration 4301 / 7640) loss: 0.505126\n",
      "(Iteration 4321 / 7640) loss: 0.563181\n",
      "(Iteration 4341 / 7640) loss: 0.510813\n",
      "(Iteration 4361 / 7640) loss: 0.361161\n",
      "(Iteration 4381 / 7640) loss: 0.489237\n",
      "(Iteration 4401 / 7640) loss: 0.473052\n",
      "(Iteration 4421 / 7640) loss: 0.393602\n",
      "(Iteration 4441 / 7640) loss: 0.480284\n",
      "(Iteration 4461 / 7640) loss: 0.566201\n",
      "(Iteration 4481 / 7640) loss: 0.409388\n",
      "(Iteration 4501 / 7640) loss: 0.484791\n",
      "(Iteration 4521 / 7640) loss: 0.442572\n",
      "(Iteration 4541 / 7640) loss: 0.381625\n",
      "(Iteration 4561 / 7640) loss: 0.416247\n",
      "(Iteration 4581 / 7640) loss: 0.498005\n",
      "Time elapsed (hh:mm:ss.ms) 1:49:16.352544\n",
      "(Epoch 12 / 20) train acc: 0.907000; val_acc: 0.659000\n",
      "(Iteration 4601 / 7640) loss: 0.412690\n",
      "(Iteration 4621 / 7640) loss: 0.505710\n",
      "(Iteration 4641 / 7640) loss: 0.416765\n",
      "(Iteration 4661 / 7640) loss: 0.399071\n",
      "(Iteration 4681 / 7640) loss: 0.373699\n",
      "(Iteration 4701 / 7640) loss: 0.408927\n",
      "(Iteration 4721 / 7640) loss: 0.336844\n",
      "(Iteration 4741 / 7640) loss: 0.371845\n",
      "(Iteration 4761 / 7640) loss: 0.482714\n",
      "(Iteration 4781 / 7640) loss: 0.372648\n",
      "(Iteration 4801 / 7640) loss: 0.348033\n",
      "(Iteration 4821 / 7640) loss: 0.574579\n",
      "(Iteration 4841 / 7640) loss: 0.392539\n",
      "(Iteration 4861 / 7640) loss: 0.346922\n",
      "(Iteration 4881 / 7640) loss: 0.479080\n",
      "(Iteration 4901 / 7640) loss: 0.414822\n",
      "(Iteration 4921 / 7640) loss: 0.339367\n",
      "(Iteration 4941 / 7640) loss: 0.360703\n",
      "(Iteration 4961 / 7640) loss: 0.322818\n",
      "Time elapsed (hh:mm:ss.ms) 1:58:18.446078\n",
      "(Epoch 13 / 20) train acc: 0.917000; val_acc: 0.667000\n",
      "(Iteration 4981 / 7640) loss: 0.400214\n",
      "(Iteration 5001 / 7640) loss: 0.427005\n",
      "(Iteration 5021 / 7640) loss: 0.321790\n",
      "(Iteration 5041 / 7640) loss: 0.402930\n",
      "(Iteration 5061 / 7640) loss: 0.368290\n",
      "(Iteration 5081 / 7640) loss: 0.397645\n",
      "(Iteration 5101 / 7640) loss: 0.384888\n",
      "(Iteration 5121 / 7640) loss: 0.395826\n",
      "(Iteration 5141 / 7640) loss: 0.499378\n",
      "(Iteration 5161 / 7640) loss: 0.408986\n",
      "(Iteration 5181 / 7640) loss: 0.373296\n",
      "(Iteration 5201 / 7640) loss: 0.354889\n",
      "(Iteration 5221 / 7640) loss: 0.364680\n",
      "(Iteration 5241 / 7640) loss: 0.385583\n",
      "(Iteration 5261 / 7640) loss: 0.394998\n",
      "(Iteration 5281 / 7640) loss: 0.467551\n",
      "(Iteration 5301 / 7640) loss: 0.351525\n",
      "(Iteration 5321 / 7640) loss: 0.317937\n",
      "(Iteration 5341 / 7640) loss: 0.394359\n",
      "Time elapsed (hh:mm:ss.ms) 2:07:20.576189\n",
      "(Epoch 14 / 20) train acc: 0.929000; val_acc: 0.662000\n",
      "(Iteration 5361 / 7640) loss: 0.377249\n",
      "(Iteration 5381 / 7640) loss: 0.322382\n",
      "(Iteration 5401 / 7640) loss: 0.333418\n",
      "(Iteration 5421 / 7640) loss: 0.360050\n",
      "(Iteration 5441 / 7640) loss: 0.374599\n",
      "(Iteration 5461 / 7640) loss: 0.357853\n",
      "(Iteration 5481 / 7640) loss: 0.332935\n",
      "(Iteration 5501 / 7640) loss: 0.347422\n",
      "(Iteration 5521 / 7640) loss: 0.450308\n",
      "(Iteration 5541 / 7640) loss: 0.378444\n",
      "(Iteration 5561 / 7640) loss: 0.421684\n",
      "(Iteration 5581 / 7640) loss: 0.329431\n",
      "(Iteration 5601 / 7640) loss: 0.302924\n",
      "(Iteration 5621 / 7640) loss: 0.278973\n",
      "(Iteration 5641 / 7640) loss: 0.342500\n",
      "(Iteration 5661 / 7640) loss: 0.367228\n",
      "(Iteration 5681 / 7640) loss: 0.282385\n",
      "(Iteration 5701 / 7640) loss: 0.368548\n",
      "(Iteration 5721 / 7640) loss: 0.254227\n",
      "Time elapsed (hh:mm:ss.ms) 2:16:22.180328\n",
      "(Epoch 15 / 20) train acc: 0.948000; val_acc: 0.645000\n",
      "(Iteration 5741 / 7640) loss: 0.336750\n",
      "(Iteration 5761 / 7640) loss: 0.372965\n",
      "(Iteration 5781 / 7640) loss: 0.283321\n",
      "(Iteration 5801 / 7640) loss: 0.315746\n",
      "(Iteration 5821 / 7640) loss: 0.303181\n",
      "(Iteration 5841 / 7640) loss: 0.259149\n",
      "(Iteration 5861 / 7640) loss: 0.357052\n",
      "(Iteration 5881 / 7640) loss: 0.318910\n",
      "(Iteration 5901 / 7640) loss: 0.258232\n",
      "(Iteration 5921 / 7640) loss: 0.360528\n",
      "(Iteration 5941 / 7640) loss: 0.300183\n",
      "(Iteration 5961 / 7640) loss: 0.298320\n",
      "(Iteration 5981 / 7640) loss: 0.383560\n",
      "(Iteration 6001 / 7640) loss: 0.275869\n",
      "(Iteration 6021 / 7640) loss: 0.316257\n",
      "(Iteration 6041 / 7640) loss: 0.243912\n",
      "(Iteration 6061 / 7640) loss: 0.289389\n",
      "(Iteration 6081 / 7640) loss: 0.294654\n",
      "(Iteration 6101 / 7640) loss: 0.283562\n",
      "Time elapsed (hh:mm:ss.ms) 2:25:25.144020\n",
      "(Epoch 16 / 20) train acc: 0.959000; val_acc: 0.665000\n",
      "(Iteration 6121 / 7640) loss: 0.328699\n",
      "(Iteration 6141 / 7640) loss: 0.349997\n",
      "(Iteration 6161 / 7640) loss: 0.308502\n",
      "(Iteration 6181 / 7640) loss: 0.258175\n",
      "(Iteration 6201 / 7640) loss: 0.401022\n",
      "(Iteration 6221 / 7640) loss: 0.286359\n",
      "(Iteration 6241 / 7640) loss: 0.244958\n",
      "(Iteration 6261 / 7640) loss: 0.287536\n",
      "(Iteration 6281 / 7640) loss: 0.283372\n",
      "(Iteration 6301 / 7640) loss: 0.298527\n",
      "(Iteration 6321 / 7640) loss: 0.248418\n",
      "(Iteration 6341 / 7640) loss: 0.296858\n",
      "(Iteration 6361 / 7640) loss: 0.230119\n",
      "(Iteration 6381 / 7640) loss: 0.279291\n",
      "(Iteration 6401 / 7640) loss: 0.292346\n",
      "(Iteration 6421 / 7640) loss: 0.225363\n",
      "(Iteration 6441 / 7640) loss: 0.281019\n",
      "(Iteration 6461 / 7640) loss: 0.287210\n",
      "(Iteration 6481 / 7640) loss: 0.287281\n",
      "Time elapsed (hh:mm:ss.ms) 2:34:27.028344\n",
      "(Epoch 17 / 20) train acc: 0.961000; val_acc: 0.660000\n",
      "(Iteration 6501 / 7640) loss: 0.334356\n",
      "(Iteration 6521 / 7640) loss: 0.250693\n",
      "(Iteration 6541 / 7640) loss: 0.303360\n",
      "(Iteration 6561 / 7640) loss: 0.245753\n",
      "(Iteration 6581 / 7640) loss: 0.294540\n",
      "(Iteration 6601 / 7640) loss: 0.269571\n",
      "(Iteration 6621 / 7640) loss: 0.290874\n",
      "(Iteration 6641 / 7640) loss: 0.286722\n",
      "(Iteration 6661 / 7640) loss: 0.237281\n",
      "(Iteration 6681 / 7640) loss: 0.264769\n",
      "(Iteration 6701 / 7640) loss: 0.254681\n",
      "(Iteration 6721 / 7640) loss: 0.245973\n",
      "(Iteration 6741 / 7640) loss: 0.305989\n",
      "(Iteration 6761 / 7640) loss: 0.245697\n",
      "(Iteration 6781 / 7640) loss: 0.292817\n",
      "(Iteration 6801 / 7640) loss: 0.245631\n",
      "(Iteration 6821 / 7640) loss: 0.281190\n",
      "(Iteration 6841 / 7640) loss: 0.192353\n",
      "(Iteration 6861 / 7640) loss: 0.302399\n",
      "Time elapsed (hh:mm:ss.ms) 2:43:32.379794\n",
      "(Epoch 18 / 20) train acc: 0.975000; val_acc: 0.655000\n",
      "(Iteration 6881 / 7640) loss: 0.230702\n",
      "(Iteration 6901 / 7640) loss: 0.249022\n",
      "(Iteration 6921 / 7640) loss: 0.228892\n",
      "(Iteration 6941 / 7640) loss: 0.231102\n",
      "(Iteration 6961 / 7640) loss: 0.344255\n",
      "(Iteration 6981 / 7640) loss: 0.227303\n",
      "(Iteration 7001 / 7640) loss: 0.259072\n",
      "(Iteration 7021 / 7640) loss: 0.209174\n",
      "(Iteration 7041 / 7640) loss: 0.244459\n",
      "(Iteration 7061 / 7640) loss: 0.247363\n",
      "(Iteration 7081 / 7640) loss: 0.212551\n",
      "(Iteration 7101 / 7640) loss: 0.220064\n",
      "(Iteration 7121 / 7640) loss: 0.255035\n",
      "(Iteration 7141 / 7640) loss: 0.221431\n",
      "(Iteration 7161 / 7640) loss: 0.225988\n",
      "(Iteration 7181 / 7640) loss: 0.297327\n",
      "(Iteration 7201 / 7640) loss: 0.247384\n",
      "(Iteration 7221 / 7640) loss: 0.205640\n",
      "(Iteration 7241 / 7640) loss: 0.260835\n",
      "Time elapsed (hh:mm:ss.ms) 2:52:26.596581\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.658000\n",
      "(Iteration 7261 / 7640) loss: 0.212813\n",
      "(Iteration 7281 / 7640) loss: 0.219917\n",
      "(Iteration 7301 / 7640) loss: 0.238152\n",
      "(Iteration 7321 / 7640) loss: 0.241584\n",
      "(Iteration 7341 / 7640) loss: 0.217775\n",
      "(Iteration 7361 / 7640) loss: 0.227811\n",
      "(Iteration 7381 / 7640) loss: 0.210802\n",
      "(Iteration 7401 / 7640) loss: 0.315000\n",
      "(Iteration 7421 / 7640) loss: 0.210051\n",
      "(Iteration 7441 / 7640) loss: 0.198338\n",
      "(Iteration 7461 / 7640) loss: 0.184853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7481 / 7640) loss: 0.258293\n",
      "(Iteration 7501 / 7640) loss: 0.245284\n",
      "(Iteration 7521 / 7640) loss: 0.249017\n",
      "(Iteration 7541 / 7640) loss: 0.195184\n",
      "(Iteration 7561 / 7640) loss: 0.229640\n",
      "(Iteration 7581 / 7640) loss: 0.186414\n",
      "(Iteration 7601 / 7640) loss: 0.205952\n",
      "(Iteration 7621 / 7640) loss: 0.247427\n",
      "Time elapsed (hh:mm:ss.ms) 3:01:01.378928\n",
      "(Epoch 20 / 20) train acc: 0.982000; val_acc: 0.673000\n"
     ]
    }
   ],
   "source": [
    "# Train a really good model on CIFAR-10\n",
    "from deeplearning.classifiers.convnet1_he import *\n",
    "model = HaoConvNet1(hidden_dim=256, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=128,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
