{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and Attention\n",
    "In this notebook, we will implement a model for neural machine translation (NMT) with attention. This notebook is adapted from the [TensorFlow tutorial on NMT](https://www.tensorflow.org/tutorials/seq2seq) at  as well as the [TensorFlow NMT package](https://github.com/tensorflow/nmt/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper TensorFlow functions\n",
    "from utils import maybe_download\n",
    "\n",
    "# The encoder-decoder architecture\n",
    "from nmt.model import AttentionalModel, LSTMCell\n",
    "from nmt.utils import vocab_utils\n",
    "from nmt.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll train our model on a small-scale dataset: an English-Vietnamese parallel corpus of TED talks (133K sentence pairs) provided by the IWSLT Evaluation Campaign (https://sites.google.com/site/iwsltevaluation2015/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.en...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/train.en\n",
      "Downloading train.vi...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/train.vi\n",
      "Downloading tst2012.en...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/tst2012.en\n",
      "Downloading tst2012.vi...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/tst2012.vi\n",
      "Downloading tst2013.en...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/tst2013.en\n",
      "Downloading tst2013.vi...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/tst2013.vi\n",
      "Downloading vocab.en...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/vocab.en\n",
      "Downloading vocab.vi...\n",
      "Finished!\n",
      "Found and verified datasets/nmt_data_vi/vocab.vi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'datasets/nmt_data_vi/vocab.vi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "site_prefix = \"https://nlp.stanford.edu/projects/nmt/data/\"\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.en', out_dir, 13603614)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.vi', out_dir, 18074646)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.en', out_dir, 140250)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.vi', out_dir, 188396)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.en', out_dir, 132264)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.vi', out_dir, 183855)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.en', out_dir, 139741)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.vi', out_dir, 46767)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NMT\n",
    "\n",
    "<figure>\n",
    "    <img src='images/encdec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 1.** Example of a general, *encoder-decoder* approach to NMT. An encoder converts a source sentence into a representation which is passed through a decoder to produce a translation</figcaption>\n",
    "</figure>\n",
    "\n",
    "A neural machine translation (NMT) system reads in a source sentence using an *encoder*, and then uses a *decoder* to emit a translation. NMT models vary in terms of their exact architectures. A natural choice for sequential data is the recurrent neural network (RNN). Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: (a) directionality – unidirectional or bidirectional (whether they read the source sentence in forwards or forwards and backwards); (b) depth – single- or multi-layer; and (c) type – often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU).\n",
    "\n",
    "We will consider a deep multi-layer RNN which is bi-directional (it reads the input sequence both forwards and backwards) and uses LSTM units with attention. At a high level, the NMT model consists of two recurrent neural networks: the encoder recurrent network simply consumes the input source words without making any prediction; the decoder, on the other hand, processes the target sentence while predicting the next words.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/seq2seq.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 2.** Example of a neural machine translation system for translating a source sentence \"I am a student\" into a target sentence \"Je suis étudiant\".  Here, $<s>$ marks the start of the decoding process while $</s>$ tells the decoder to stop.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "At the bottom layer, the encoder and decoder recurrent networks receive as input the following: first, the source sentence, then a boundary marker $</s>$ which indicates the transition from the encoding to the decoding mode, and the target sentence. We now go into the details of how the model deals with source and target sentences.\n",
    "\n",
    "### Embedding\n",
    "Given the categorical nature of words, the model must first look up the source and target embeddings to retrieve the corresponding word representations. For this embedding layer to work, a vocabulary is first chosen for each language. Usually, a vocabulary size $V$ is selected, and only the most frequent $V$ words in the corpus are treated as unique. All other words are converted to an \"unknown\" token $<$UNK$>$ and all get the same embedding. The embedding weights, one set per language, are usually learned during training (but pretrained word embeddings may be used instead).\n",
    "\n",
    "### Encoder\n",
    "Once retrieved, the word embeddings are then fed as input into the main network, which consists of two multi-layer recurrent neural networks -- an encoder for the source language and a decoder for the target language. These two networks, in principle, can share the same weights; however, in practice, we often use two different sets of parameters (such models do a better job when fitting large training datasets). The encoder uses zero vectors as its starting states (before it sees the source sequence). In TensorFlow:\n",
    "\n",
    "    # Build RNN cell\n",
    "    encoder_cell = YourEncoderRNNCell(num_units)\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outputs: [max_time, batch_size, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_emb_inp,\n",
    "        sequence_length=source_sequence_length, time_major=True)\n",
    "\n",
    "### Decoder\n",
    "The decoder also needs to have access to the source information, and one simple way to achieve that is to initialize it with the last hidden state of the encoder, `encoder_state`. In Figure 2, we pass the hidden state at the source word \"student\" to the decoder side.\n",
    "\n",
    "    # Build RNN cell\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    # Helper\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        decoder_emb_inp, decoder_lengths, time_major=True)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "    \n",
    "    # Dynamic decoding\n",
    "    outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "    logits = outputs.rnn_output\n",
    "\n",
    "### Loss\n",
    "Given the logits above, we are now ready to compute the training loss:\n",
    "\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size)\n",
    "\n",
    "Here, target_weights is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0.\n",
    "\n",
    "Important note: It's worth pointing out that we should divide the loss by `batch_size`, so our hyperparameters are \"invariant\" to `batch_size`. Some people divide the loss by (`batch_size * num_time_steps`), which plays down the errors made on short sentences. More subtly, the same hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of `1.0`, the latter approach effectively uses a much smaller learning rate of `1 / num_time_steps`.\n",
    "\n",
    "### How to generate translations at test time\n",
    "\n",
    "While you're training your NMT models (and once you have trained models), you can obtain translations given previously unseen source sentences. At test time, we only have access to the source sentence; i.e., `encoder_inputs`. There are many ways to perform decoding given those inputs. Decoding methods include greedy, sampling, and beam-search decoding. Here, we will discuss the greedy decoding strategy.\n",
    "\n",
    "The idea is simple and illustrated in Figure 3:\n",
    "\n",
    "1. We still encode the source sentence in the same way as during training to obtain an `encoder_state`, and this `encoder_state` is used to initialize the decoder.\n",
    "\n",
    "2. The decoding (translation) process is started as soon as the decoder receives a starting symbol $<$/s$>$.\n",
    "\n",
    "3. For each timestep on the decoder side, we treat the recurrent network's output as a set of logits. We choose the most likely word, the id associated with the maximum logit value, as the emitted word (this is the \"greedy\" behavior). For example in Figure 3, the word \"moi\" has the highest translation probability in the first decoding step. We then feed this word as input to the next timestep. (At training time, however, we may feed in the true target as input to the next timestep in a process called *teacher forcing*.)\n",
    "\n",
    "4. The process continues until the end-of-sentence marker $<$/s$>$ is produced as an output symbol.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/greedy_dec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 3.** Example of how a trained NMT model produces a translation for a source sentence \"Je suis étudiant\" using greedy search.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Attention\n",
    "\n",
    "The attention mechanism was first introduced by Bahdanau et al., 2015 [1] and then later refined by Luong et al., 2015 [2] and others. The key idea of the attention mechanism is to establish direct short-cut connections between the target and the source by paying \"attention\" to relevant source content as we translate (produce output tokens). A nice byproduct of the attention mechanism is an easy-to-visualize alignment matrix between the source and target sentences that we will visualize at the end of this notebook.\n",
    " \n",
    "Remember that in a vanilla seq2seq model, we pass the last source state $h_{s_{T_s}}$ from the encoder to the decoder when starting the decoding process. This works well for short and medium-length sentences; however, for long sentences, the single fixed-size hidden state becomes an information bottleneck. Instead of discarding all of the hidden states computed in the source RNN, the attention mechanism provides an approach that allows the decoder to peek at them (treating them as a dynamic memory of the source information). By doing so, the attention mechanism improves the translation of longer sentences. Nowadays, attention mechanisms are the *de facto* standard and have been successfully applied to many other tasks (including image caption generation, speech recognition, and text summarization).\n",
    "\n",
    "<figure>\n",
    "    <img src='images/att.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 4.** Example of an attention-based NMT system with the first step of the attention computation in detail. For clarity, the embedding and projection layers are omitted.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "### How do we actually attend over the input sequence?\n",
    "\n",
    "There are many different ways of formalizing attention. These variants depend on the form of a *scoring* function and an *attention* function (and on whether the previous state of the decoder $h_{t_{i-1}}$ is used instead of $h_{t_{i}}$ in the scoring function as originally suggested in Bahdanau et al. (2015); **we will stick to using $h_{t_{i}}$** in this notebook). Luong et al. (2015) demonstrate that only a few choices actually matter:\n",
    "\n",
    "1. First, the basic form of attention, i.e., **direct connections between target and source**, needs to be present. \n",
    "\n",
    "2. Second, it's important to **feed the attention vector to the next timestep** to inform the network about past attention decisions.\n",
    "\n",
    "3. Lastly, **choices of the scoring function** can often result in different performance. See Luong et al. (2015) for further details.\n",
    "\n",
    "### A general framework for computing attention\n",
    "\n",
    "The attention computation happens at every decoder time step. It consists of the following stages:\n",
    "\n",
    "1. The current target (encoder) hidden state $h_{t_i}$ is compared with all source (decoder) states $h_{s_j}$ to derive *attention weights* $\\alpha_{ij}$.\n",
    "2. Based on the attention weights we compute a *context vector* $c_{i}$ as the weighted average of the source states.\n",
    "3. We combine the context vector $c_{i}$ with the current target hidden state $h_{s_j}$ to yield the final *attention vector* $a_t$.\n",
    "4. The attention vector $a_i$ is fed as an input to the next time step (*input feeding*). \n",
    "\n",
    "The first three steps can be summarized by the equations below:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\alpha_{ij} &= \\frac{\n",
    "    \\exp(\\text{score}(h_{t_i}, h_{s_j}))\n",
    "}{\n",
    "    \\sum_{k=1}^{T_s}{\\exp(\\text{score}(h_{t_i}, h_{s_k}))}\n",
    "} \\tag{attention weights} \\\\\\\\\n",
    "c_{i} &= \\sum_{j=1}^{T_s} \\alpha_{ij} h_{s_j} \\tag{context vector} \\\\\\\\\n",
    "a_{i} &= f(c_{i}, h_{t_i}) \\tag{attention vector} \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, the function `score` is used to compare the target hidden state $h_{t_i}$ with each of the source hidden states $h_{s_j}$, and the result is normalized over the source timesteps $j = 1, \\dots, T_s$ to produce attention weights $\\alpha_{ij}$ (which define a distribution over source positions $j$ for a given source timestep $i$). (There are various choices of the scoring function; we will consider three below.) Note that we make use of the current decoder (or *target*) hidden state $h_{t_i}$, which is computed as a function of the previous hidden state $h_{t_{i-1}}$, the embedding of the input token $x_{i}$ (which is either the emission or the ground truth token from the previous timestep) using the standard formula for a recurrent cell. Optionally, in the case of *input feeding*, we combine $h_{t_{i-1}}$ with the context vector from the previous timestep, $c_{t_{i-1}}$ (which may require a change in the size of the kernel matrix, depending on how the combination is implemented). The encoder (or *source*) hidden states $h_{s_j}$ for $j=1, \\dots T_s$ are similarly the standard hidden state for a recurrent cell.\n",
    "\n",
    "We can also vectorize the computation of the context vector $c_i$ for every target timestep as follows: Given the source hidden states $h_{s_1}, \\dots, h_{s_{T_s}}$, we construct a matrix $H_s$ of size `hidden_size` $\\times$ `input_seq_len` by stacking the source hidden states into columns. Attention allows us to dynamically weight certain timesteps of the input sequence in a fixed size vector $c_i$ by taking a convex combination of the columns of $H_s$. In particular, we calculate a nonzero and normalized attention weight vector $\\vec{\\alpha}_i = [\\alpha_{i1}, \\dots, \\alpha_{iT_s}]^T$ that weights the source hidden states in the computation\n",
    "\n",
    "$$\\large c_i = H_s\\vec{\\alpha}_i~.$$\n",
    "\n",
    "\n",
    "\n",
    "The attention vector $a_i$ is used to derive the softmax logits and thereafter the loss by transformation under a function $f$.The function $f$ is commonly the a concatenation followed by $\\tanh$ layer:\n",
    "\n",
    "$$\\large a_{i} = \\tanh(W_a[c_i; h_{t_i}])$$\n",
    "\n",
    "but could take other forms. We then compute the predictive distribution over output tokens as\n",
    "\n",
    "$$\\large p(y_i \\mid y_1, \\dots y_{i-1}, x_i) = \\text{softmax}(W_s a_{i})~.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. LSTM cell with attention (8 pts)\n",
    "\n",
    "In the block below, you will implement the method `call`, which computes a single step of an LSTM cell using a method `attention` that computes an attention vector with some score function, as described above. **Complete the skeleton below**; assume inputs is already the input embedding (i.e., there is no need to construct an embedding matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCellWithAttention(LSTMCell):\n",
    "    \n",
    "    def __init__(self, num_units, memory):\n",
    "        super(LSTMCellWithAttention, self).__init__(num_units)\n",
    "        self.memory = memory\n",
    "        \n",
    "    def attention(self):\n",
    "        raise NotImplementedError(\"The subclass must implement this method!\")\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Run this LSTM cell with attention on inputs, conditional on state.\"\"\"\n",
    "        \n",
    "        # Cell and hidden states of the LSTM\n",
    "        c, h = state\n",
    "        \n",
    "        # Source (encoder) states to attend over\n",
    "        source_states = self.memory\n",
    "        \n",
    "        # Cell activation (e.g., tanh, relu, etc.)\n",
    "        activation = self._activation\n",
    "        \n",
    "        # LSTM cell parameters\n",
    "        kernel = self._kernel\n",
    "        bias = self._bias\n",
    "        forget_bias = self._forget_bias\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Need to implement an LSTM cell with \"\n",
    "                                  \"attention.\")\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        ### Your code should compute attention vector, new_c and new_h\n",
    "\n",
    "        # Adhering to convention\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "    \n",
    "        return attention_vector, new_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a \"dummy\" version of attention in order to test that the LSTM cell step function is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCellWithDummyAttention(LSTMCellWithAttention):\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Just return the target state so that the update becomes the vanilla\n",
    "        LSTM update.\"\"\"\n",
    "        return target_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2A. Dot-product Attention (8 pts)\n",
    "\n",
    "We first consider the simplest version of attention, which simply calculates the similarity between $h_{t_i}$ and $h_{s_j}$ by computing their dot product:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T}\\, h_{s_j}~.\n",
    "\\end{align*}$$\n",
    "\n",
    "This computation has no additional parameters, but it limits the expressivity of the model since its forces the input and output encodings to be close in order to have high score.\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using dot-product attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCellWithDotProductAttention(LSTMCellWithAttention):\n",
    "        \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithDotProductAttention, self).build(inputs_shape)\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Need to implement dot-product attention.\")\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2B. Bilinear Attention (8 pts)\n",
    "\n",
    "To make the score function more expressive, we may consider using a bilinear function of the form\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T} W_\\text{att} h_{s_j}~,\n",
    "\\end{align*}$$\n",
    "\n",
    "which transforms the source encoding $h_{s_j}$ by a linear transformation parameterized by $W_\\text{att}$ before taking the dot product. This formulation adds additional parameters that must be learned, but increases expressivity and also allows the source and target encodings to be of different dimensionality (if we so wish).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using bilinear attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn`or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCellWithBilinearAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithBilinearAttention, self).build(inputs_shape)\n",
    "        self._W_att = self.add_variable(\"W_att\", \n",
    "                                        shape=[self._num_units, \n",
    "                                               self._num_units])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Need to implement bilinear attention.\"\n",
    "                                  \"using the weight matrix self._W_att_1.\")\n",
    "       \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2C. Feedforward Attention (8 pts)\n",
    "\n",
    "Instead of simply using a linear transformation, why don't we use an even more expressive feedforward neural network to compute the score?\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=W_{\\text{att}_2} \\tanh( W_{\\text{att}_1} [h_{t_i}; h_{s_j}])~,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $[v_1; v_2]$ denotes a concatenation of the vectors $v_1$ and $v_2$, and $W_{\\text{att}_1}$ and $W_{\\text{att}_2}$ are learned parameter matrices. The feedforward approach typically has fewer parameters (depending on the size of the hidden layer) than the bilinear attention mechanism (which requires `source_embedding_dim` $\\times$ `target_embedding_dim` parameters).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using feedforward attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCellWithFeedForwardAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithFeedForwardAttention, self).build(inputs_shape)\n",
    "\n",
    "        self._W_att_1 = self.add_variable(\"W_att_1\", \n",
    "                                          shape=[self._num_units + self._num_units, \n",
    "                                                 self._num_units])\n",
    "        self._W_att_2 = self.add_variable(\"W_att_2\", \n",
    "                                          shape=[self._num_units, 1])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "        \n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Need to implement feedforward attention \"\n",
    "                                  \"using the weight matrices self._W_att_1 \"\n",
    "                                  \"and self._W_att_2.\")\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter settings\n",
    "\n",
    "You may find it useful to tune some of these parameters (but not necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_standard_hparams(data_path, out_dir):\n",
    "    \n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        \n",
    "        # Data\n",
    "        src=\"vi\",\n",
    "        tgt=\"en\",\n",
    "        train_prefix=os.path.join(data_path, \"train\"),\n",
    "        dev_prefix=os.path.join(data_path, \"tst2012\"),\n",
    "        test_prefix=os.path.join(data_path, \"tst2013\"),\n",
    "        vocab_prefix=\"\",\n",
    "        embed_prefix=\"\",\n",
    "        out_dir=out_dir,\n",
    "        src_vocab_file=os.path.join(data_path, \"vocab.vi\"),\n",
    "        tgt_vocab_file=os.path.join(data_path, \"vocab.en\"),\n",
    "        src_embed_file=\"\",\n",
    "        tgt_embed_file=\"\",\n",
    "        src_file=os.path.join(data_path, \"train.vi\"),\n",
    "        tgt_file=os.path.join(data_path, \"train.en\"),\n",
    "        dev_src_file=os.path.join(data_path, \"tst2012.vi\"),\n",
    "        dev_tgt_file=os.path.join(data_path, \"tst2012.en\"),\n",
    "        test_src_file=os.path.join(data_path, \"tst2013.vi\"),\n",
    "        test_tgt_file=os.path.join(data_path, \"tst2013.en\"),\n",
    "\n",
    "        # Networks\n",
    "        num_units=512,\n",
    "        num_layers=1,\n",
    "        num_encoder_layers=1,\n",
    "        num_decoder_layers=1,\n",
    "        num_encoder_residual_layers=0,\n",
    "        num_decoder_residual_layers=0,\n",
    "        dropout=0.2,\n",
    "        unit_type=\"lstm\",\n",
    "        encoder_type=\"uni\",\n",
    "        residual=False,\n",
    "        time_major=True,\n",
    "        num_embeddings_partitions=0,\n",
    "\n",
    "        # Train\n",
    "        optimizer=\"adam\",\n",
    "        batch_size=128,\n",
    "        init_op=\"uniform\",\n",
    "        init_weight=0.1,\n",
    "        max_gradient_norm=100.0,\n",
    "        learning_rate=0.001,\n",
    "        warmup_steps=0,\n",
    "        warmup_scheme=\"t2t\",\n",
    "        decay_scheme=\"luong234\",\n",
    "        colocate_gradients_with_ops=True,\n",
    "        num_train_steps=12000,\n",
    "\n",
    "        # Data constraints\n",
    "        num_buckets=5,\n",
    "        max_train=0,\n",
    "        src_max_len=25,\n",
    "        tgt_max_len=25,\n",
    "        src_max_len_infer=0,\n",
    "        tgt_max_len_infer=0,\n",
    "\n",
    "        # Data format\n",
    "        sos=\"<s>\",\n",
    "        eos=\"</s>\",\n",
    "        subword_option=\"\",\n",
    "        check_special_token=True,\n",
    "\n",
    "        # Misc\n",
    "        forget_bias=1.0,\n",
    "        num_gpus=1,\n",
    "        epoch_step=0,  # record where we were within an epoch.\n",
    "        steps_per_stats=100,\n",
    "        steps_per_external_eval=0,\n",
    "        share_vocab=False,\n",
    "        metrics=[\"bleu\"],\n",
    "        log_device_placement=False,\n",
    "        random_seed=None,\n",
    "        # only enable beam search during inference when beam_width > 0.\n",
    "        beam_width=0,\n",
    "        length_penalty_weight=0.0,\n",
    "        override_loaded_hparams=True,\n",
    "        num_keep_ckpts=5,\n",
    "        avg_ckpts=False,\n",
    "        num_intra_threads=0,\n",
    "        num_inter_threads=0,\n",
    "\n",
    "        # For inference\n",
    "        inference_indices=None,\n",
    "        infer_batch_size=32,\n",
    "        sampling_temperature=0.0,\n",
    "        num_translations_per_input=1,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    src_vocab_size, _ = vocab_utils.check_vocab(hparams.src_vocab_file, hparams.out_dir)\n",
    "    tgt_vocab_size, _ = vocab_utils.check_vocab(hparams.tgt_vocab_file, hparams.out_dir)\n",
    "    hparams.add_hparam('src_vocab_size', src_vocab_size)\n",
    "    hparams.add_hparam('tgt_vocab_size', tgt_vocab_size)\n",
    "    \n",
    "    out_dir = hparams.out_dir\n",
    "    if not tf.gfile.Exists(out_dir):\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "         \n",
    "    for metric in hparams.metrics:\n",
    "        hparams.add_hparam(\"best_\" + metric, 0)  # larger is better\n",
    "        best_metric_dir = os.path.join(hparams.out_dir, \"best_\" + metric)\n",
    "        hparams.add_hparam(\"best_\" + metric + \"_dir\", best_metric_dir)\n",
    "        tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "        if hparams.avg_ckpts:\n",
    "            hparams.add_hparam(\"avg_best_\" + metric, 0)  # larger is better\n",
    "            best_metric_dir = os.path.join(hparams.out_dir, \"avg_best_\" + metric)\n",
    "            hparams.add_hparam(\"avg_best_\" + metric + \"_dir\", best_metric_dir)\n",
    "            tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Training (8 pts)\n",
    "\n",
    "For this question, **train at least two of the models that use the attention modules you defined above**. Did you notice any difference in the training or evaluation of the different models? **Provide a brief written answer below.**\n",
    "\n",
    "*Note*: Make sure you **remove the model checkpoints** in the appropriate folders (`nmt_model_dotprod_att`, `nmt_model_binlinear_att` or `nmt_model_feedforward_att`)  if you would like to start training from scratch. (It's safe to delete all the files saved in the directory, or move them elsewhere.) Otherwise, the saved parameters will automatically be reloaded from the latest checkpoint and training will resume where it left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your written answer here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If desired as a baseline, train a vanilla LSTM model without attention\n",
    "hparams = create_standard_hparams(\n",
    "    data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "    out_dir=\"nmt_model_noatt\"\n",
    ")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDummyAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train an LSTM model with dot-product attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_dotprodatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDotProductAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train an LSTM model with bilinear attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"),\n",
    "                                  out_dir=\"nmt_model_bilinearatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithBilinearAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train an LSTM model with feedforward attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_ffatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithFeedForwardAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
