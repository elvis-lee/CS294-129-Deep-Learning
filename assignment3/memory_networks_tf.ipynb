{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Networks\n",
    "\n",
    "So far, we have covered two components that have enable neural network models to solve textual tasks: **word embeddings** and **attention**. In this notebook, we will be introducing a **memory** component, which is relevant to cases in which long-term memory is needed, such as answering questions about a sequence of events.\n",
    "\n",
    "While RNNs do have a memory component in the form of a hidden state, this often does not sufficiently capture long-term dependencies, as such long-term information must be condensed into a single dense vector representation. Memory networks were designed to overcome this information bottleneck.\n",
    "\n",
    "In this notebook, we will build the base model for an end-to-end trainable memory network ([\"End-To-End Memory Networks\" (Sukhbaatar et al.)](https://arxiv.org/pdf/1503.08895.pdf].))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper TensorFlow functions\n",
    "from utils import get_session, maybe_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "You will evaluate your results on Facebook's bAbi dataset, which assesses performance on 20 different question answering tasks for reasoning over text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading babi_tasks_1-20_v1-2.tar.gz...\n",
      "Finished!\n",
      "Found and verified datasets/babi_tasks_1-20_v1-2.tar.gz\n",
      "Some housekeeping...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Get Facebook's bAbi dataset\n",
    "from memn2n.babi_utils import get_babi_en\n",
    "\n",
    "get_babi_en()\n",
    "\n",
    "# For 10K dataset, uncomment below:\n",
    "# get_babi_en(get_10k=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemN2N\n",
    "\n",
    "The base MemN2N model has the following components:\n",
    "\n",
    "## Input Map\n",
    "\n",
    "First, we will need to convert our data, which come in the form of **stories** (a list of facts, typically a description of events; e.g.,\n",
    "*joe go playground; bob go office; joe get football*), the **query** being asked about our story (e.g. *where is joe?*) and its respective **answer** into an internal feature representation.\n",
    "\n",
    "Here, we used an input map that assigns a unique ID to each word in the vocabulary of the stories and queries (built from words of the story and query of the test and training sets). The answer is one-hot encoded.\n",
    "\n",
    "As sentences vary in length, we pad sentences with a null symbol so they are padded to the same size. The value of the null embedding was chosen to be 0.\n",
    "\n",
    "You can find more details about this step in the `get_data_info` function of `memn2n/data_utils.py`.\n",
    "\n",
    "## Sentence Representation\n",
    "\n",
    "To represent the positions of words within a sentence, we will adopt positional encodings (PE) to allow the order of words to impact our memories. It takes the form:\n",
    "\n",
    "$$m_i = \\sum_{j}l_j \\cdot Ax_{ij}$$\n",
    "\n",
    "where $\\cdot$ is an element wise multiplication, and $l_j$ is a column vector with the following elements:\n",
    "\n",
    "$$l_{kj} = (1 - j / J) - (k/d)(1 - 2j/J)$$\n",
    "\n",
    "where $J$ is the number of words in the sentence and $d$ is the dimension of the embedding.\n",
    "\n",
    "## Q1A. Implement Position Encoding\n",
    "\n",
    "Open `memn2n/memn2n_skeleton.py` and fill in the `position_encoding` function.\n",
    "\n",
    "## Input Memory Representation\n",
    "\n",
    "Our inputs, $x_1,...,x_i$, for stories, queries, and answers need to be stored in memory, represented by memory vectors $m_1,...,m_i$ of dimension $d$. To accomplish this, we will use an embedding matrix A (of size $d \\times V$). We will use embedding matrix $B$ to represent the queries into an internal state $u$. \n",
    "\n",
    "The match between $u$ and each $m_i$ is computed by taking the inner product and a softmax:\n",
    "\n",
    "$$p_i = \\text{softmax}(u^Tm_i)$$\n",
    "\n",
    "## Output Memory Representation\n",
    "\n",
    "Each $x_i$ has a corresponding output vector $c_i$ which is given by another embedding matrix, $C$. The response from the memory $o$ is computed by taking the sum over the transformed inputs $c_i$ weighted by the probability vector from the input:\n",
    "\n",
    "$$o = \\sum_{i}p_ic_i$$\n",
    "\n",
    "# Generating Predictions\n",
    "\n",
    "In a single layer memory network, the sum of the output vector $o$ and input embedding $u$ is transformed by a weight matrix $W$ and passed through a softmax to generate a predicted answer.\n",
    "\n",
    "$$\\hat{a} = \\text{softmax}(W(o + u))$$\n",
    "\n",
    "To extend this to a multiple layer model, we will iterate over the memory $K$ times, or for $K$ **hops**, and adopt **adjacent weight tying.**\n",
    "\n",
    "The layers are stacked as such:\n",
    "\n",
    "* The question embedding is constrained to match the input embedding of the first layer: \n",
    "$$B = A^1$$\n",
    "\n",
    "\n",
    "* The output embedding for one layer is the input embedding for the layer above it: \n",
    "$$A^{k+1}= C^k$$\n",
    "\n",
    "\n",
    "* The inputs to all layers following the first is the sum of the output $o^k$ and the input $u^k$ from layer $k$: $$u^{k+1}=u^k+o^k$$\n",
    "\n",
    "\n",
    "* The input to W combines the input and output of the top memory layer: \n",
    "$$\\hat{a} = \\text{softmax}(W(o^k + u^k))$$\n",
    "\n",
    "\n",
    "* The answer prediction matrix is constrained to be the same as the final output embedding: \n",
    "$$W^T = C^K$$\n",
    "\n",
    "<figure>\n",
    "    <img src='images/arch.png' alt='missing' />\n",
    "    <figcaption>**Figure 1.** (a):  A single layer version of our model. (b):  A three layer version of our model. In practice, we will constrain several of the embedding matrices to be the same.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for grading purpose only, the source code is in memn2n/memn2n_skeleton.py. \n",
    "# this block should not be run\n",
    "def position_encoding(sentence_size, embedding_size):\n",
    "    \"\"\"\n",
    "    Position Encoding module: TODO\n",
    "    \"\"\"\n",
    "    ##################################################\n",
    "    ### Q1 - Position Encoding                     ###\n",
    "    ##################################################\n",
    "\n",
    "    # returns: a 2D numpy array (sentence_size, embedding_size)\n",
    "    # encoding: a 2D numpy array (embedding_size, sentence_size)\n",
    "\n",
    "    encoding = np.zeros((embedding_size, sentence_size))\n",
    "    ls = sentence_size+1\n",
    "    le = embedding_size+1\n",
    "    for k in np.arange(1, le):\n",
    "        for j in np.arange(1, ls):\n",
    "            encoding[k-1, j-1] = (1-np.true_divide(j,sentence_size))-np.true_divide(k,embedding_size)*(1-2*np.true_divide(j,sentence_size))\n",
    "\n",
    "    # Make position encoding of time words identity to avoid modifying them \n",
    "    encoding[:, -1] = 1.0\n",
    "    return np.transpose(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2, Q3, Q4\n",
    "\n",
    "Implement the MemN2N model according to the above specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from memn2n.memn2n_skeleton import MemN2N_Base\n",
    "\n",
    "class MemN2N(MemN2N_Base):\n",
    "        \n",
    "    def _build_inputs(self):\n",
    "        self._stories = tf.placeholder(tf.int32, [None, self._memory_size, self._sentence_size], name=\"stories\")\n",
    "        self._queries = tf.placeholder(tf.int32, [None, self._sentence_size], name=\"queries\")\n",
    "        self._answers = tf.placeholder(tf.int32, [None, self._vocab_size], name=\"answers\")\n",
    "        self._lr = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n",
    "        \n",
    "    def _build_vars(self):\n",
    "        with tf.variable_scope(self._name):\n",
    "            nil_word_slot = tf.zeros([1, self._embedding_size])\n",
    "            A = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "            C = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "            \n",
    "            self.A_1 = tf.Variable(A, name=\"A\")\n",
    "            self.C = []\n",
    "\n",
    "            for hop in range(self._hops):\n",
    "                with tf.variable_scope('hop_{}'.format(hop)):\n",
    "                    self.C.append(tf.Variable(C, name=\"C\"))     \n",
    "        \n",
    "        self._nil_vars = set([self.A_1.name] + [x.name for x in self.C])\n",
    "    \n",
    "    \n",
    "    def _inference(self, stories, queries):\n",
    "        with tf.variable_scope(\"inference\"):\n",
    "            \n",
    "            \n",
    "            ##################################################\n",
    "            ### Q2 - Input memory representation           ###\n",
    "            ##################################################\n",
    "            # shapes of tensors\n",
    "            # stories: Tensor (None, memory_size, sentence_size)\n",
    "            # queries: Tensor (None, sentence_size)\n",
    "            # answers: Tensor (None, vocab_size)\n",
    "            # u_0: (batch, embed_size)\n",
    "            # encoding: (sentence_size, embed_size)\n",
    "            encoding = self._encoding\n",
    "            q_emb = self.A_1 # YOUR CODE HERE\n",
    "            u = []\n",
    "            u_0 = tf.gather(q_emb, queries) # (batch, sentence_size, embed_size)\n",
    "            u_0 = tf.cast(u_0, tf.float64)\n",
    "            u_0 = tf.reduce_sum(encoding * u_0, 1) #(batch, embed_size)\n",
    "            u.append(u_0)\n",
    "               \n",
    "            for hop in range(self._hops):\n",
    "                if hop == 0:\n",
    "                    # input_emb: (vocab_size, embed_size)\n",
    "                    input_emb = self.A_1 # YOUR CODE HERE\n",
    "                    \n",
    "                else:\n",
    "                    with tf.variable_scope('hop_{}'.format(hop - 1)):\n",
    "                        # YOUR CODE HERE\n",
    "                        #input_emb = tf.get_variable(\"C\")\n",
    "                        input_emb = self.C[hop-1]\n",
    "                \n",
    "                # Remember to element wise multiply position encoding\n",
    "                # stories_embedded: (batch, memory_size, sentence_size, embed_size)\n",
    "                stories_embedded_input = tf.gather(input_emb, stories)\n",
    "                stories_embedded_input = tf.cast(stories_embedded_input, tf.float64)\n",
    "                stories_embedded_input = encoding * stories_embedded_input\n",
    "                m_A = tf.reduce_sum(stories_embedded_input, 2) # (batch, memory_size, embed_size)\n",
    "           \n",
    "                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n",
    "                dot_prod = tf.reduce_sum(m_A * u_temp, 2)\n",
    "                probs = tf.nn.softmax(dot_prod) # (batch, memory_size)\n",
    "\n",
    "                ##################################################\n",
    "                ### Q3 - Output memory representation          ###\n",
    "                ##################################################\n",
    "                with tf.variable_scope('hop_{}'.format(hop)):\n",
    "                    #output_emb = tf.get_variable(\"C\") # (vocab_size, embed_size)\n",
    "                    output_emb = self.C[hop]\n",
    "                # Remember to element wise multiply position encoding!\n",
    "                stories_embedded_output = tf.gather(output_emb, stories) # (batch, memory_size, sentence_size, embed_size)\n",
    "                stories_embedded_output = tf.cast(stories_embedded_output, tf.float64)\n",
    "                stories_embedded_output = encoding * stories_embedded_output # (batch, memory_size, sentence_size, embed_size)\n",
    "                m_C = tf.reduce_sum(stories_embedded_output, 2) # (batch, memory_size, embed_size)\n",
    "                o_k = tf.squeeze(tf.matmul(m_C, tf.expand_dims(probs, -1), transpose_a=True), -1) # (batch, embed_size)\n",
    "\n",
    "                ##################################################\n",
    "                ### Q4 - Generating predictions                ###\n",
    "                ##################################################\n",
    "                \n",
    "                u_k = o_k + u[-1]\n",
    "                u.append(u_k)\n",
    "                \n",
    "            # Final output embedding, W_T = C_K from adjacent weight sharing\n",
    "            with tf.variable_scope('hop_{}'.format(self._hops)):\n",
    "                return tf.matmul(u[-1], tf.cast(self.C[-1], tf.float64), transpose_b=True) # (batch, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:\n",
    "\n",
    "We will now evaluate your MemN2N implementation on Facebook's bAbi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from memn2n.data_utils import get_data_info, split_train_valid_test, generate_batches\n",
    "from sklearn import metrics\n",
    "from six.moves import range\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter intialization\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01 # Learning rate for the Adam Optimizer\n",
    "lr_decay_epoch = 25 # Number of epochs until lr is halved\n",
    "lr_decay_stop = 100 # Epoch to stop annealing lr\n",
    "max_grad_norm = 40 # Clip gradients to this norm\n",
    "\n",
    "hops = 3 # Number of hops\n",
    "embedding_size = 20 # Embedding size for embedding matrices\n",
    "memory_size = 50 # Maximum size of memory\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 32\n",
    "evaluation_interval = 10\n",
    "data_dir = \"datasets/babi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# Get data and process for our model\n",
    "word_idx, vocab_size, sentence_size = get_data_info(data_dir, memory_size)\n",
    "\n",
    "train_stories, train_queries, train_answers, \\\n",
    "val_stories, val_queries, val_answers, \\\n",
    "test_stories, test_queries, test_answers = split_train_valid_test(data_dir, word_idx, sentence_size, memory_size)\n",
    "\n",
    "# Number of train/val/test examples\n",
    "n_train = train_stories.shape[0]\n",
    "n_val = val_stories.shape[0]\n",
    "n_test = test_stories.shape[0]\n",
    "\n",
    "# Create labels\n",
    "train_labels = np.argmax(train_answers, axis=1)\n",
    "test_labels = np.argmax(test_answers, axis=1)\n",
    "val_labels = np.argmax(val_answers, axis=1)\n",
    "\n",
    "# Generate batches\n",
    "batches = generate_batches(batch_size, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Epoch 10\n",
      "Total Cost: 13837.354844279183\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 0.9933333333333333\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.979\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.5855555555555556\n",
      "Validation Accuracy = 0.49\n",
      "Testing Accuracy = 0.555\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.5711111111111111\n",
      "Validation Accuracy = 0.43\n",
      "Testing Accuracy = 0.45\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.7277777777777777\n",
      "Validation Accuracy = 0.71\n",
      "Testing Accuracy = 0.727\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.86\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.8\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.54\n",
      "Validation Accuracy = 0.44\n",
      "Testing Accuracy = 0.522\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.5877777777777777\n",
      "Validation Accuracy = 0.56\n",
      "Testing Accuracy = 0.574\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.6911111111111111\n",
      "Validation Accuracy = 0.76\n",
      "Testing Accuracy = 0.671\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.6655555555555556\n",
      "Validation Accuracy = 0.7\n",
      "Testing Accuracy = 0.636\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.5066666666666667\n",
      "Validation Accuracy = 0.44\n",
      "Testing Accuracy = 0.436\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9866666666666667\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.972\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 1.0\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.98\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.98\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.7677777777777778\n",
      "Validation Accuracy = 0.64\n",
      "Testing Accuracy = 0.682\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.2833333333333333\n",
      "Validation Accuracy = 0.22\n",
      "Testing Accuracy = 0.243\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.47555555555555556\n",
      "Validation Accuracy = 0.48\n",
      "Testing Accuracy = 0.426\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.5611111111111111\n",
      "Validation Accuracy = 0.42\n",
      "Testing Accuracy = 0.483\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.52\n",
      "Validation Accuracy = 0.44\n",
      "Testing Accuracy = 0.479\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.12666666666666668\n",
      "Validation Accuracy = 0.09\n",
      "Testing Accuracy = 0.08\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 0.9844444444444445\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.975\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Total Cost: 10669.568058097395\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.99\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.8455555555555555\n",
      "Validation Accuracy = 0.72\n",
      "Testing Accuracy = 0.729\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.8077777777777778\n",
      "Validation Accuracy = 0.66\n",
      "Testing Accuracy = 0.673\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8111111111111111\n",
      "Validation Accuracy = 0.73\n",
      "Testing Accuracy = 0.762\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.8944444444444445\n",
      "Validation Accuracy = 0.84\n",
      "Testing Accuracy = 0.83\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9033333333333333\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.82\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.7766666666666666\n",
      "Validation Accuracy = 0.75\n",
      "Testing Accuracy = 0.729\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.7544444444444445\n",
      "Validation Accuracy = 0.71\n",
      "Testing Accuracy = 0.693\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.84\n",
      "Validation Accuracy = 0.81\n",
      "Testing Accuracy = 0.806\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.6255555555555555\n",
      "Validation Accuracy = 0.61\n",
      "Testing Accuracy = 0.583\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9755555555555555\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.97\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.99\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.986\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.985\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9577777777777777\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.849\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.3388888888888889\n",
      "Validation Accuracy = 0.38\n",
      "Testing Accuracy = 0.26\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.49333333333333335\n",
      "Validation Accuracy = 0.56\n",
      "Testing Accuracy = 0.476\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.6566666666666666\n",
      "Validation Accuracy = 0.56\n",
      "Testing Accuracy = 0.586\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.5911111111111111\n",
      "Validation Accuracy = 0.47\n",
      "Testing Accuracy = 0.531\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.16333333333333333\n",
      "Validation Accuracy = 0.09\n",
      "Testing Accuracy = 0.094\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 0.9866666666666667\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.983\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 30\n",
      "Total Cost: 7485.9093829576605\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9022222222222223\n",
      "Validation Accuracy = 0.71\n",
      "Testing Accuracy = 0.783\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.8866666666666667\n",
      "Validation Accuracy = 0.74\n",
      "Testing Accuracy = 0.72\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8577777777777778\n",
      "Validation Accuracy = 0.79\n",
      "Testing Accuracy = 0.774\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9533333333333334\n",
      "Validation Accuracy = 0.9\n",
      "Testing Accuracy = 0.853\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9877777777777778\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.951\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.8733333333333333\n",
      "Validation Accuracy = 0.87\n",
      "Testing Accuracy = 0.814\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.88\n",
      "Validation Accuracy = 0.82\n",
      "Testing Accuracy = 0.802\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9777777777777777\n",
      "Validation Accuracy = 0.94\n",
      "Testing Accuracy = 0.967\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9388888888888889\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.881\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9977777777777778\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.986\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9966666666666667\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.991\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.988\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.907\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.5622222222222222\n",
      "Validation Accuracy = 0.5\n",
      "Testing Accuracy = 0.473\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.55\n",
      "Validation Accuracy = 0.51\n",
      "Testing Accuracy = 0.467\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.6733333333333333\n",
      "Validation Accuracy = 0.61\n",
      "Testing Accuracy = 0.565\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.7088888888888889\n",
      "Validation Accuracy = 0.64\n",
      "Testing Accuracy = 0.592\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.15555555555555556\n",
      "Validation Accuracy = 0.04\n",
      "Testing Accuracy = 0.076\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.998\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 40\n",
      "Total Cost: 6639.3682438324695\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9044444444444445\n",
      "Validation Accuracy = 0.76\n",
      "Testing Accuracy = 0.776\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.9077777777777778\n",
      "Validation Accuracy = 0.66\n",
      "Testing Accuracy = 0.688\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8822222222222222\n",
      "Validation Accuracy = 0.78\n",
      "Testing Accuracy = 0.803\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.9633333333333334\n",
      "Validation Accuracy = 0.91\n",
      "Testing Accuracy = 0.86\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 0.9977777777777778\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.962\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.8866666666666667\n",
      "Validation Accuracy = 0.88\n",
      "Testing Accuracy = 0.821\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.9044444444444445\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.814\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9944444444444445\n",
      "Validation Accuracy = 0.97\n",
      "Testing Accuracy = 0.974\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9822222222222222\n",
      "Validation Accuracy = 0.95\n",
      "Testing Accuracy = 0.934\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.991\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.988\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.992\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 0.9977777777777778\n",
      "Validation Accuracy = 0.95\n",
      "Testing Accuracy = 0.919\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.6433333333333333\n",
      "Validation Accuracy = 0.54\n",
      "Testing Accuracy = 0.563\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.5722222222222222\n",
      "Validation Accuracy = 0.54\n",
      "Testing Accuracy = 0.471\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.7188888888888889\n",
      "Validation Accuracy = 0.56\n",
      "Testing Accuracy = 0.572\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.7788888888888889\n",
      "Validation Accuracy = 0.76\n",
      "Testing Accuracy = 0.706\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.19666666666666666\n",
      "Validation Accuracy = 0.12\n",
      "Testing Accuracy = 0.081\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.985\n",
      "\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 50\n",
      "Total Cost: 5983.704984218799\n",
      "\n",
      "Task 1\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.996\n",
      "\n",
      "Task 2\n",
      "Training Accuracy = 0.9388888888888889\n",
      "Validation Accuracy = 0.8\n",
      "Testing Accuracy = 0.791\n",
      "\n",
      "Task 3\n",
      "Training Accuracy = 0.9366666666666666\n",
      "Validation Accuracy = 0.68\n",
      "Testing Accuracy = 0.705\n",
      "\n",
      "Task 4\n",
      "Training Accuracy = 0.8655555555555555\n",
      "Validation Accuracy = 0.78\n",
      "Testing Accuracy = 0.798\n",
      "\n",
      "Task 5\n",
      "Training Accuracy = 0.98\n",
      "Validation Accuracy = 0.92\n",
      "Testing Accuracy = 0.86\n",
      "\n",
      "Task 6\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.964\n",
      "\n",
      "Task 7\n",
      "Training Accuracy = 0.8988888888888888\n",
      "Validation Accuracy = 0.83\n",
      "Testing Accuracy = 0.796\n",
      "\n",
      "Task 8\n",
      "Training Accuracy = 0.9088888888888889\n",
      "Validation Accuracy = 0.86\n",
      "Testing Accuracy = 0.814\n",
      "\n",
      "Task 9\n",
      "Training Accuracy = 0.9988888888888889\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.983\n",
      "\n",
      "Task 10\n",
      "Training Accuracy = 0.9933333333333333\n",
      "Validation Accuracy = 0.96\n",
      "Testing Accuracy = 0.952\n",
      "\n",
      "Task 11\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.97\n",
      "Testing Accuracy = 0.989\n",
      "\n",
      "Task 12\n",
      "Training Accuracy = 0.9977777777777778\n",
      "Validation Accuracy = 1.0\n",
      "Testing Accuracy = 0.995\n",
      "\n",
      "Task 13\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.98\n",
      "Testing Accuracy = 0.992\n",
      "\n",
      "Task 14\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.93\n",
      "Testing Accuracy = 0.91\n",
      "\n",
      "Task 15\n",
      "Training Accuracy = 0.7155555555555555\n",
      "Validation Accuracy = 0.63\n",
      "Testing Accuracy = 0.627\n",
      "\n",
      "Task 16\n",
      "Training Accuracy = 0.5566666666666666\n",
      "Validation Accuracy = 0.48\n",
      "Testing Accuracy = 0.48\n",
      "\n",
      "Task 17\n",
      "Training Accuracy = 0.6766666666666666\n",
      "Validation Accuracy = 0.59\n",
      "Testing Accuracy = 0.59\n",
      "\n",
      "Task 18\n",
      "Training Accuracy = 0.9155555555555556\n",
      "Validation Accuracy = 0.9\n",
      "Testing Accuracy = 0.89\n",
      "\n",
      "Task 19\n",
      "Training Accuracy = 0.17666666666666667\n",
      "Validation Accuracy = 0.1\n",
      "Testing Accuracy = 0.078\n",
      "\n",
      "Task 20\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.99\n",
      "Testing Accuracy = 0.997\n",
      "\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Accuracy Results\n",
    "train_eval = None\n",
    "val_eval = None\n",
    "test_eval = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, embedding_size, session=sess,\n",
    "                   hops=hops, max_grad_norm=max_grad_norm)\n",
    "    for i in range(1, epochs):\n",
    "        \n",
    "        # Stepped learning rate\n",
    "        if i - 1 <= lr_decay_stop:\n",
    "            anneal = 2.0 ** ((i - 1) // lr_decay_epoch)\n",
    "        else:\n",
    "            anneal = 2.0 ** (lr_decay_stop // lr_decay_epoch)\n",
    "        lr = learning_rate / anneal\n",
    "\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            s = train_stories[start:end]\n",
    "            q = train_queries[start:end]\n",
    "            a = train_answers[start:end]\n",
    "            cost_t = model.batch_fit(s, q, a, lr)\n",
    "            total_cost += cost_t\n",
    "\n",
    "        if i % evaluation_interval == 0:\n",
    "            train_accs = []\n",
    "            for start in range(0, n_train, n_train//20):\n",
    "                end = start + n_train//20\n",
    "                s = train_stories[start:end]\n",
    "                q = train_queries[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = metrics.accuracy_score(pred, train_labels[start:end])\n",
    "                train_accs.append(acc)\n",
    "\n",
    "            val_accs = []\n",
    "            for start in range(0, n_val, n_val//20):\n",
    "                end = start + n_val//20\n",
    "                s = val_stories[start:end]\n",
    "                q = val_queries[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = metrics.accuracy_score(pred, val_labels[start:end])\n",
    "                val_accs.append(acc)\n",
    "\n",
    "            test_accs = []\n",
    "            for start in range(0, n_test, n_test//20):\n",
    "                end = start + n_test//20\n",
    "                s = test_stories[start:end]\n",
    "                q = test_queries[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                acc = metrics.accuracy_score(pred, test_labels[start:end])\n",
    "                test_accs.append(acc)\n",
    "\n",
    "            print('-----------------------')\n",
    "            print('Epoch', i)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print()\n",
    "            t = 1\n",
    "            for t1, t2, t3 in zip(train_accs, val_accs, test_accs):\n",
    "                print(\"Task {}\".format(t))\n",
    "                print(\"Training Accuracy = {}\".format(t1))\n",
    "                print(\"Validation Accuracy = {}\".format(t2))\n",
    "                print(\"Testing Accuracy = {}\".format(t3))\n",
    "                print()\n",
    "                t += 1\n",
    "            print('-----------------------')\n",
    "        \n",
    "        #if i == FLAGS.epochs:\n",
    "        if i == epochs:\n",
    "            train_eval, val_eval, test_eval = train_accs, val_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.791</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.865556</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.860</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.898889</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.814</td>\n",
       "      <td>0.908889</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.952</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997778</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.627</td>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.556667</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.078</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Testing Accuracy  Training Accuracy  Validation Accuracy\n",
       "Task                                                          \n",
       "1                0.996           1.000000                 0.99\n",
       "2                0.791           0.938889                 0.80\n",
       "3                0.705           0.936667                 0.68\n",
       "4                0.798           0.865556                 0.78\n",
       "5                0.860           0.980000                 0.92\n",
       "6                0.964           1.000000                 0.96\n",
       "7                0.796           0.898889                 0.83\n",
       "8                0.814           0.908889                 0.86\n",
       "9                0.983           0.998889                 0.96\n",
       "10               0.952           0.993333                 0.96\n",
       "11               0.989           1.000000                 0.97\n",
       "12               0.995           0.997778                 1.00\n",
       "13               0.992           1.000000                 0.98\n",
       "14               0.910           1.000000                 0.93\n",
       "15               0.627           0.715556                 0.63\n",
       "16               0.480           0.556667                 0.48\n",
       "17               0.590           0.676667                 0.59\n",
       "18               0.890           0.915556                 0.90\n",
       "19               0.078           0.176667                 0.10\n",
       "20               0.997           1.000000                 0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View results in pd dataframe\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "df = pd.DataFrame({\n",
    "    'Training Accuracy': train_accs,\n",
    "    'Validation Accuracy': val_accs,\n",
    "    'Testing Accuracy': test_accs\n",
    "    }, index=range(1, 21))\n",
    "df.index.name = 'Task'\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
