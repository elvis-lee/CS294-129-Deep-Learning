{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient methods\n",
    "\n",
    "We will start with the standard policy gradient algorithm. This is a batch algorithm, which means that we will collect a large number of samples per iteration, and perform a single update to the policy using these samples. Recall that the formula for policy gradient is given by\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T\\gamma^t r_t \\Big] = \n",
    "\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T \\nabla_{\\theta} \\log\\pi_{\\theta}(a_t|s_t)\\big(R_t - b(s_t)\\big) \\Big]$$\n",
    "\n",
    "- $\\pi_{\\theta}$ is a stochastic policy parameterized by $\\theta$;\n",
    "- $\\gamma$ is the discount factor;\n",
    "- $s_t$, $a_t$ and $r_t$ are the state, action, and reward at time $t$ respectively;\n",
    "- $T$ is the length of a single episode;\n",
    "- $b(s_t)$ is a baseline funcion which only depends on the current state $s_t$\n",
    "- $R_t$ is the discounted cumulative return (already defined in the DQN exercise)\n",
    "Instead of optimizing this formula, we will optimize a sample-based estimation of the expectation, based on $N$ trajectories. For this you will first implement a function that computes $\\log\\pi_{\\theta}(a_t|s_t)$ given any $s,~a$. \n",
    "\n",
    "Let's assume for now that $\\pi_{\\theta}$ is a Gaussian with mean $\\mu=\\theta^T\\tilde{s}$, , where $\\tilde{s}$ is the vector obtained by appending 1 to the state $s$, so that we don't need a separate bias term.\n",
    "- Compute what should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gym\n",
    "import click\n",
    "from simplepg.simple_utils import gradient_check, log_softmax, softmax, weighted_sample, include_bias, test_once, nprs\n",
    "import tests.simplepg_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-717d47239d9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.Linear1 = nn.Linear(obs_size, 256)\n",
    "        self.Linear2 = nn.Linear(256, 256)\n",
    "        self.Linear3 = nn.Linear(256, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = F.relu(self.Linear1(obs))\n",
    "        out = F.relu(self.Linear2(out))\n",
    "        out = self.Linear3(out) \n",
    "        return out\n",
    "    \n",
    "class NN_linear(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN_linear, self).__init__()\n",
    "        self.Linear = nn.Linear(obs_size, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.Linear(obs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# Methods for Point-v0\n",
    "##############################\n",
    "class GaussianMLP_Policy(object):\n",
    "    def __init__(self, obs_size, act_size, NN, ):\n",
    "        self.NN = NN(obs_size, act_size)\n",
    "#         self.optimizer = optim.Adam(self.NN.parameters(), lr=0.0001)\n",
    "        \n",
    "    def get_action(obs):\n",
    "        mean = self.NN.forward(obs)\n",
    "        return np.random(loc=mean, scale=1.)\n",
    "    \n",
    "    def get_logp_action(obs, action):\n",
    "        # should I variablilify the action and the obs?\n",
    "        mean = self.NN.forward(obs)\n",
    "        return -0.5 * F.log(2 * np.pi) * action.shape[0] - 0.5 * F.mse_loss(mean, action, size_average=False)\n",
    "    \n",
    "    def get_grad_logp_action(obs, action):\n",
    "        logp = get_logp_action(obs, action)\n",
    "        logp.backward()\n",
    "        return [p.grads() for p in list(self.NN.parameters())]\n",
    "    \n",
    "def point_get_logp_action(theta, ob, action):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :param action: A vector of size |A|\n",
    "    :return: A scalar\n",
    "    \"\"\"\n",
    "    ob_1 = include_bias(ob) ########### todo: remove this!!\n",
    "    mean = theta.dot(ob_1)  ########### todo: replace by NN!!\n",
    "    zs = action - mean\n",
    "    return -0.5 * np.log(2 * np.pi) * theta.shape[0] - 0.5 * np.sum(np.square(zs))  ### the output here should be symb!\n",
    "\n",
    "\n",
    "def point_get_grad_logp_action(theta, ob, action):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :param action: A vector of size |A|\n",
    "    :return: A matrix of size |A| * (|S|+1)\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(theta)\n",
    "    # BEGIN SOLUTION\n",
    "    ob_1 = include_bias(ob) ########### todo: remove this!!\n",
    "    grad = np.outer(action - theta.dot(ob_1), ob_1) ########### todo: replace by autodiff\n",
    "    # END SOLUTION\n",
    "    return grad\n",
    "\n",
    "\n",
    "def point_get_action(theta, ob, rng=np.random):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :return: A vector of size |A|\n",
    "    \"\"\"\n",
    "    ob_1 = include_bias(ob)\n",
    "    mean = theta.dot(ob_1)\n",
    "    return rng.normal(loc=mean, scale=1.)\n",
    "\n",
    "\n",
    "def point_test_grad_impl():\n",
    "    # check gradient implementation\n",
    "    rng = nprs(42)\n",
    "    test_ob = rng.uniform(size=(4,))\n",
    "    test_act = rng.uniform(size=(4,))\n",
    "    test_theta = rng.uniform(size=(4, 5))\n",
    "    # Check that the shape matches\n",
    "    assert point_get_grad_logp_action(test_theta, test_ob, test_act).shape == test_theta.shape\n",
    "    gradient_check(\n",
    "        lambda x: point_get_logp_action(x.reshape(test_theta.shape), test_ob, test_act),\n",
    "        lambda x: point_get_grad_logp_action(x.reshape(test_theta.shape), test_ob, test_act).flatten(),\n",
    "        test_theta.flatten()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_baselines(all_returns):\n",
    "    \"\"\"\n",
    "    :param all_returns: A vector of size T\n",
    "    :return: A vector of size T\n",
    "    \"\"\"\n",
    "    baselines = np.zeros(len(all_returns))\n",
    "    for t in range(len(all_returns)):\n",
    "        # BEGIN SOLUTION\n",
    "        # Update the baselines\n",
    "        if len(all_returns[t]) > 0:\n",
    "            baselines[t] = np.mean(all_returns[t])\n",
    "        else:\n",
    "            baselines[t] = 0.\n",
    "        # END SOLUTION\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-05 16:37:31,659] Making new env: Point-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "point_test_grad_impl()\n",
    "\n",
    "from simplepg import point_env\n",
    "env = gym.make('Point-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "get_action = point_get_action\n",
    "get_grad_logp_action = point_get_grad_logp_action\n",
    "\n",
    "env.seed(42)\n",
    "timestep_limit = env.spec.timestep_limit\n",
    "\n",
    "# Initialize parameters\n",
    "theta = rng.normal(scale=0.1, size=(action_dim, obs_dim + 1))\n",
    "\n",
    "# Store baselines for each time step.\n",
    "baselines = np.zeros(timestep_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_update(discount, R_tplus1, theta, s_t, a_t, r_t, b_t, get_grad_logp_action):\n",
    "    \"\"\"\n",
    "    :param discount: A scalar\n",
    "    :param R_tplus1: A scalar\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param s_t: A vector of size |S|\n",
    "    :param a_t: Either a vector of size |A| or an integer, depending on the environment\n",
    "    :param r_t: A scalar\n",
    "    :param b_t: A scalar\n",
    "    :param get_grad_logp_action: A function, mapping from (theta, ob, action) to the gradient (a \n",
    "    matrix of size |A| * (|S|+1) )\n",
    "    :return: A tuple, consisting of a scalar and a matrix of size |A| * (|S|+1)\n",
    "    \"\"\"\n",
    "    R_t = 0.\n",
    "    grad_t = np.zeros_like(theta)\n",
    "    # BEGIN SOLUTION\n",
    "    R_t = r_t + discount * R_tplus1\n",
    "    grad_t = get_grad_logp_action(theta, s_t, a_t) * (R_t - b_t)\n",
    "    \n",
    "    policy.NN.zero_grads()\n",
    "    logp = get_logp_action(obs, action)\n",
    "    logp.backward()\n",
    "    for p in list(policy.NN.parameters()):\n",
    "        p.data -= learning_rate * p.grad * (R_t - b_t)\n",
    "    # END SOLUTION\n",
    "    return R_t, grad_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_update(discount, R_tplus1, theta, s_t, a_t, r_t, b_t, get_grad_logp_action):\n",
    "    \"\"\"\n",
    "    :param discount: A scalar\n",
    "    :param R_tplus1: A scalar\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param s_t: A vector of size |S|\n",
    "    :param a_t: Either a vector of size |A| or an integer, depending on the environment\n",
    "    :param r_t: A scalar\n",
    "    :param b_t: A scalar\n",
    "    :param get_grad_logp_action: A function, mapping from (theta, ob, action) to the gradient (a \n",
    "    matrix of size |A| * (|S|+1) )\n",
    "    :return: A tuple, consisting of a scalar and a matrix of size |A| * (|S|+1)\n",
    "    \"\"\"\n",
    "    R_t = 0.\n",
    "    grad_t = np.zeros_like(theta)\n",
    "    # BEGIN SOLUTION\n",
    "    R_t = r_t + discount * R_tplus1\n",
    "    grad_t = get_grad_logp_action(theta, s_t, a_t) * (R_t - b_t)\n",
    "    # END SOLUTION\n",
    "    return R_t, grad_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for __main__.compute_update passed!\n",
      "Test for __main__.compute_baselines passed!\n",
      "Iteration: 0 AverageReturn: -41.86 |theta|_2: 0.20\n",
      "Iteration: 1 AverageReturn: -41.96 |theta|_2: 0.25\n",
      "Iteration: 2 AverageReturn: -40.19 |theta|_2: 0.27\n",
      "Iteration: 3 AverageReturn: -41.91 |theta|_2: 0.33\n",
      "Iteration: 4 AverageReturn: -38.64 |theta|_2: 0.40\n",
      "Iteration: 5 AverageReturn: -38.51 |theta|_2: 0.47\n",
      "Iteration: 6 AverageReturn: -36.26 |theta|_2: 0.55\n",
      "Iteration: 7 AverageReturn: -37.21 |theta|_2: 0.62\n",
      "Iteration: 8 AverageReturn: -37.65 |theta|_2: 0.70\n",
      "Iteration: 9 AverageReturn: -35.10 |theta|_2: 0.79\n",
      "Iteration: 10 AverageReturn: -33.95 |theta|_2: 0.86\n",
      "Iteration: 11 AverageReturn: -34.01 |theta|_2: 0.95\n",
      "Iteration: 12 AverageReturn: -32.43 |theta|_2: 1.03\n",
      "Iteration: 13 AverageReturn: -32.80 |theta|_2: 1.12\n",
      "Iteration: 14 AverageReturn: -32.18 |theta|_2: 1.19\n",
      "Iteration: 15 AverageReturn: -31.46 |theta|_2: 1.28\n",
      "Iteration: 16 AverageReturn: -30.63 |theta|_2: 1.35\n",
      "Iteration: 17 AverageReturn: -29.84 |theta|_2: 1.42\n",
      "Iteration: 18 AverageReturn: -29.97 |theta|_2: 1.49\n",
      "Iteration: 19 AverageReturn: -29.73 |theta|_2: 1.57\n",
      "Iteration: 20 AverageReturn: -30.21 |theta|_2: 1.66\n",
      "Iteration: 21 AverageReturn: -27.95 |theta|_2: 1.75\n",
      "Iteration: 22 AverageReturn: -27.60 |theta|_2: 1.83\n",
      "Iteration: 23 AverageReturn: -28.19 |theta|_2: 1.90\n",
      "Iteration: 24 AverageReturn: -26.67 |theta|_2: 1.99\n",
      "Iteration: 25 AverageReturn: -26.14 |theta|_2: 2.08\n",
      "Iteration: 26 AverageReturn: -26.03 |theta|_2: 2.17\n",
      "Iteration: 27 AverageReturn: -26.52 |theta|_2: 2.23\n",
      "Iteration: 28 AverageReturn: -25.38 |theta|_2: 2.31\n",
      "Iteration: 29 AverageReturn: -25.31 |theta|_2: 2.37\n",
      "Iteration: 30 AverageReturn: -24.53 |theta|_2: 2.45\n",
      "Iteration: 31 AverageReturn: -24.67 |theta|_2: 2.52\n",
      "Iteration: 32 AverageReturn: -24.84 |theta|_2: 2.59\n",
      "Iteration: 33 AverageReturn: -24.59 |theta|_2: 2.66\n",
      "Iteration: 34 AverageReturn: -24.27 |theta|_2: 2.72\n",
      "Iteration: 35 AverageReturn: -23.56 |theta|_2: 2.76\n",
      "Iteration: 36 AverageReturn: -23.80 |theta|_2: 2.84\n",
      "Iteration: 37 AverageReturn: -23.48 |theta|_2: 2.91\n",
      "Iteration: 38 AverageReturn: -22.52 |theta|_2: 2.98\n",
      "Iteration: 39 AverageReturn: -22.91 |theta|_2: 3.06\n",
      "Iteration: 40 AverageReturn: -22.50 |theta|_2: 3.15\n",
      "Iteration: 41 AverageReturn: -23.04 |theta|_2: 3.20\n",
      "Iteration: 42 AverageReturn: -22.32 |theta|_2: 3.27\n",
      "Iteration: 43 AverageReturn: -22.61 |theta|_2: 3.34\n",
      "Iteration: 44 AverageReturn: -22.21 |theta|_2: 3.41\n",
      "Iteration: 45 AverageReturn: -22.27 |theta|_2: 3.48\n",
      "Iteration: 46 AverageReturn: -21.48 |theta|_2: 3.55\n",
      "Iteration: 47 AverageReturn: -22.35 |theta|_2: 3.62\n",
      "Iteration: 48 AverageReturn: -22.44 |theta|_2: 3.68\n",
      "Iteration: 49 AverageReturn: -21.30 |theta|_2: 3.73\n",
      "Iteration: 50 AverageReturn: -21.72 |theta|_2: 3.75\n",
      "Iteration: 51 AverageReturn: -22.54 |theta|_2: 3.79\n",
      "Iteration: 52 AverageReturn: -21.51 |theta|_2: 3.81\n",
      "Iteration: 53 AverageReturn: -21.44 |theta|_2: 3.88\n",
      "Iteration: 54 AverageReturn: -21.18 |theta|_2: 3.89\n",
      "Iteration: 55 AverageReturn: -21.09 |theta|_2: 3.94\n",
      "Iteration: 56 AverageReturn: -21.24 |theta|_2: 3.95\n",
      "Iteration: 57 AverageReturn: -20.98 |theta|_2: 3.99\n",
      "Iteration: 58 AverageReturn: -21.12 |theta|_2: 4.00\n",
      "Iteration: 59 AverageReturn: -21.18 |theta|_2: 4.05\n",
      "Iteration: 60 AverageReturn: -21.11 |theta|_2: 4.14\n",
      "Iteration: 61 AverageReturn: -21.68 |theta|_2: 4.19\n",
      "Iteration: 62 AverageReturn: -21.29 |theta|_2: 4.21\n",
      "Iteration: 63 AverageReturn: -21.51 |theta|_2: 4.28\n",
      "Iteration: 64 AverageReturn: -21.18 |theta|_2: 4.35\n",
      "Iteration: 65 AverageReturn: -20.33 |theta|_2: 4.42\n",
      "Iteration: 66 AverageReturn: -20.83 |theta|_2: 4.48\n",
      "Iteration: 67 AverageReturn: -21.50 |theta|_2: 4.54\n",
      "Iteration: 68 AverageReturn: -20.52 |theta|_2: 4.53\n",
      "Iteration: 69 AverageReturn: -20.52 |theta|_2: 4.58\n",
      "Iteration: 70 AverageReturn: -20.32 |theta|_2: 4.55\n",
      "Iteration: 71 AverageReturn: -20.36 |theta|_2: 4.59\n",
      "Iteration: 72 AverageReturn: -21.10 |theta|_2: 4.64\n",
      "Iteration: 73 AverageReturn: -20.43 |theta|_2: 4.71\n",
      "Iteration: 74 AverageReturn: -20.39 |theta|_2: 4.75\n",
      "Iteration: 75 AverageReturn: -20.26 |theta|_2: 4.81\n",
      "Iteration: 76 AverageReturn: -19.58 |theta|_2: 4.85\n",
      "Iteration: 77 AverageReturn: -19.77 |theta|_2: 4.89\n",
      "Iteration: 78 AverageReturn: -20.26 |theta|_2: 4.92\n",
      "Iteration: 79 AverageReturn: -19.54 |theta|_2: 4.95\n",
      "Iteration: 80 AverageReturn: -20.32 |theta|_2: 5.03\n",
      "Iteration: 81 AverageReturn: -20.57 |theta|_2: 5.05\n",
      "Iteration: 82 AverageReturn: -20.14 |theta|_2: 5.03\n",
      "Iteration: 83 AverageReturn: -19.84 |theta|_2: 5.07\n",
      "Iteration: 84 AverageReturn: -19.98 |theta|_2: 5.06\n",
      "Iteration: 85 AverageReturn: -20.12 |theta|_2: 5.10\n",
      "Iteration: 86 AverageReturn: -19.40 |theta|_2: 5.16\n",
      "Iteration: 87 AverageReturn: -20.27 |theta|_2: 5.22\n",
      "Iteration: 88 AverageReturn: -19.93 |theta|_2: 5.24\n",
      "Iteration: 89 AverageReturn: -19.85 |theta|_2: 5.27\n",
      "Iteration: 90 AverageReturn: -19.92 |theta|_2: 5.23\n",
      "Iteration: 91 AverageReturn: -19.53 |theta|_2: 5.28\n",
      "Iteration: 92 AverageReturn: -19.48 |theta|_2: 5.34\n",
      "Iteration: 93 AverageReturn: -20.17 |theta|_2: 5.37\n",
      "Iteration: 94 AverageReturn: -19.75 |theta|_2: 5.44\n",
      "Iteration: 95 AverageReturn: -19.35 |theta|_2: 5.48\n",
      "Iteration: 96 AverageReturn: -19.17 |theta|_2: 5.49\n",
      "Iteration: 97 AverageReturn: -20.18 |theta|_2: 5.49\n",
      "Iteration: 98 AverageReturn: -19.75 |theta|_2: 5.54\n",
      "Iteration: 99 AverageReturn: -19.77 |theta|_2: 5.57\n"
     ]
    }
   ],
   "source": [
    "n_itrs = 100\n",
    "batch_size = 2000\n",
    "discount = 0.99\n",
    "learning_rate = 0.1\n",
    "render = True\n",
    "natural_step_size = 0.01\n",
    "\n",
    "# Policy training loop\n",
    "for itr in range(n_itrs):\n",
    "    # Collect trajectory loop\n",
    "    n_samples = 0\n",
    "    grad = np.zeros_like(theta)\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Store cumulative returns for each time step\n",
    "    all_returns = [[] for _ in range(timestep_limit)]\n",
    "\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "\n",
    "    while n_samples < batch_size:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = env.reset()\n",
    "        done = False\n",
    "        # Only render the first trajectory\n",
    "        render_episode = n_samples == 0\n",
    "        # Collect a new trajectory\n",
    "        while not done:\n",
    "            action = get_action(theta, ob, rng=rng)\n",
    "            next_ob, rew, done, _ = env.step(action)\n",
    "            observations.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            ob = next_ob\n",
    "            n_samples += 1\n",
    "            if render and render_episode:\n",
    "                env.render()\n",
    "        # Go back in time to compute returns and accumulate gradient\n",
    "        # Compute the gradient along this trajectory\n",
    "        R = 0.\n",
    "            \n",
    "        for t in reversed(range(len(observations))):\n",
    "            # Test the implementation, but only once\n",
    "            test_once(compute_update)\n",
    "\n",
    "            R, grad_t = compute_update(\n",
    "                discount=discount,\n",
    "                R_tplus1=R,\n",
    "                theta=theta,\n",
    "                s_t=observations[t],\n",
    "                a_t=actions[t],\n",
    "                r_t=rewards[t],\n",
    "                b_t=baselines[t],\n",
    "                get_grad_logp_action=get_grad_logp_action\n",
    "            )\n",
    "            all_returns[t].append(R)\n",
    "            grad += grad_t\n",
    "\n",
    "        episode_rewards.append(np.sum(rewards))\n",
    "        all_observations.extend(observations)\n",
    "        all_actions.extend(actions)\n",
    "        \n",
    "    test_once(compute_baselines)\n",
    "\n",
    "    baselines = compute_baselines(all_returns)\n",
    "\n",
    "    # Roughly normalize the gradient\n",
    "    grad = grad / (np.linalg.norm(grad) + 1e-8)\n",
    "\n",
    "    theta += learning_rate * grad\n",
    "\n",
    "    print(\"Iteration: %d AverageReturn: %.2f |theta|_2: %.2f\" % (\n",
    "    itr, np.mean(episode_rewards), np.linalg.norm(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:drl]",
   "language": "python",
   "name": "conda-env-drl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
