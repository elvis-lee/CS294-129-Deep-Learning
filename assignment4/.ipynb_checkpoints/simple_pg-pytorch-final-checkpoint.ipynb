{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient methods\n",
    "\n",
    "We will start with the standard policy gradient algorithm. This is a batch algorithm, which means that we will collect a large number of samples per iteration, and perform a single update to the policy using these samples. Recall that the formula for policy gradient is given by\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T\\gamma^t r_t \\Big] = \n",
    "\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T \\nabla_{\\theta} \\log\\pi_{\\theta}(a_t|s_t)\\big(R_t - b(s_t)\\big) \\Big]$$\n",
    "\n",
    "- $\\pi_{\\theta}$ is a stochastic policy parameterized by $\\theta$;\n",
    "- $\\gamma$ is the discount factor;\n",
    "- $s_t$, $a_t$ and $r_t$ are the state, action, and reward at time $t$;\n",
    "- $T$ is the length of a single episode;\n",
    "- $b(s_t)$ is any funcion which does not depend on the current action $a_t$, and is called baseline;\n",
    "- $R_t$ is the discounted cumulative future return (already defined in the DQN exercise);\n",
    "Instead of optimizing this formula, we will optimize a sample-based estimation of the expectation, based on $N$ trajectories. For this you will first implement a function that computes $\\log\\pi_{\\theta}(a_t|s_t)$ given any $s,~a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import gym\n",
    "from simplepg.simple_utils import gradient_check, log_softmax, softmax, weighted_sample, include_bias, test_once, nprs\n",
    "import tests.simplepg_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a stochastic policy\n",
    "\n",
    "Let's assume that $\\pi_{\\theta}$ is a Gaussian with unit variance $\\Sigma=I$ and mean $\\mu=NN_{\\theta}(s)$, where $NN_{\\theta}$ is a Neural Network parameterized by $\\theta$.\n",
    "\n",
    "### 1. Create a Linear NN\n",
    "Single layer, no non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.Linear1 = nn.Linear(obs_size, 256)\n",
    "        self.Linear2 = nn.Linear(256, 256)\n",
    "        self.Linear3 = nn.Linear(256, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = F.relu(self.Linear1(obs))\n",
    "        out = F.relu(self.Linear2(out))\n",
    "        out = self.Linear3(out) \n",
    "        return out\n",
    "    \n",
    "class NN_linear(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN_linear, self).__init__()\n",
    "        self.Linear = nn.Linear(obs_size, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.Linear(obs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMLP_Policy(object):\n",
    "    def __init__(self, obs_size, act_size, NN, ):\n",
    "        self.NN = NN(obs_size, act_size)\n",
    "        \n",
    "    def get_action(self, obs, rng=np.random):\n",
    "        obs_var = autograd.Variable(torch.Tensor(obs), requires_grad=False)\n",
    "        mean = self.NN.forward(obs_var)\n",
    "        return rng.normal(loc=mean.data.numpy(), scale=1.)\n",
    "    \n",
    "    def get_logp_action(self, obs, action):\n",
    "        mean = self.NN.forward(obs)\n",
    "        return float(-0.5 * np.log(2 * np.pi) * action.shape[-1]) - 0.5 * torch.sum((action - mean) ** 2, dim=-1)\n",
    "    \n",
    "    def get_grad_logp_action(self, obs, action):\n",
    "        logp = self.get_logp_action(obs, action)\n",
    "        logp.backward()\n",
    "        return [p.grads() for p in list(self.NN.parameters())]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Methods for Point-v0\n",
    "##############################\n",
    "\n",
    "def point_get_logp_action(theta, ob, action):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :param action: A vector of size |A|\n",
    "    :return: A scalar\n",
    "    \"\"\"\n",
    "    ob_1 = include_bias(ob) ########### todo: remove this!!\n",
    "    mean = theta.dot(ob_1)  ########### todo: replace by NN!!\n",
    "    zs = action - mean\n",
    "    return -0.5 * np.log(2 * np.pi) * theta.shape[0] - 0.5 * np.sum(np.square(zs))  ### the output here should be symb!\n",
    "\n",
    "\n",
    "def point_get_grad_logp_action(theta, ob, action):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :param action: A vector of size |A|\n",
    "    :return: A matrix of size |A| * (|S|+1)\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(theta)\n",
    "    # BEGIN SOLUTION\n",
    "    ob_1 = include_bias(ob) ########### todo: remove this!!\n",
    "    grad = np.outer(action - theta.dot(ob_1), ob_1) ########### todo: replace by autodiff\n",
    "    # END SOLUTION\n",
    "    return grad\n",
    "\n",
    "\n",
    "def point_get_action(theta, ob, rng=np.random):\n",
    "    \"\"\"\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param ob: A vector of size |S|\n",
    "    :return: A vector of size |A|\n",
    "    \"\"\"\n",
    "    ob_1 = include_bias(ob)\n",
    "    mean = theta.dot(ob_1)\n",
    "    return rng.normal(loc=mean, scale=1.)\n",
    "\n",
    "\n",
    "def point_test_grad_impl():\n",
    "    # check gradient implementation\n",
    "    rng = nprs(42)\n",
    "    test_ob = rng.uniform(size=(4,))\n",
    "    test_act = rng.uniform(size=(4,))\n",
    "    test_theta = rng.uniform(size=(4, 5))\n",
    "    # Check that the shape matches\n",
    "    assert point_get_grad_logp_action(test_theta, test_ob, test_act).shape == test_theta.shape\n",
    "    gradient_check(\n",
    "        lambda x: point_get_logp_action(x.reshape(test_theta.shape), test_ob, test_act),\n",
    "        lambda x: point_get_grad_logp_action(x.reshape(test_theta.shape), test_ob, test_act).flatten(),\n",
    "        test_theta.flatten()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baselines(all_returns):\n",
    "    \"\"\"\n",
    "    :param all_returns: A vector of size T\n",
    "    :return: A vector of size T\n",
    "    \"\"\"\n",
    "    baselines = np.zeros(len(all_returns))\n",
    "    for t in range(len(all_returns)):\n",
    "        # BEGIN SOLUTION\n",
    "        # Update the baselines\n",
    "        if len(all_returns[t]) > 0:\n",
    "            baselines[t] = np.mean(all_returns[t])\n",
    "        else:\n",
    "            baselines[t] = 0.\n",
    "        # END SOLUTION\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-06 19:32:44,194] Making new env: Point-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "point_test_grad_impl()\n",
    "\n",
    "from simplepg import point_env\n",
    "env = gym.make('Point-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "policy = GaussianMLP_Policy(obs_dim, action_dim, NN_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p.data.numpy() for p in policy.NN.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.concatenate([params[0], params[1].reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check the computations\n",
    "\n",
    "# get_action = point_get_action\n",
    "get_action = policy.get_action\n",
    "\n",
    "get_grad_logp_action = point_get_grad_logp_action\n",
    "\n",
    "env.seed(42)\n",
    "timestep_limit = env.spec.timestep_limit\n",
    "\n",
    "# Initialize parameters  ###### now taking the NN initialization\n",
    "# theta = rng.normal(scale=0.1, size=(action_dim, obs_dim + 1))\n",
    "\n",
    "# Store baselines for each time step.\n",
    "baselines = np.zeros(timestep_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_update(discount, R_tplus1, theta, s_t, a_t, r_t, b_t, get_grad_logp_action):\n",
    "    \"\"\"\n",
    "    :param discount: A scalar\n",
    "    :param R_tplus1: A scalar\n",
    "    :param theta: A matrix of size |A| * (|S|+1)\n",
    "    :param s_t: A vector of size |S|\n",
    "    :param a_t: Either a vector of size |A| or an integer, depending on the environment\n",
    "    :param r_t: A scalar\n",
    "    :param b_t: A scalar\n",
    "    :param get_grad_logp_action: A function, mapping from (theta, ob, action) to the gradient (a \n",
    "    matrix of size |A| * (|S|+1) )\n",
    "    :return: A tuple, consisting of a scalar and a matrix of size |A| * (|S|+1)\n",
    "    \"\"\"\n",
    "    R_t = 0.\n",
    "    grad_t = np.zeros_like(theta)\n",
    "    # BEGIN SOLUTION\n",
    "    R_t = r_t + discount * R_tplus1\n",
    "    grad_t = get_grad_logp_action(theta, s_t, a_t) * (R_t - b_t)\n",
    "    # END SOLUTION\n",
    "    return R_t, grad_t\n",
    "\n",
    "def compute_centered_returns(discount, rewards, baseline):\n",
    "    centered_cum_returns = np.zeros_like(rewards)\n",
    "    R_tplus1 = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        R = rewards[t] + discount * R_tplus1\n",
    "        centered_cum_returns[t] = R - baseline[t]\n",
    "        R_tplus1 = R\n",
    "    return centered_cum_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for __main__.compute_update passed!\n",
      "Test for __main__.compute_baselines passed!\n",
      "Iteration: 0 AverageReturn: -43.45 |theta|_2: 1.06\n",
      "Iteration: 1 AverageReturn: -41.55 |theta|_2: 1.04\n",
      "Iteration: 2 AverageReturn: -40.83 |theta|_2: 0.97\n",
      "Iteration: 3 AverageReturn: -41.75 |theta|_2: 0.99\n",
      "Iteration: 4 AverageReturn: -39.26 |theta|_2: 1.05\n",
      "Iteration: 5 AverageReturn: -40.52 |theta|_2: 1.04\n",
      "Iteration: 6 AverageReturn: -38.22 |theta|_2: 1.04\n",
      "Iteration: 7 AverageReturn: -40.50 |theta|_2: 1.05\n",
      "Iteration: 8 AverageReturn: -38.15 |theta|_2: 1.09\n",
      "Iteration: 9 AverageReturn: -36.58 |theta|_2: 1.15\n",
      "Iteration: 10 AverageReturn: -37.19 |theta|_2: 1.13\n",
      "Iteration: 11 AverageReturn: -38.39 |theta|_2: 1.15\n",
      "Iteration: 12 AverageReturn: -36.47 |theta|_2: 1.18\n",
      "Iteration: 13 AverageReturn: -34.71 |theta|_2: 1.19\n",
      "Iteration: 14 AverageReturn: -36.22 |theta|_2: 1.20\n",
      "Iteration: 15 AverageReturn: -38.09 |theta|_2: 1.24\n",
      "Iteration: 16 AverageReturn: -35.61 |theta|_2: 1.23\n",
      "Iteration: 17 AverageReturn: -35.76 |theta|_2: 1.23\n",
      "Iteration: 18 AverageReturn: -36.77 |theta|_2: 1.23\n",
      "Iteration: 19 AverageReturn: -34.74 |theta|_2: 1.23\n",
      "Iteration: 20 AverageReturn: -37.10 |theta|_2: 1.23\n",
      "Iteration: 21 AverageReturn: -35.71 |theta|_2: 1.24\n",
      "Iteration: 22 AverageReturn: -34.67 |theta|_2: 1.25\n",
      "Iteration: 23 AverageReturn: -36.24 |theta|_2: 1.25\n",
      "Iteration: 24 AverageReturn: -34.83 |theta|_2: 1.27\n",
      "Iteration: 25 AverageReturn: -32.39 |theta|_2: 1.32\n",
      "Iteration: 26 AverageReturn: -32.90 |theta|_2: 1.35\n",
      "Iteration: 27 AverageReturn: -36.19 |theta|_2: 1.38\n",
      "Iteration: 28 AverageReturn: -31.82 |theta|_2: 1.44\n",
      "Iteration: 29 AverageReturn: -33.36 |theta|_2: 1.49\n",
      "Iteration: 30 AverageReturn: -31.91 |theta|_2: 1.55\n",
      "Iteration: 31 AverageReturn: -34.04 |theta|_2: 1.59\n",
      "Iteration: 32 AverageReturn: -31.59 |theta|_2: 1.66\n",
      "Iteration: 33 AverageReturn: -29.72 |theta|_2: 1.73\n",
      "Iteration: 34 AverageReturn: -32.36 |theta|_2: 1.79\n",
      "Iteration: 35 AverageReturn: -32.44 |theta|_2: 1.85\n",
      "Iteration: 36 AverageReturn: -30.96 |theta|_2: 1.92\n",
      "Iteration: 37 AverageReturn: -31.33 |theta|_2: 2.00\n",
      "Iteration: 38 AverageReturn: -31.40 |theta|_2: 2.07\n",
      "Iteration: 39 AverageReturn: -34.21 |theta|_2: 2.14\n",
      "Iteration: 40 AverageReturn: -30.92 |theta|_2: 2.23\n",
      "Iteration: 41 AverageReturn: -30.97 |theta|_2: 2.30\n",
      "Iteration: 42 AverageReturn: -30.27 |theta|_2: 2.38\n",
      "Iteration: 43 AverageReturn: -32.68 |theta|_2: 2.46\n",
      "Iteration: 44 AverageReturn: -30.43 |theta|_2: 2.55\n",
      "Iteration: 45 AverageReturn: -31.11 |theta|_2: 2.63\n",
      "Iteration: 46 AverageReturn: -28.45 |theta|_2: 2.72\n",
      "Iteration: 47 AverageReturn: -29.06 |theta|_2: 2.81\n",
      "Iteration: 48 AverageReturn: -31.26 |theta|_2: 2.89\n",
      "Iteration: 49 AverageReturn: -26.38 |theta|_2: 2.96\n",
      "Iteration: 50 AverageReturn: -29.66 |theta|_2: 3.03\n",
      "Iteration: 51 AverageReturn: -30.66 |theta|_2: 3.12\n",
      "Iteration: 52 AverageReturn: -29.66 |theta|_2: 3.21\n",
      "Iteration: 53 AverageReturn: -28.37 |theta|_2: 3.30\n",
      "Iteration: 54 AverageReturn: -28.14 |theta|_2: 3.39\n",
      "Iteration: 55 AverageReturn: -27.50 |theta|_2: 3.48\n",
      "Iteration: 56 AverageReturn: -28.03 |theta|_2: 3.56\n",
      "Iteration: 57 AverageReturn: -27.10 |theta|_2: 3.65\n",
      "Iteration: 58 AverageReturn: -27.06 |theta|_2: 3.73\n",
      "Iteration: 59 AverageReturn: -26.89 |theta|_2: 3.82\n",
      "Iteration: 60 AverageReturn: -25.64 |theta|_2: 3.90\n",
      "Iteration: 61 AverageReturn: -26.19 |theta|_2: 3.99\n",
      "Iteration: 62 AverageReturn: -25.97 |theta|_2: 4.07\n",
      "Iteration: 63 AverageReturn: -25.33 |theta|_2: 4.16\n",
      "Iteration: 64 AverageReturn: -24.44 |theta|_2: 4.25\n",
      "Iteration: 65 AverageReturn: -23.78 |theta|_2: 4.35\n",
      "Iteration: 66 AverageReturn: -23.90 |theta|_2: 4.43\n",
      "Iteration: 67 AverageReturn: -24.45 |theta|_2: 4.51\n",
      "Iteration: 68 AverageReturn: -23.60 |theta|_2: 4.57\n",
      "Iteration: 69 AverageReturn: -22.99 |theta|_2: 4.57\n",
      "Iteration: 70 AverageReturn: -22.62 |theta|_2: 4.67\n",
      "Iteration: 71 AverageReturn: -22.74 |theta|_2: 4.73\n",
      "Iteration: 72 AverageReturn: -23.44 |theta|_2: 4.80\n",
      "Iteration: 73 AverageReturn: -22.90 |theta|_2: 4.88\n",
      "Iteration: 74 AverageReturn: -22.69 |theta|_2: 4.96\n",
      "Iteration: 75 AverageReturn: -22.43 |theta|_2: 5.05\n",
      "Iteration: 76 AverageReturn: -21.46 |theta|_2: 5.10\n",
      "Iteration: 77 AverageReturn: -21.61 |theta|_2: 5.19\n",
      "Iteration: 78 AverageReturn: -22.14 |theta|_2: 5.27\n",
      "Iteration: 79 AverageReturn: -21.15 |theta|_2: 5.26\n",
      "Iteration: 80 AverageReturn: -22.16 |theta|_2: 5.34\n",
      "Iteration: 81 AverageReturn: -22.29 |theta|_2: 5.41\n",
      "Iteration: 82 AverageReturn: -21.55 |theta|_2: 5.39\n",
      "Iteration: 83 AverageReturn: -21.43 |theta|_2: 5.47\n",
      "Iteration: 84 AverageReturn: -21.62 |theta|_2: 5.54\n",
      "Iteration: 85 AverageReturn: -21.47 |theta|_2: 5.62\n",
      "Iteration: 86 AverageReturn: -20.88 |theta|_2: 5.62\n",
      "Iteration: 87 AverageReturn: -21.78 |theta|_2: 5.69\n",
      "Iteration: 88 AverageReturn: -21.31 |theta|_2: 5.73\n",
      "Iteration: 89 AverageReturn: -21.08 |theta|_2: 5.75\n",
      "Iteration: 90 AverageReturn: -21.23 |theta|_2: 5.77\n",
      "Iteration: 91 AverageReturn: -20.71 |theta|_2: 5.78\n",
      "Iteration: 92 AverageReturn: -20.74 |theta|_2: 5.77\n",
      "Iteration: 93 AverageReturn: -21.12 |theta|_2: 5.80\n",
      "Iteration: 94 AverageReturn: -20.69 |theta|_2: 5.82\n",
      "Iteration: 95 AverageReturn: -20.44 |theta|_2: 5.86\n",
      "Iteration: 96 AverageReturn: -20.08 |theta|_2: 5.84\n",
      "Iteration: 97 AverageReturn: -21.09 |theta|_2: 5.90\n",
      "Iteration: 98 AverageReturn: -20.63 |theta|_2: 5.87\n",
      "Iteration: 99 AverageReturn: -20.78 |theta|_2: 5.93\n"
     ]
    }
   ],
   "source": [
    "n_itrs = 100\n",
    "batch_size = 2000\n",
    "discount = 0.99\n",
    "learning_rate = 0.1\n",
    "render = False  # True\n",
    "natural_step_size = 0.01\n",
    "\n",
    "# Policy training loop\n",
    "for itr in range(n_itrs):\n",
    "    # Collect trajectory loop\n",
    "    n_samples = 0\n",
    "    grad = np.zeros_like(theta)\n",
    "    policy.NN.zero_grad()\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Store cumulative returns for each time step\n",
    "    all_returns = [[] for _ in range(timestep_limit)]\n",
    "\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    all_centered_cum_rews = []\n",
    "\n",
    "    while n_samples < batch_size:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = env.reset()\n",
    "        done = False\n",
    "        # Only render the first trajectory\n",
    "        render_episode = n_samples == 0\n",
    "        # Collect a new trajectory\n",
    "        while not done:\n",
    "#             action = get_action(theta, ob, rng=rng)\n",
    "            action = get_action(ob, rng=rng)\n",
    "            next_ob, rew, done, _ = env.step(action)\n",
    "            observations.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            ob = next_ob\n",
    "            n_samples += 1\n",
    "            if render and render_episode:\n",
    "                env.render()\n",
    "                \n",
    "        # Go back in time to compute returns and accumulate gradient\n",
    "        # Compute the gradient along this trajectory\n",
    "        \n",
    "        R = 0.\n",
    "        centered_cum_rews = np.zeros(np.shape(observations)[0])\n",
    "        \n",
    "        for t in reversed(range(len(observations))):\n",
    "            # Test the implementation, but only once\n",
    "            test_once(compute_update)\n",
    "\n",
    "            R, grad_t = compute_update(\n",
    "                discount=discount,\n",
    "                R_tplus1=R,\n",
    "                theta=theta,\n",
    "                s_t=observations[t],\n",
    "                a_t=actions[t],\n",
    "                r_t=rewards[t],\n",
    "                b_t=baselines[t],\n",
    "                get_grad_logp_action=get_grad_logp_action\n",
    "            )\n",
    "            centered_cum_rews[t] = R - baselines[t]\n",
    "            all_returns[t].append(R)\n",
    "            grad += grad_t\n",
    "\n",
    "        centered_cum_rews = compute_centered_returns(discount, rewards, baselines)\n",
    "#         ccr = compute_centered_returns(discount, rewards, baselines)\n",
    "#         print(np.sum(ccr - centered_cum_rews))\n",
    "\n",
    "        episode_rewards.append(np.sum(rewards))\n",
    "        all_observations.extend(observations)\n",
    "        all_actions.extend(actions)\n",
    "        all_centered_cum_rews.extend(centered_cum_rews)\n",
    "    \n",
    "    # autodiff loss\n",
    "    obs_vars = autograd.Variable(torch.Tensor(all_observations), requires_grad=False)\n",
    "    act_vars = autograd.Variable(torch.Tensor(all_actions), requires_grad=False)\n",
    "    centered_cum_rews_vars = autograd.Variable(torch.Tensor(all_centered_cum_rews), requires_grad=False)\n",
    "    \n",
    "    logps = policy.get_logp_action(obs_vars, act_vars)\n",
    "         \n",
    "    surr_loss = torch.dot(logps, centered_cum_rews_vars)\n",
    "    surr_loss.backward()\n",
    "    \n",
    "    flat_grad = np.concatenate([p.grad.data.numpy().reshape((-1)) for p in policy.NN.parameters()])\n",
    "    grad_norm = np.linalg.norm(flat_grad)\n",
    "    \n",
    "    for p in policy.NN.parameters():\n",
    "#         print(\"current data: {}, grad: {}, update: {}\".format(p.data, p.grad.data, learning_rate * p.grad.data / (grad_norm + 1e-8)))\n",
    "        p.data += learning_rate * p.grad.data / (grad_norm + 1e-8)\n",
    "    \n",
    "    # Roughly normalize the gradient\n",
    "    norm_grad = grad / (np.linalg.norm(grad) + 1e-8)\n",
    "\n",
    "    theta += learning_rate * norm_grad\n",
    "        \n",
    "    test_once(compute_baselines)\n",
    "\n",
    "    baselines = compute_baselines(all_returns)\n",
    "    \n",
    "    print(\"Iteration: %d AverageReturn: %.2f |theta|_2: %.2f\" % (\n",
    "    itr, np.mean(episode_rewards), np.linalg.norm(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
