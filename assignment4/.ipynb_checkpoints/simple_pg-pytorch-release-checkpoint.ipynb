{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient methods\n",
    "\n",
    "We will start with the standard policy gradient algorithm. This is a batch algorithm, which means that we will collect a large number of samples per iteration, and perform a single update to the policy using these samples. Recall that the formula for policy gradient is given by\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T\\gamma^t r_t \\Big] = \n",
    "\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T \\nabla_{\\theta} \\log\\pi_{\\theta}(a_t|s_t)\\big(R_t - b(s_t)\\big) \\Big]$$\n",
    "\n",
    "- $\\pi_{\\theta}$ is a stochastic policy parameterized by $\\theta$;\n",
    "- $\\gamma$ is the discount factor;\n",
    "- $s_t$, $a_t$ and $r_t$ are the state, action, and reward at time $t$;\n",
    "- $T$ is the length of a single episode;\n",
    "- $b(s_t)$ is any funcion which does not depend on the current action $a_t$, and is called baseline;\n",
    "- $R_t$ is the discounted cumulative future return (already defined in the DQN exercise);\n",
    "Instead of optimizing this formula, we will optimize a sample-based estimation of the expectation, based on $N$ trajectories. For this you will first implement a function that computes $\\log\\pi_{\\theta}(a_t|s_t)$ given any $s,~a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import gym\n",
    "from simplepg.simple_utils import gradient_check, log_softmax, softmax, weighted_sample, include_bias, test_once, nprs\n",
    "import tests.simplepg_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a stochastic policy\n",
    "\n",
    "Let's assume that $\\pi_{\\theta}$ is a Gaussian with unit variance $\\Sigma=I$ and mean $\\mu=NN_{\\theta}(s)$, where $NN_{\\theta}$ is a Neural Network parameterized by $\\theta$.\n",
    "\n",
    "### 1. Create a Linear NN\n",
    "Use three linear layer with ReLu non-linearity and no output non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.Linear1 = nn.Linear(obs_size, 256)\n",
    "        self.Linear2 = nn.Linear(256, 256)\n",
    "        self.Linear3 = nn.Linear(256, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # YOUR CODE HERE\n",
    "        out = F.relu(self.Linear1(obs))\n",
    "        out = F.relu(self.Linear2(out))\n",
    "        out = self.Linear3(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Gaussian MLP policy\n",
    "For Policy Gradient methods the policy needs to be stochastic. In our case, we will assume the distribution is a Gaussian where the mean $\\mu_{\\theta}(o)$ is the output of an MLP given the observation $o$ and unit variance. You will need to implement the `get_action` method that, given an observation $o$, samples an action $a$ from $\\mathcal{N}(\\mu_{\\theta}(o),I)$; and the `get_logp_action` that gives the logprobability of a given action $a$ under the policy when observation $o$ is inputed. Remember the probability of a $n$-dimensional multivariate Gaussian with unit variance can be written as:\n",
    "$$\\frac{1}{\\sqrt{(2\\pi)^{n}}}\\exp{-\\frac{1}{2}(a-\\mu_{\\theta}(o))^T(a-\\mu_{\\theta}(o))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMLP_Policy(object):\n",
    "    def __init__(self, obs_size, act_size, NN):\n",
    "        self.NN = NN(obs_size, act_size)\n",
    "        \n",
    "    def get_action(self, obs, rng=np.random):\n",
    "        # YOUR CODE HERE\n",
    "        obs_var = autograd.Variable(torch.Tensor(obs), requires_grad=False)\n",
    "        mean = self.NN.forward(obs_var)\n",
    "        return rng.normal(loc=mean.data.numpy(), scale=1.)\n",
    "    \n",
    "    def get_logp_action(self, obs, action):\n",
    "        # YOUR CODE HERE\n",
    "        mean = self.NN.forward(obs)\n",
    "        return float(-0.5 * np.log(2 * np.pi) * action.shape[-1]) - 0.5 * torch.sum((action - mean) ** 2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute time-based baselines\n",
    "Any function that does not depend on the action can be used as a baseline. The most usual one is to have a state-based baseline. In our case we will keep a simple time-based baseline that is the average return obtained at that particular time-step accross all paths collected in the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baselines(all_returns):\n",
    "    baselines = np.zeros(len(all_returns))\n",
    "    for t in range(len(all_returns)):\n",
    "        # YOUR CODE HERE\n",
    "        # Update the baselines\n",
    "        if len(all_returns[t]) > 0:\n",
    "            baselines[t] = np.mean(all_returns[t])\n",
    "        else:\n",
    "            baselines[t] = 0.\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute returns\n",
    "Given the rewards obtained in a path, return the discounted returns with the formula:\n",
    "$$\n",
    "R_t = \\begin{cases}\n",
    "r_t +\\gamma R_{t+1} \\qquad\\text{ if non-terminal transition}\\\\\n",
    "r_t \\qquad\\qquad\\quad \\text{ for terminal transition}\n",
    "\\end{cases} \\qquad\\qquad (3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(discount, rewards):\n",
    "    returns = np.zeros_like(rewards)\n",
    "    R_tplus1 = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        returns[t] = rewards[t] + discount * R_tplus1\n",
    "        R_tplus1 = returns[t]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-06 20:15:48,841] Making new env: Point-v0\n"
     ]
    }
   ],
   "source": [
    "from simplepg import point_env\n",
    "env = gym.make('Point-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Store baselines for each time step.\n",
    "timestep_limit = env.spec.timestep_limit\n",
    "baselines = np.zeros(timestep_limit)\n",
    "\n",
    "# instantiate the policy\n",
    "policy = GaussianMLP_Policy(obs_dim, action_dim, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for __main__.compute_baselines passed!\n",
      "Iteration: 0 AverageReturn: -42.02 GradNorm: 3647.95\n",
      "Iteration: 1 AverageReturn: -40.57 GradNorm: 1145.47\n",
      "Iteration: 2 AverageReturn: -39.13 GradNorm: 990.36\n",
      "Iteration: 3 AverageReturn: -39.73 GradNorm: 884.72\n",
      "Iteration: 4 AverageReturn: -35.80 GradNorm: 1941.28\n",
      "Iteration: 5 AverageReturn: -36.23 GradNorm: 1924.74\n",
      "Iteration: 6 AverageReturn: -32.61 GradNorm: 1136.40\n",
      "Iteration: 7 AverageReturn: -32.93 GradNorm: 1052.11\n",
      "Iteration: 8 AverageReturn: -32.80 GradNorm: 1253.08\n",
      "Iteration: 9 AverageReturn: -29.23 GradNorm: 1216.34\n",
      "Iteration: 10 AverageReturn: -27.60 GradNorm: 848.60\n",
      "Iteration: 11 AverageReturn: -27.17 GradNorm: 936.59\n",
      "Iteration: 12 AverageReturn: -25.36 GradNorm: 951.73\n",
      "Iteration: 13 AverageReturn: -24.57 GradNorm: 649.54\n",
      "Iteration: 14 AverageReturn: -23.91 GradNorm: 934.86\n",
      "Iteration: 15 AverageReturn: -23.42 GradNorm: 1081.44\n",
      "Iteration: 16 AverageReturn: -22.63 GradNorm: 920.18\n",
      "Iteration: 17 AverageReturn: -21.98 GradNorm: 935.18\n",
      "Iteration: 18 AverageReturn: -21.73 GradNorm: 644.96\n",
      "Iteration: 19 AverageReturn: -22.19 GradNorm: 810.63\n",
      "Iteration: 20 AverageReturn: -22.50 GradNorm: 872.89\n",
      "Iteration: 21 AverageReturn: -20.44 GradNorm: 716.66\n",
      "Iteration: 22 AverageReturn: -20.07 GradNorm: 570.20\n",
      "Iteration: 23 AverageReturn: -20.53 GradNorm: 907.20\n",
      "Iteration: 24 AverageReturn: -19.62 GradNorm: 653.35\n",
      "Iteration: 25 AverageReturn: -20.33 GradNorm: 382.79\n",
      "Iteration: 26 AverageReturn: -19.96 GradNorm: 604.82\n",
      "Iteration: 27 AverageReturn: -20.10 GradNorm: 576.76\n",
      "Iteration: 28 AverageReturn: -19.57 GradNorm: 518.03\n",
      "Iteration: 29 AverageReturn: -20.12 GradNorm: 719.46\n",
      "Iteration: 30 AverageReturn: -19.32 GradNorm: 880.07\n",
      "Iteration: 31 AverageReturn: -19.55 GradNorm: 1718.52\n",
      "Iteration: 32 AverageReturn: -19.52 GradNorm: 664.49\n",
      "Iteration: 33 AverageReturn: -18.97 GradNorm: 251.41\n",
      "Iteration: 34 AverageReturn: -19.40 GradNorm: 569.23\n",
      "Iteration: 35 AverageReturn: -19.32 GradNorm: 924.64\n",
      "Iteration: 36 AverageReturn: -19.05 GradNorm: 359.51\n",
      "Iteration: 37 AverageReturn: -19.51 GradNorm: 588.94\n",
      "Iteration: 38 AverageReturn: -18.65 GradNorm: 915.60\n",
      "Iteration: 39 AverageReturn: -18.89 GradNorm: 413.11\n",
      "Iteration: 40 AverageReturn: -19.22 GradNorm: 741.80\n",
      "Iteration: 41 AverageReturn: -19.24 GradNorm: 463.62\n",
      "Iteration: 42 AverageReturn: -18.70 GradNorm: 377.75\n",
      "Iteration: 43 AverageReturn: -19.07 GradNorm: 520.56\n",
      "Iteration: 44 AverageReturn: -18.77 GradNorm: 456.01\n",
      "Iteration: 45 AverageReturn: -18.86 GradNorm: 1048.23\n",
      "Iteration: 46 AverageReturn: -18.38 GradNorm: 720.08\n",
      "Iteration: 47 AverageReturn: -18.50 GradNorm: 786.66\n",
      "Iteration: 48 AverageReturn: -19.12 GradNorm: 960.43\n",
      "Iteration: 49 AverageReturn: -18.38 GradNorm: 977.77\n"
     ]
    }
   ],
   "source": [
    "n_itrs = 50\n",
    "batch_size = 2000\n",
    "discount = 0.99\n",
    "learning_rate = 0.1\n",
    "render = False  # True\n",
    "natural_step_size = 0.01\n",
    "\n",
    "# Policy training loop\n",
    "for itr in range(n_itrs):\n",
    "    # Collect trajectory loop\n",
    "    n_samples = 0\n",
    "    policy.NN.zero_grad()\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Store cumulative returns for each time step\n",
    "    all_returns = [[] for _ in range(timestep_limit)]\n",
    "\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    all_centered_cum_rews = []\n",
    "\n",
    "    while n_samples < batch_size:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = env.reset()\n",
    "        done = False\n",
    "        # Only render the first trajectory\n",
    "        render_episode = n_samples == 0\n",
    "        # Collect a new trajectory\n",
    "        while not done:\n",
    "            action = policy.get_action(ob, rng=rng)\n",
    "            next_ob, rew, done, _ = env.step(action)\n",
    "            observations.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            ob = next_ob\n",
    "            n_samples += 1\n",
    "            if render and render_episode:\n",
    "                env.render()\n",
    "                \n",
    "        # Go back in time to compute returns \n",
    "        returns = compute_returns(discount, rewards)\n",
    "        # center the rewards by substracting the baseline\n",
    "        centered_cum_rews = returns - baselines[:len(returns)]\n",
    "        # save them in all_returns to compute time-based baseline for next iteration \n",
    "        for t, r in enumerate(returns):\n",
    "            all_returns[t].append(r)\n",
    "\n",
    "        episode_rewards.append(np.sum(rewards))\n",
    "        all_observations.extend(observations)\n",
    "        all_actions.extend(actions)\n",
    "        all_centered_cum_rews.extend(centered_cum_rews)\n",
    "    \n",
    "    # autodiff loss\n",
    "    obs_vars = autograd.Variable(torch.Tensor(all_observations), requires_grad=False)\n",
    "    act_vars = autograd.Variable(torch.Tensor(all_actions), requires_grad=False)\n",
    "    centered_cum_rews_vars = autograd.Variable(torch.Tensor(all_centered_cum_rews), requires_grad=False)\n",
    "    \n",
    "    logps = policy.get_logp_action(obs_vars, act_vars)\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    surr_loss = torch.dot(logps, centered_cum_rews_vars)\n",
    "    surr_loss.backward()\n",
    "    \n",
    "    flat_grad = np.concatenate([p.grad.data.numpy().reshape((-1)) for p in policy.NN.parameters()])\n",
    "    grad_norm = np.linalg.norm(flat_grad)\n",
    "    \n",
    "    for p in policy.NN.parameters():\n",
    "        # roughly normalize gradiend and take step\n",
    "        p.data += learning_rate * p.grad.data / (grad_norm + 1e-8)\n",
    "\n",
    "    test_once(compute_baselines)\n",
    "\n",
    "    baselines = compute_baselines(all_returns)\n",
    "    \n",
    "    print(\"Iteration: %d AverageReturn: %.2f GradNorm: %.2f\" % (\n",
    "    itr, np.mean(episode_rewards), grad_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
