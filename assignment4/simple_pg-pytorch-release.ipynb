{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient methods\n",
    "\n",
    "We will start with the standard policy gradient algorithm. This is a batch algorithm, which means that we will collect a large number of samples per iteration, and perform a single update to the policy using these samples. Recall that the formula for policy gradient is given by\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T\\gamma^t r_t \\Big] = \n",
    "\\mathbb{E}_{\\pi_{\\theta}}\\Big[ \\sum_{t=0}^T \\nabla_{\\theta} \\log\\pi_{\\theta}(a_t|s_t)\\big(R_t - b(s_t)\\big) \\Big] \\qquad\\qquad (1)$$\n",
    "\n",
    "- $\\pi_{\\theta}$ is a stochastic policy parameterized by $\\theta$;\n",
    "- $\\gamma$ is the discount factor;\n",
    "- $s_t$, $a_t$ and $r_t$ are the state, action, and reward at time $t$;\n",
    "- $T$ is the length of a single episode;\n",
    "- $b(s_t)$ is any funcion which does not depend on the current action $a_t$, and is called baseline;\n",
    "- $R_t$ is the discounted cumulative future return (already defined in the DQN exercise);\n",
    "Instead of optimizing this formula, we will optimize a sample-based estimation of the expectation, based on $N$ trajectories. For this you will first implement a function that computes $\\log\\pi_{\\theta}(a_t|s_t)$ given any $s,~a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import gym\n",
    "from simplepg.simple_utils import gradient_check, log_softmax, softmax, weighted_sample, include_bias, test_once, nprs\n",
    "import tests.simplepg_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a stochastic policy\n",
    "\n",
    "Let's assume that $\\pi_{\\theta}$ is a Gaussian with unit variance $\\Sigma=I$ and mean $\\mu=NN_{\\theta}(s)$, where $NN_{\\theta}$ is a Neural Network parameterized by $\\theta$.\n",
    "\n",
    "### 1. Create a Linear NN\n",
    "Use two hidden linear layer with 256 hidden units and ReLu non-linearity, and use a linear output layer with no output non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(MLP, self).__init__()\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        self.linear1 = nn.Linear(obs_size, 256)\n",
    "        self.linear2 = nn.Linear(256,256)\n",
    "        self.linear3 = nn.Linear(256, act_size)\n",
    "    def forward(self, obs):\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        out = F.relu(self.linear1(obs))\n",
    "        out = F.relu(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Gaussian MLP policy\n",
    "For Policy Gradient methods the policy needs to be stochastic. In our case, we will assume the distribution is a Gaussian where the mean $\\mu_{\\theta}(o)$ is the output of an MLP given the observation $o$ and unit variance. You will need to implement the `get_action` method that, given an observation $o$, samples an action $a$ from $\\mathcal{N}(\\mu_{\\theta}(o),I)$; and the `get_logp_action` that gives the logprobability of a given action $a$ under the policy when observation $o$ is inputed. Remember the probability of a $n$-dimensional multivariate Gaussian with unit variance can be written as:\n",
    "$$\\frac{1}{\\sqrt{(2\\pi)^{n}}}\\exp^{-\\frac{1}{2}(a-\\mu_{\\theta}(o))^T(a-\\mu_{\\theta}(o))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianMLP_Policy(object):\n",
    "    def __init__(self, obs_size, act_size, NN):\n",
    "        self.NN = NN(obs_size, act_size)\n",
    "        \n",
    "    def get_action(self, obs, rng=np.random):\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        obs_var = autograd.Variable(torch.from_numpy(obs).float(), requires_grad=False)\n",
    "        mean = self.NN(obs_var).data.numpy()\n",
    "        cov = np.identity(mean.shape[0])\n",
    "        sampled_action = rng.multivariate_normal(mean, cov)\n",
    "        return sampled_action\n",
    "    \n",
    "    def get_logp_action(self, obs, action):\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        # obs: Variable\n",
    "        # action: Variable\n",
    "        # log_p: Variable\n",
    "        mean_var = self.NN(obs)\n",
    "        n = action.size(1)\n",
    "        diff = action - mean_var\n",
    "        power = -0.5 * torch.sum(torch.pow(diff ,2.0) ,1)\n",
    "        log_p = torch.log(torch.exp(power) / math.sqrt(pow(2*math.pi, float(n))))\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute time-based baselines\n",
    "Any function that does not depend on the action can be used as a baseline. The most usual one is to have a state-based baseline. In our case we will keep a simple time-based baseline that is the average return obtained at that particular time-step accross all paths collected in the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_baselines(all_returns):\n",
    "    baselines = np.zeros(len(all_returns))\n",
    "    for t in range(len(all_returns)):\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        # Update the baselines\n",
    "        # all_returns: list of lists. all_returns[time_step][path]\n",
    "        baselines[t] = np.mean(all_returns[t]) if len(all_returns[t])>0 else 0\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute returns\n",
    "Given the rewards obtained in a path, return the discounted returns with the formula:\n",
    "$$\n",
    "R_t = \\begin{cases}\n",
    "r_t +\\gamma R_{t+1} \\qquad\\text{ if non-terminal transition}\\\\\n",
    "r_t \\qquad\\qquad\\quad \\text{ for terminal transition}\n",
    "\\end{cases} \\qquad\\qquad (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_returns(discount, rewards):\n",
    "    returns = np.zeros_like(rewards)\n",
    "    #\"*** YOUR CODE HERE ***\"\n",
    "    for i in np.arange(len(returns)-1,-1,-1):\n",
    "        returns[i] = rewards[i] + (0 if i == len(returns)-1 else discount*returns[i+1])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm\n",
    "You are only asked to implement the surrogate reward that we take the gradient of. We do so by approximating the expectation in Eq. (1) by a sum over paths. In other words, the surrogate function can be written as:\n",
    "$$ \\sum_{i=0}^N\\sum_{t=0}^{T_i} \\log\\pi_{\\theta}(a^i_t|s^i_t)\\big(R^i_t - b(t)\\big) \\qquad\\qquad (3)$$\n",
    "If you implemented it correctly, the reward should reach arround -20 in about 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 22:22:39,338] Making new env: Point-v0\n"
     ]
    }
   ],
   "source": [
    "from simplepg import point_env\n",
    "env = gym.make('Point-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Store baselines for each time step.\n",
    "timestep_limit = env.spec.timestep_limit\n",
    "baselines = np.zeros(timestep_limit)\n",
    "\n",
    "# instantiate the policy\n",
    "policy = GaussianMLP_Policy(obs_dim, action_dim, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for __main__.compute_baselines passed!\n",
      "Iteration: 0 AverageReturn: -41.47 GradNorm: 4683.90\n",
      "Iteration: 1 AverageReturn: -39.78 GradNorm: 1197.01\n",
      "Iteration: 2 AverageReturn: -39.59 GradNorm: 887.91\n",
      "Iteration: 3 AverageReturn: -36.59 GradNorm: 1195.10\n",
      "Iteration: 4 AverageReturn: -36.08 GradNorm: 997.15\n",
      "Iteration: 5 AverageReturn: -34.29 GradNorm: 1128.68\n",
      "Iteration: 6 AverageReturn: -32.95 GradNorm: 1409.37\n",
      "Iteration: 7 AverageReturn: -32.17 GradNorm: 1654.11\n",
      "Iteration: 8 AverageReturn: -31.85 GradNorm: 2034.69\n",
      "Iteration: 9 AverageReturn: -28.44 GradNorm: 1100.82\n",
      "Iteration: 10 AverageReturn: -28.17 GradNorm: 1134.04\n",
      "Iteration: 11 AverageReturn: -27.77 GradNorm: 1483.22\n",
      "Iteration: 12 AverageReturn: -26.80 GradNorm: 839.32\n",
      "Iteration: 13 AverageReturn: -25.60 GradNorm: 1296.70\n",
      "Iteration: 14 AverageReturn: -23.46 GradNorm: 748.20\n",
      "Iteration: 15 AverageReturn: -24.82 GradNorm: 1173.66\n",
      "Iteration: 16 AverageReturn: -23.46 GradNorm: 786.92\n",
      "Iteration: 17 AverageReturn: -22.39 GradNorm: 878.82\n",
      "Iteration: 18 AverageReturn: -23.22 GradNorm: 1867.40\n",
      "Iteration: 19 AverageReturn: -22.28 GradNorm: 518.45\n",
      "Iteration: 20 AverageReturn: -21.29 GradNorm: 606.31\n",
      "Iteration: 21 AverageReturn: -21.73 GradNorm: 1140.74\n",
      "Iteration: 22 AverageReturn: -21.17 GradNorm: 1414.94\n",
      "Iteration: 23 AverageReturn: -21.33 GradNorm: 656.82\n",
      "Iteration: 24 AverageReturn: -20.71 GradNorm: 880.13\n",
      "Iteration: 25 AverageReturn: -20.56 GradNorm: 614.01\n",
      "Iteration: 26 AverageReturn: -20.91 GradNorm: 1366.66\n",
      "Iteration: 27 AverageReturn: -20.98 GradNorm: 1031.35\n",
      "Iteration: 28 AverageReturn: -20.54 GradNorm: 1023.36\n",
      "Iteration: 29 AverageReturn: -19.97 GradNorm: 1108.04\n",
      "Iteration: 30 AverageReturn: -19.54 GradNorm: 297.48\n",
      "Iteration: 31 AverageReturn: -19.18 GradNorm: 594.79\n",
      "Iteration: 32 AverageReturn: -19.92 GradNorm: 847.16\n",
      "Iteration: 33 AverageReturn: -20.16 GradNorm: 1158.32\n",
      "Iteration: 34 AverageReturn: -20.02 GradNorm: 434.58\n",
      "Iteration: 35 AverageReturn: -20.04 GradNorm: 525.81\n",
      "Iteration: 36 AverageReturn: -20.78 GradNorm: 873.64\n",
      "Iteration: 37 AverageReturn: -19.45 GradNorm: 1104.83\n",
      "Iteration: 38 AverageReturn: -19.80 GradNorm: 723.37\n",
      "Iteration: 39 AverageReturn: -18.98 GradNorm: 817.94\n",
      "Iteration: 40 AverageReturn: -19.28 GradNorm: 795.82\n",
      "Iteration: 41 AverageReturn: -18.59 GradNorm: 755.74\n",
      "Iteration: 42 AverageReturn: -19.32 GradNorm: 576.81\n",
      "Iteration: 43 AverageReturn: -19.43 GradNorm: 633.93\n",
      "Iteration: 44 AverageReturn: -18.78 GradNorm: 748.62\n",
      "Iteration: 45 AverageReturn: -18.50 GradNorm: 585.67\n",
      "Iteration: 46 AverageReturn: -19.06 GradNorm: 1132.68\n",
      "Iteration: 47 AverageReturn: -19.14 GradNorm: 565.58\n",
      "Iteration: 48 AverageReturn: -19.11 GradNorm: 1300.42\n",
      "Iteration: 49 AverageReturn: -18.65 GradNorm: 942.61\n"
     ]
    }
   ],
   "source": [
    "n_itrs = 50\n",
    "batch_size = 2000\n",
    "discount = 0.99\n",
    "learning_rate = 0.1\n",
    "render = False  # True  # see setup_instructions.pdf to render point-mass policy\n",
    "natural_step_size = 0.01\n",
    "\n",
    "# Policy training loop\n",
    "for itr in range(n_itrs):\n",
    "    # Collect trajectory loop\n",
    "    n_samples = 0\n",
    "    policy.NN.zero_grad()\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Store cumulative returns for each time step\n",
    "    all_returns = [[] for _ in range(timestep_limit)]\n",
    "\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    all_centered_cum_rews = []\n",
    "\n",
    "    while n_samples < batch_size:\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = env.reset()\n",
    "        done = False\n",
    "        # Only render the first trajectory\n",
    "        render_episode = n_samples == 0\n",
    "        # Collect a new trajectory\n",
    "        while not done:\n",
    "            action = policy.get_action(ob)\n",
    "            next_ob, rew, done, _ = env.step(action)\n",
    "            observations.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(rew)\n",
    "            ob = next_ob\n",
    "            n_samples += 1\n",
    "            if render and render_episode:\n",
    "                env.render()\n",
    "                \n",
    "        # Go back in time to compute returns \n",
    "        returns = compute_returns(discount, rewards)\n",
    "        # center the rewards by substracting the baseline\n",
    "        centered_cum_rews = returns - baselines[:len(returns)]\n",
    "        # save them in all_returns to compute time-based baseline for next iteration \n",
    "        for t, r in enumerate(returns):\n",
    "            all_returns[t].append(r)\n",
    "\n",
    "        episode_rewards.append(np.sum(rewards))\n",
    "        all_observations.extend(observations)\n",
    "        all_actions.extend(actions)\n",
    "        all_centered_cum_rews.extend(centered_cum_rews)\n",
    "    \n",
    "    # autodiff loss\n",
    "    obs_vars = autograd.Variable(torch.Tensor(all_observations), requires_grad=False)\n",
    "    act_vars = autograd.Variable(torch.Tensor(all_actions), requires_grad=False)\n",
    "    centered_cum_rews_vars = autograd.Variable(torch.Tensor(all_centered_cum_rews), requires_grad=False)\n",
    "    \n",
    "    logps = policy.get_logp_action(obs_vars, act_vars)\n",
    "        \n",
    "    #\"*** YOUR CODE HERE ***\"\n",
    "    surr_loss = torch.sum(logps * centered_cum_rews_vars)\n",
    "    \n",
    "    surr_loss.backward()\n",
    "    \n",
    "    flat_grad = np.concatenate([p.grad.data.numpy().reshape((-1)) for p in policy.NN.parameters()])\n",
    "    grad_norm = np.linalg.norm(flat_grad)\n",
    "    \n",
    "    for p in policy.NN.parameters():\n",
    "        # roughly normalize gradiend and take step\n",
    "        p.data += learning_rate * p.grad.data / (grad_norm + 1e-8)\n",
    "\n",
    "    test_once(compute_baselines)\n",
    "\n",
    "    baselines = compute_baselines(all_returns)\n",
    "    \n",
    "    print(\"Iteration: %d AverageReturn: %.2f GradNorm: %.2f\" % (\n",
    "    itr, np.mean(episode_rewards), grad_norm))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deeprlbootcamp]",
   "language": "python",
   "name": "conda-env-deeprlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
