{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "The goal of a DQN agent is to maximize the future discounted return at each timestep $t$, namely\n",
    "\n",
    "$$ R_t = \\sum_{t'=t}^T \\gamma^{t'-t}r_{t'} $$\n",
    "\n",
    "assuming the environment episode ends at timestep $T$. The optimal action-value function $Q^*(s; a)$ defines the maximum discounted return achievable, i.e. when following an optimal policy $\\pi^*$. This optimal action-value function satisfies a recursive relationship called the Bellman optimality Eq. $(1)$, where $\\mathcal{S}$ is the distribution over next states $s'$ given a state $s_t$ and action $a_t$:\n",
    "$$\n",
    "Q^*(s,a) := \\max_{\\pi}\\mathbb{E}_{\\pi} \\Big[R_t ~\\big|~ s_t=s, ~a_t=a\\Big] \\implies Q^*(s,a) = \\mathbb{E}_{s'\\sim \\mathcal{S}}\\Big[r + \\gamma \\max_{a'} Q^*(s',a') ~\\big|~ s, a\\Big] \\qquad (1)\n",
    "$$  \n",
    "\n",
    "Generally, we can estimate this optimal Q-function by updating the Q-value function in an iterative fashion as\n",
    "$$ Q_{i+1}(s,a) = \\mathbb{E}_{s'\\sim \\mathcal{S}}\\Big[r + \\gamma \\max_{a'} Q_i(s',a') ~\\big|~ s, a\\Big] \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (2)$$\n",
    "which ultimately converges to $Q^*$ as the iterations $i$ goes to infinity. In DQN we use a function approximator to represent the Q-value function. Therefore, instead of assigning values as in Eq. $(2)$ we solve a regression problem, as detailed below in Section 2. Also, instead of trying to impose Eq. $(2)$ in all $(s,a)$ pairs, they are sampled from a *replay buffer* that at every iteration received new pairs obtained by executing in the environment the actions given by an \"epsilon-greedy\" sampling proceedure also described in Section 2.\n",
    "\n",
    "In this assignemnt you will be asked to implement three parts:\n",
    "- Define a Neural Network class that will be used as the Q-function approximator.\n",
    "- Implement the epsilon-greedy sampling proceedure.\n",
    "- Implement the Q-learning loss function.\n",
    "\n",
    "Then you will be able to test your algorithm in two environments: a simple grid-world and a more complex Atari game called Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import helpers, gym environments, and other needed dependencies\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path as osp\n",
    "import click\n",
    "import gym\n",
    "\n",
    "from simpledqn.replay_buffer import ReplayBuffer\n",
    "import logger\n",
    "from simpledqn.wrappers import NoopResetEnv, EpisodicLifeEnv\n",
    "from simpledqn import gridworld_env\n",
    "from simpledqn.main import assert_allclose, preprocess_obs_gridworld, preprocess_obs_ram, LinearSchedule\n",
    "\n",
    "nprs = np.random.RandomState\n",
    "rng = nprs(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construcing a Neural Network\n",
    "Build a NN with **3 linear layers** (take 256 for all hidden sizes) and **relu** non-linearities at each layer output but the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class NN_linear(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN_linear, self).__init__()\n",
    "        self.Linear = nn.Linear(obs_size, act_size)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.Linear(obs)\n",
    "        return out\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(NN, self).__init__()\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        self.linear1 = NN_linear(obs_size, 256)\n",
    "        self.linear2 = NN_linear(256, 256)\n",
    "        self.linear3 = NN_linear(256, act_size)\n",
    "    def forward(self, obs):\n",
    "        #\"*** YOUR CODE HERE ***\"\n",
    "        out = F.relu(self.linear1(obs))\n",
    "        out = F.relu(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the Q-function approximators\n",
    "\n",
    "The function $Q(s,a; \\theta)$ is trained to approximate $Q^*(s,a)$ over time using a loss function defined as:\n",
    "$$ \\mathbb{E}_{(s,a,s')\\sim\\mathcal{D}}\\big[(y-Q(s,a;\\theta))^2\\big], \\qquad\\text{ where }\\quad y= \\begin{cases}\n",
    "r+\\gamma\\max_{a'}Q(s',a';\\theta') \\qquad\\text{ if non-terminal transition}\\\\\n",
    "r \\qquad\\qquad\\qquad\\qquad\\qquad\\text{ for terminal transition}\n",
    "\\end{cases} \\qquad\\qquad (3)\n",
    "$$\n",
    "where the network $Q(s; a; \\theta')$ is called the target network, and its parameters $\\theta'$ are updated (i.e. set to the current value of $\\theta$) at a specific interval.\n",
    "DQN is inherently off-policy, which means that we can update the agent towards the goal behavior through using data that is sampled from arbitrary behavior. Therefore, all sampled $(s; a; s'; r)$ tuples are stored in a replay buffer $\\mathcal{D}$.\n",
    "The approximator $Q(s,a; \\theta)$ is updated by minimizing the loss described in Eq. $(3)$. In between updates, we add new tuples $(s,a,s',r)$ to the replay buffer by taking actions in the environment with and **epsilon greedy** proceedure:\n",
    "\n",
    "**for** $t$ from 1 to T do:\n",
    "* with probability $\\epsilon$ select random action $a_t$, otherwise select $a_t = \\max_a Q(s, a; \\theta)$\n",
    "* execute action $a_t$ in environment and observe reward $r_t$, next state $s_{t+1}$ and episode termination signal $d_t$\n",
    "* store transition $(s_t, a_t, r_t, s_{t+1}, d_t)$ in $\\mathcal{D}$.\n",
    "\n",
    "**end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the next DQN class do the following:*\n",
    "- complete the **epsilon greedy** action sampling\n",
    "- write the full `compute_q_learning_loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, env, obs_dim, act_dim, obs_preprocessor, replay_buffer, NN, \n",
    "                 opt_batch_size, discount, initial_step, max_steps, learning_start_itr, target_q_update_freq,\n",
    "                 train_q_freq, log_freq, final_eps, initial_eps, fraction_eps, render):\n",
    "        self._env = env\n",
    "        self._obs_dim = obs_dim\n",
    "        self._act_dim = act_dim\n",
    "        self._obs_preprocessor = obs_preprocessor\n",
    "        self._replay_buffer = replay_buffer\n",
    "        self._initial_step = initial_step\n",
    "        self._max_steps = max_steps\n",
    "        self._target_q_update_freq = target_q_update_freq\n",
    "        self._learning_start_itr = learning_start_itr\n",
    "        self._train_q_freq = train_q_freq\n",
    "        self._log_freq = log_freq\n",
    "        self._opt_batch_size = opt_batch_size\n",
    "        self._discount = discount\n",
    "        self._render = render\n",
    "\n",
    "        self._q = NN(self._obs_dim, self._act_dim)  # Q function which params are optimized\n",
    "        self._qt = NN(self._obs_dim, self._act_dim)  # target Q copying the params in Q after several updates\n",
    "        self._qt.requires_grad = False\n",
    "        \n",
    "        self.optimizer = optim.Adam(self._q.parameters(), lr=0.0001)\n",
    "\n",
    "        self.exploration = LinearSchedule(  # gives value of eps across iterations\n",
    "            schedule_timesteps=int(fraction_eps * max_steps),\n",
    "            initial_p=initial_eps,\n",
    "            final_p=final_eps)\n",
    "\n",
    "    def eps_greedy(self, obs, epsilon):\n",
    "        # Check Q function, do argmax.\n",
    "        rnd = rng.rand()\n",
    "        if rnd > epsilon:\n",
    "            obs = self._obs_preprocessor(obs)\n",
    "            #\"*** YOUR CODE HERE ***\"\n",
    "            # compute q_values of obs\n",
    "            obs_var = autograd.Variable(torch.from_numpy(obs), requires_grad=False)\n",
    "            q_values = self._qt(obs_var) #TO DO: or self._q?\n",
    "            # return the greedy action\n",
    "            return np.argmax(q_values.data.numpy())\n",
    "        else:\n",
    "            return rng.randint(0, self._act_dim)\n",
    "\n",
    "    def compute_q_learning_loss(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "        \"\"\"\n",
    "        :param l_obs: A np.array holding a list of observations. Should be of shape N * |S|.\n",
    "        :param l_act: A np.array variable holding a list of actions. Should be of shape N.\n",
    "        :param l_rew: A np.array variable holding a list of rewards. Should be of shape N.\n",
    "        :param l_next_obs: A np.array variable holding a list of observations at the next time step. Should be of\n",
    "        shape N * |S|.\n",
    "        :param l_done: A np.array variable holding a list of binary values (indicating whether episode ended after this\n",
    "        time step). Should be of shape N.\n",
    "        :return: A PyTorch Variable holding a scalar loss.\n",
    "        \"\"\"\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # wrap the observations into Variables\n",
    "        l_next_obs_var = autograd.Variable(torch.Tensor(l_next_obs), requires_grad=False)\n",
    "        l_obs_var = autograd.Variable(torch.Tensor(l_obs), requires_grad=False)\n",
    "\n",
    "        # compute Q values of the next_obs based on the target Q network self._qt, and convert back to numpy\n",
    "        qt_next = self._qt(l_next_obs_var).data.numpy()  # shape (N, act_dim)\n",
    "        qt_next = np.amax(qt_next, 1) # shape N\n",
    "        \n",
    "        # compute the target for the MSELoss (you can do it entirely in numpy). Use self._discount\n",
    "        target = l_rew + (1-l_done) * self._discount * qt_next\n",
    "\n",
    "        # wrap into a Variable\n",
    "        target = autograd.Variable(torch.Tensor(target), requires_grad=False)\n",
    "        # compute Q values self._q of current obs and select the one corresponding to the action that was taken\n",
    "        q_all = self._q(l_obs_var)\n",
    "        \n",
    "        l_act_tensor = torch.from_numpy(l_act).long().unsqueeze(1)\n",
    "        l_act_var = autograd.Variable(l_act_tensor, requires_grad=False)\n",
    "        q_sel = torch.gather(q_all, 1, l_act_var)\n",
    "\n",
    "        # form the MSELOss and compute it\n",
    "        #loss = autograd.Variable(torch.Tensor([0]), requires_grad=False)\n",
    "        loss = F.mse_loss(q_sel, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train_q(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "        \"\"\"Update Q-value function by sampling from the replay buffer.\"\"\"\n",
    "        self._q.zero_grad()\n",
    "        \n",
    "        l_obs = self._obs_preprocessor(l_obs)\n",
    "        l_next_obs = self._obs_preprocessor(l_next_obs)\n",
    "        \n",
    "        loss = self.compute_q_learning_loss(\n",
    "            l_obs, l_act, l_rew, l_next_obs, l_done)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.data\n",
    "\n",
    "    def _update_target_q(self):\n",
    "        \"\"\"Update the target Q-value function by copying the current Q-value function weights.\"\"\"\n",
    "        q_params_dict = dict(self._q.named_parameters())\n",
    "        self._qt.load_state_dict(q_params_dict)\n",
    "\n",
    "    def train(self):\n",
    "        obs = self._env.reset()\n",
    "\n",
    "        episode_rewards = []\n",
    "        n_episodes = 0\n",
    "        l_episode_return = deque([], maxlen=10)\n",
    "        l_discounted_episode_return = deque([], maxlen=10)\n",
    "        l_tq_squared_error = deque(maxlen=50)\n",
    "        log_itr = -1\n",
    "        for itr in range(self._initial_step, self._max_steps):\n",
    "            act = self.eps_greedy(obs[np.newaxis, :],\n",
    "                                  self.exploration.value(itr))\n",
    "            next_obs, rew, done, _ = self._env.step(act)\n",
    "            if self._render:\n",
    "                self._env.render()\n",
    "            self._replay_buffer.add(obs, act, rew, next_obs, float(done))\n",
    "\n",
    "            episode_rewards.append(rew)\n",
    "\n",
    "            if done:\n",
    "                obs = self._env.reset()\n",
    "                episode_return = np.sum(episode_rewards)\n",
    "                discounted_episode_return = np.sum(\n",
    "                    episode_rewards * self._discount ** np.arange(len(episode_rewards)))\n",
    "                l_episode_return.append(episode_return)\n",
    "                l_discounted_episode_return.append(discounted_episode_return)\n",
    "                episode_rewards = []\n",
    "                n_episodes += 1\n",
    "            else:\n",
    "                obs = next_obs\n",
    "\n",
    "            if itr % self._target_q_update_freq == 0 and itr > self._learning_start_itr:\n",
    "                self._update_target_q()\n",
    "\n",
    "            if itr % self._train_q_freq == 0 and itr > self._learning_start_itr:\n",
    "                # Sample from replay buffer.\n",
    "                l_obs, l_act, l_rew, l_obs_prime, l_done = self._replay_buffer.sample(\n",
    "                    self._opt_batch_size)\n",
    "                # Train Q value function with sampled data.\n",
    "                td_squared_error = self.train_q(\n",
    "                    l_obs, l_act, l_rew, l_obs_prime, l_done)\n",
    "                l_tq_squared_error.append(td_squared_error)\n",
    "\n",
    "            if (itr + 1) % self._log_freq == 0 and len(l_episode_return) > 5:\n",
    "                log_itr += 1\n",
    "                logger.logkv('Iteration', log_itr)\n",
    "                logger.logkv('Steps', itr)\n",
    "                logger.logkv('Epsilon', self.exploration.value(itr))\n",
    "                logger.logkv('Episodes', n_episodes)\n",
    "                logger.logkv('AverageReturn', np.mean(l_episode_return))\n",
    "                logger.logkv('AverageDiscountedReturn',\n",
    "                             np.mean(l_discounted_episode_return))\n",
    "                logger.logkv('TDError^2', np.mean(l_tq_squared_error))\n",
    "                logger.dumpkvs()\n",
    "#                 self._q.dump(logger.get_dir() + '/weights.pkl')\n",
    "\n",
    "    def test(self, epsilon):\n",
    "        try:\n",
    "            self._q.set_params(self._q.load(logger.get_dir() + '/weights.pkl'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        obs = self._env.reset()\n",
    "        while True:\n",
    "            act = self.eps_greedy(obs[np.newaxis, :], epsilon)\n",
    "            obs_prime, rew, done, _ = self._env.step(act)\n",
    "            self._env.render()\n",
    "            if done:\n",
    "                obs = self._env.reset()\n",
    "                print('Done!')\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                obs = obs_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Test the algorithm on grid world\n",
    "Now let's train a simple GridWorld to test out our algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 22:32:47,690] Making new env: GridWorld-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.21811724]\n",
      "Test for compute_q_learning_loss passed!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "test_dir = \"data/local/dqn_gridworld_test\"\n",
    "log_dir = \"data/local/dqn_gridworld2\"\n",
    "logger.session(log_dir).__enter__()\n",
    "env.seed(42)\n",
    "\n",
    "# Initialize the replay buffer that we will use.\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# Initialize DQN training procedure.\n",
    "dqn_gridworld = DQN(\n",
    "    env=env,\n",
    "    obs_dim=env.observation_space.n,\n",
    "    act_dim=env.action_space.n,\n",
    "    NN=NN_linear,\n",
    "    obs_preprocessor=preprocess_obs_gridworld,\n",
    "    replay_buffer=replay_buffer,\n",
    "    opt_batch_size=64,\n",
    "    # DQN gamma parameter\n",
    "    discount=0.99,\n",
    "    # Training procedure length\n",
    "    initial_step=0,\n",
    "    max_steps=100000,\n",
    "    learning_start_itr=1000,\n",
    "    # Frequency of copying the actual Q to the target Q\n",
    "    target_q_update_freq=100,\n",
    "    # Frequency of updating the Q-value function\n",
    "    train_q_freq=4,\n",
    "    # Exploration parameters\n",
    "    initial_eps=1.0,\n",
    "    final_eps=0.05,\n",
    "    fraction_eps=0.1,\n",
    "    # Logging\n",
    "    log_freq=1000,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "from simpledqn.main import test_loss\n",
    "test_loss(dqn_gridworld, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the previous test, let's train the full policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| Iteration               | 0        |\n",
      "| Steps                   | 999      |\n",
      "| Epsilon                 | 0.90509  |\n",
      "| Episodes                | 53       |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.091352 |\n",
      "| TDError^2               | nan      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/deeprlbootcamp/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Applications/anaconda/envs/deeprlbootcamp/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| Iteration               | 1        |\n",
      "| Steps                   | 1999     |\n",
      "| Epsilon                 | 0.8101   |\n",
      "| Episodes                | 108      |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.077004 |\n",
      "| TDError^2               | 0.074823 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 2        |\n",
      "| Steps                   | 2999     |\n",
      "| Epsilon                 | 0.7151   |\n",
      "| Episodes                | 165      |\n",
      "| AverageReturn           | 0        |\n",
      "| AverageDiscountedReturn | 0        |\n",
      "| TDError^2               | 0.062754 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 3        |\n",
      "| Steps                   | 3999     |\n",
      "| Epsilon                 | 0.6201   |\n",
      "| Episodes                | 234      |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.093207 |\n",
      "| TDError^2               | 0.05468  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 4        |\n",
      "| Steps                   | 4999     |\n",
      "| Epsilon                 | 0.5251   |\n",
      "| Episodes                | 301      |\n",
      "| AverageReturn           | 0.6      |\n",
      "| AverageDiscountedReturn | 0.51879  |\n",
      "| TDError^2               | 0.045616 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 5        |\n",
      "| Steps                   | 5999     |\n",
      "| Epsilon                 | 0.4301   |\n",
      "| Episodes                | 378      |\n",
      "| AverageReturn           | 0.2      |\n",
      "| AverageDiscountedReturn | 0.18456  |\n",
      "| TDError^2               | 0.040972 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 6        |\n",
      "| Steps                   | 6999     |\n",
      "| Epsilon                 | 0.3351   |\n",
      "| Episodes                | 435      |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.094148 |\n",
      "| TDError^2               | 0.034993 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 7        |\n",
      "| Steps                   | 7999     |\n",
      "| Epsilon                 | 0.24009  |\n",
      "| Episodes                | 498      |\n",
      "| AverageReturn           | 0.2      |\n",
      "| AverageDiscountedReturn | 0.18737  |\n",
      "| TDError^2               | 0.028431 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 8        |\n",
      "| Steps                   | 8999     |\n",
      "| Epsilon                 | 0.14509  |\n",
      "| Episodes                | 562      |\n",
      "| AverageReturn           | 0        |\n",
      "| AverageDiscountedReturn | 0        |\n",
      "| TDError^2               | 0.023904 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 9        |\n",
      "| Steps                   | 9999     |\n",
      "| Epsilon                 | 0.050095 |\n",
      "| Episodes                | 599      |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.084294 |\n",
      "| TDError^2               | 0.019751 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 10       |\n",
      "| Steps                   | 10999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 632      |\n",
      "| AverageReturn           | 0.2      |\n",
      "| AverageDiscountedReturn | 0.16379  |\n",
      "| TDError^2               | 0.014555 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 11       |\n",
      "| Steps                   | 11999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 664      |\n",
      "| AverageReturn           | 0.1      |\n",
      "| AverageDiscountedReturn | 0.094148 |\n",
      "| TDError^2               | 0.011821 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 12       |\n",
      "| Steps                   | 12999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 809      |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.94436  |\n",
      "| TDError^2               | 0.011044 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 13       |\n",
      "| Steps                   | 13999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 964      |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95099  |\n",
      "| TDError^2               | 0.011273 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 14       |\n",
      "| Steps                   | 14999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 1123     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.94721  |\n",
      "| TDError^2               | 0.011528 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 15       |\n",
      "| Steps                   | 15999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 1284     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95099  |\n",
      "| TDError^2               | 0.011049 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 16        |\n",
      "| Steps                   | 16999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 1442      |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.854     |\n",
      "| TDError^2               | 0.0094323 |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 17       |\n",
      "| Steps                   | 17999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 1604     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95099  |\n",
      "| TDError^2               | 0.010256 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 18        |\n",
      "| Steps                   | 18999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 1764      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94353   |\n",
      "| TDError^2               | 0.0095138 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 19        |\n",
      "| Steps                   | 19999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 1923      |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.85494   |\n",
      "| TDError^2               | 0.0089441 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 20        |\n",
      "| Steps                   | 20999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2085      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95004   |\n",
      "| TDError^2               | 0.0076041 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 21        |\n",
      "| Steps                   | 21999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2244      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95004   |\n",
      "| TDError^2               | 0.0064127 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 22        |\n",
      "| Steps                   | 22999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2404      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0084456 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 23        |\n",
      "| Steps                   | 23999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2563      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0064245 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 24        |\n",
      "| Steps                   | 24999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2723      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0075563 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 25        |\n",
      "| Steps                   | 25999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 2884      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0059904 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 26        |\n",
      "| Steps                   | 26999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3042      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0074594 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 27        |\n",
      "| Steps                   | 27999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3202      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0054952 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 28        |\n",
      "| Steps                   | 28999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3362      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0063458 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 29        |\n",
      "| Steps                   | 29999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3522      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95004   |\n",
      "| TDError^2               | 0.0056208 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 30        |\n",
      "| Steps                   | 30999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3681      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0051003 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 31        |\n",
      "| Steps                   | 31999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 3842      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0049228 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 32        |\n",
      "| Steps                   | 32999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4003      |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.854     |\n",
      "| TDError^2               | 0.0041008 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 33        |\n",
      "| Steps                   | 33999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4161      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94626   |\n",
      "| TDError^2               | 0.0030242 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 34        |\n",
      "| Steps                   | 34999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4317      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94438   |\n",
      "| TDError^2               | 0.0035409 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 35        |\n",
      "| Steps                   | 35999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4476      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0029583 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 36        |\n",
      "| Steps                   | 36999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4634      |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.85305   |\n",
      "| TDError^2               | 0.0029798 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 37        |\n",
      "| Steps                   | 37999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4795      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0031517 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 38        |\n",
      "| Steps                   | 38999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 4955      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94815   |\n",
      "| TDError^2               | 0.0019956 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 39        |\n",
      "| Steps                   | 39999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 5116      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0018903 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 40        |\n",
      "| Steps                   | 40999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 5275      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94534   |\n",
      "| TDError^2               | 0.0018912 |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 41       |\n",
      "| Steps                   | 41999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 5434     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.94156  |\n",
      "| TDError^2               | 0.002019 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 42        |\n",
      "| Steps                   | 42999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 5594      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0013095 |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 43       |\n",
      "| Steps                   | 43999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 5754     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.94531  |\n",
      "| TDError^2               | 0.001506 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 44        |\n",
      "| Steps                   | 44999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 5916      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0012307 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 45        |\n",
      "| Steps                   | 45999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6078      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9472    |\n",
      "| TDError^2               | 0.0011052 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 46        |\n",
      "| Steps                   | 46999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6200      |\n",
      "| AverageReturn           | 0.8       |\n",
      "| AverageDiscountedReturn | 0.75704   |\n",
      "| TDError^2               | 0.0014229 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 47        |\n",
      "| Steps                   | 47999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6305      |\n",
      "| AverageReturn           | 0.2       |\n",
      "| AverageDiscountedReturn | 0.15132   |\n",
      "| TDError^2               | 0.0022801 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 48        |\n",
      "| Steps                   | 48999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6454      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0024836 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 49        |\n",
      "| Steps                   | 49999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6600      |\n",
      "| AverageReturn           | 0.8       |\n",
      "| AverageDiscountedReturn | 0.75701   |\n",
      "| TDError^2               | 0.0019243 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 50        |\n",
      "| Steps                   | 50999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6731      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94542   |\n",
      "| TDError^2               | 0.0016162 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 51        |\n",
      "| Steps                   | 51999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 6878      |\n",
      "| AverageReturn           | 0.8       |\n",
      "| AverageDiscountedReturn | 0.74221   |\n",
      "| TDError^2               | 0.0035218 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 52        |\n",
      "| Steps                   | 52999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7025      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94817   |\n",
      "| TDError^2               | 0.0020907 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 53        |\n",
      "| Steps                   | 53999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7183      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0023268 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 54        |\n",
      "| Steps                   | 54999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7343      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0022476 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 55        |\n",
      "| Steps                   | 55999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7505      |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.854     |\n",
      "| TDError^2               | 0.0019097 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 56        |\n",
      "| Steps                   | 56999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7666      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0020106 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 57        |\n",
      "| Steps                   | 57999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 7827      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94815   |\n",
      "| TDError^2               | 0.0031277 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 58         |\n",
      "| Steps                   | 58999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 7989       |\n",
      "| AverageReturn           | 0.9        |\n",
      "| AverageDiscountedReturn | 0.854      |\n",
      "| TDError^2               | 0.00085906 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 59        |\n",
      "| Steps                   | 59999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 8148      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0011824 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 60         |\n",
      "| Steps                   | 60999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 8310       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00082055 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 61         |\n",
      "| Steps                   | 61999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 8470       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00048689 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 62        |\n",
      "| Steps                   | 62999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 8633      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0001764 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 63         |\n",
      "| Steps                   | 63999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 8793       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00013961 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 64         |\n",
      "| Steps                   | 64999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 8928       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.95099    |\n",
      "| TDError^2               | 0.00049801 |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 65       |\n",
      "| Steps                   | 65999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 9064     |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.94721  |\n",
      "| TDError^2               | 0.001038 |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 66         |\n",
      "| Steps                   | 66999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 9221       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00043546 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 67         |\n",
      "| Steps                   | 67999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 9384       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.94629    |\n",
      "| TDError^2               | 0.00076797 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 68        |\n",
      "| Steps                   | 68999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 9542      |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0010483 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 69         |\n",
      "| Steps                   | 69999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 9704       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.94909    |\n",
      "| TDError^2               | 0.00079955 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 70         |\n",
      "| Steps                   | 70999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 9863       |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00034144 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 71        |\n",
      "| Steps                   | 71999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 10005     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94435   |\n",
      "| TDError^2               | 0.0016721 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 72         |\n",
      "| Steps                   | 72999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 10163      |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00084058 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 73         |\n",
      "| Steps                   | 73999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 10322      |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00092193 |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 74       |\n",
      "| Steps                   | 74999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 10457    |\n",
      "| AverageReturn           | 0.5      |\n",
      "| AverageDiscountedReturn | 0.46795  |\n",
      "| TDError^2               | 0.001017 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 75        |\n",
      "| Steps                   | 75999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 10581     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0026338 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 76         |\n",
      "| Steps                   | 76999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 10741      |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.9491     |\n",
      "| TDError^2               | 0.00078881 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 77         |\n",
      "| Steps                   | 77999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 10898      |\n",
      "| AverageReturn           | 0.9        |\n",
      "| AverageDiscountedReturn | 0.854      |\n",
      "| TDError^2               | 0.00062791 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 78         |\n",
      "| Steps                   | 78999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 11004      |\n",
      "| AverageReturn           | 0.9        |\n",
      "| AverageDiscountedReturn | 0.85025    |\n",
      "| TDError^2               | 0.00094903 |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 79       |\n",
      "| Steps                   | 79999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 11164    |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95099  |\n",
      "| TDError^2               | 0.001735 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 80        |\n",
      "| Steps                   | 80999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 11324     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95004   |\n",
      "| TDError^2               | 0.0018957 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 81        |\n",
      "| Steps                   | 81999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 11485     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0008685 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 82        |\n",
      "| Steps                   | 82999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 11645     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94815   |\n",
      "| TDError^2               | 0.0014358 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 83         |\n",
      "| Steps                   | 83999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 11801      |\n",
      "| AverageReturn           | 0.9        |\n",
      "| AverageDiscountedReturn | 0.854      |\n",
      "| TDError^2               | 0.00065436 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 84        |\n",
      "| Steps                   | 84999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 11961     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0014733 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 85         |\n",
      "| Steps                   | 85999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 12118      |\n",
      "| AverageReturn           | 0.9        |\n",
      "| AverageDiscountedReturn | 0.854      |\n",
      "| TDError^2               | 0.00053405 |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 86       |\n",
      "| Steps                   | 86999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 12280    |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95004  |\n",
      "| TDError^2               | 0.001025 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 87        |\n",
      "| Steps                   | 87999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 12439     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94531   |\n",
      "| TDError^2               | 0.0010579 |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 88         |\n",
      "| Steps                   | 88999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 12574      |\n",
      "| AverageReturn           | 0.6        |\n",
      "| AverageDiscountedReturn | 0.55076    |\n",
      "| TDError^2               | 0.00051726 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| Iteration               | 89         |\n",
      "| Steps                   | 89999      |\n",
      "| Epsilon                 | 0.05       |\n",
      "| Episodes                | 12707      |\n",
      "| AverageReturn           | 1          |\n",
      "| AverageDiscountedReturn | 0.95004    |\n",
      "| TDError^2               | 0.00063419 |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 90        |\n",
      "| Steps                   | 90999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 12813     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0027217 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 91        |\n",
      "| Steps                   | 91999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 12973     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0024985 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 92        |\n",
      "| Steps                   | 92999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13132     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94438   |\n",
      "| TDError^2               | 0.0029096 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 93        |\n",
      "| Steps                   | 93999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13289     |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.854     |\n",
      "| TDError^2               | 0.0011467 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 94        |\n",
      "| Steps                   | 94999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13448     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94721   |\n",
      "| TDError^2               | 0.0024111 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 95        |\n",
      "| Steps                   | 95999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13607     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.95099   |\n",
      "| TDError^2               | 0.0025553 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 96        |\n",
      "| Steps                   | 96999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13766     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.94341   |\n",
      "| TDError^2               | 0.0021854 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 97        |\n",
      "| Steps                   | 97999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 13926     |\n",
      "| AverageReturn           | 0.9       |\n",
      "| AverageDiscountedReturn | 0.84465   |\n",
      "| TDError^2               | 0.0014632 |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Iteration               | 98       |\n",
      "| Steps                   | 98999    |\n",
      "| Epsilon                 | 0.05     |\n",
      "| Episodes                | 14036    |\n",
      "| AverageReturn           | 1        |\n",
      "| AverageDiscountedReturn | 0.95099  |\n",
      "| TDError^2               | 0.001324 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Iteration               | 99        |\n",
      "| Steps                   | 99999     |\n",
      "| Epsilon                 | 0.05      |\n",
      "| Episodes                | 14195     |\n",
      "| AverageReturn           | 1         |\n",
      "| AverageDiscountedReturn | 0.9491    |\n",
      "| TDError^2               | 0.0010862 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dqn_gridworld.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NN_linear' object has no attribute 'set_params'\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FFFH\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FFFF\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Done!\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FFFH\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FFFF\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Done!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a2aac5f29ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize learned policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdqn_gridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b26c52e70a62>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, epsilon)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# visualize learned policy\n",
    "dqn_gridworld.test(epsilon=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test algorithm on Pong\n",
    "Now we can train for longer on a substantially more complex environment: Pong from the Atari suite. To speed up training, instead of playing from pixels we will be playing directly from the ram state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-23 22:33:35,997] Making new env: Pong-ram-v0\n"
     ]
    }
   ],
   "source": [
    "env = EpisodicLifeEnv(NoopResetEnv(gym.make('Pong-ram-v0')))\n",
    "log_dir = \"data/local/dqn_pong\"\n",
    "\n",
    "logger.session(log_dir).__enter__()\n",
    "env.seed(42)\n",
    "\n",
    "# Initialize the replay buffer that we will use.\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# Initialize DQN training procedure.\n",
    "dqn_pong = DQN(\n",
    "    env=env,\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    act_dim=env.action_space.n,\n",
    "    NN=NN,\n",
    "    obs_preprocessor=preprocess_obs_ram,\n",
    "    replay_buffer=replay_buffer,\n",
    "    opt_batch_size=64,\n",
    "    # DQN gamma parameter\n",
    "    discount=0.99,\n",
    "    # Training procedure length\n",
    "    initial_step=1000000,\n",
    "    max_steps=10000000,\n",
    "    learning_start_itr=100000,\n",
    "    # Frequency of copying the actual Q to the target Q\n",
    "    target_q_update_freq=1000,\n",
    "    # Frequency of updating the Q-value function\n",
    "    train_q_freq=4,\n",
    "    # Exploration parameters\n",
    "    initial_eps=1.0,\n",
    "    final_eps=0.05,\n",
    "    fraction_eps=0.1,\n",
    "    # Logging\n",
    "    log_freq=10000,\n",
    "    render=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn_pong.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization\n",
    "To visualize your learning curves, you can use the `viskit` tool by calling in a terminal:\n",
    "`python viskit/frontend.py path/to/log_dir`\n",
    "where `path/to/log_dir` is by default `data/local/exp_name`, where `exp_name` is `dqn_pong` in the case of pong for example.\n",
    "For this visualization to work you need to have the path to the homework directory to be added to your `$PYTHONPATH`. You should then see in your browser something like this:\n",
    "![title](simpledqn/pong_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Student Visualization Result\n",
    "![title](simpledqn/pong_learning_student.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deeprlbootcamp]",
   "language": "python",
   "name": "conda-env-deeprlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
